{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classify price after pattern.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romqn1999/Predict-price-after-pattern/blob/model-classification/Classify_price_after_pattern.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzopf-OyxSMl",
        "outputId": "8b54ff3e-e9e4-420b-efee-00b4d5ef1aa7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JkUOuKONzqJx",
        "outputId": "dbe3506a-1d30-4fbb-8dbb-f50cf7567ea3"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bbe8a1db-16af-4120-b081-6262e2a178a5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bbe8a1db-16af-4120-b081-6262e2a178a5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ZM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ZM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving Z_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to Z_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving Y_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to Y_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving XPEV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to XPEV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving XOM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to XOM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving X_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to X_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving WMT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to WMT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving WFC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to WFC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving W_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to W_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving VZ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to VZ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving VXRT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to VXRT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving VICI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to VICI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving VIAC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to VIAC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving V_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to V_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving TSLA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to TSLA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving T_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to T_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving SPY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to SPY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving SPCE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to SPCE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving SNP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to SNP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving RIOT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to RIOT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving RIDE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to RIDE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving R_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to R_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving QYLD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to QYLD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving QQQ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to QQQ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving QCOM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to QCOM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving PLUG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to PLUG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving NVDA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to NVDA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving NOK_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to NOK_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving NFLX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to NFLX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving KO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to KO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving IBM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to IBM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving GOOG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to GOOG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving GME_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to GME_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving FB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to FB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving F_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to F_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ETH-USD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ETH-USD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving DAX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to DAX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving BTC-USD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to BTC-USD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving BB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to BB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving BABA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to BABA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving BA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to BA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AMZN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AMZN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AMD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AMD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AMC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AMC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFZ.SG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFZ.SG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFZ.MU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFZ.MU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFZ.F_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFZ.F_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFZ.BE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFZ.BE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFYA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFYA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFX.VI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFX.VI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFX.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFX.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFX.HA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFX.HA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFX.F_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFX.F_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFX.DE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFX.DE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFW.SG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFW.SG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFW.MU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFW.MU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFW.F_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFW.F_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFW.DU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFW.DU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFW.DE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFW.DE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFTPY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFTPY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFTM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFTM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFSIP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFSIP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFSIN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFSIN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFSIM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFSIM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFSIC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFSIC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFSIB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFSIB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFS.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFS.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFRM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFRM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFPW_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFPW_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFP.SW_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFP.SW_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFN.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFN.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFMD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFMD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFLYY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFLYY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFLT.ME_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFLT.ME_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFLG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFLG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFL.SG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFL.SG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFKS.ME_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFKS.ME_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFK.OL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFK.OL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFK_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFK_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFJCX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFJCX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFJAX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFJAX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFINP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFINP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFIN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFIN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFIB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFIB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFHP.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFHP.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFHL.TA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFHL.TA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFHIF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFHIF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFHBL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFHBL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFH.JO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFH.JO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFGC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFGC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFGD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFGD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFG.OL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFG.OL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFE.JO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFE.JO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFDVX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFDVX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFDIX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFDIX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFCG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFCG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFC.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFC.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFBA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFBA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFAZX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFAZX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AFAVX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AFAVX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEZS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEZS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEYE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEYE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEXE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEXE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEXAY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEXAY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AETUF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AETUF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AESC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AESC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AES_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AES_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AER_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AER_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEPT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEPT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEPPL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEPPL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AENZ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AENZ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEMD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEMD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEIS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEIS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEHR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEHR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEHL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEHL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEGY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEGY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEFC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEFC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AEB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AEB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADYYF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADYYF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADYX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADYX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADYEY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADYEY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADXS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADXS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADXN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADXN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADVM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADVM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADTX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADTX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADTN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADTN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADTM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADTM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADSK_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADSK_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADRNY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADRNY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADPT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADPT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADNT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADNT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADMP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADMP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADMA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADMA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADIL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADIL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADHC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADHC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADES_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADES_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADDYY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADDYY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADDDF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADDDF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADCT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADCT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADBE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADBE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ADAP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ADAP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACXIF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACXIF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACWX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACWX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACWV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACWV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACWI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACWI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACTG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACTG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACST_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACST_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACTC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACTC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACRX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACRX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACRS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACRS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACRHF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACRHF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACRE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACRE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACRDF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACRDF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACNB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACNB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACMR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACMR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACLS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACLS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACLLF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACLLF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACKAY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACKAY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACIW_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACIW_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACIU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACIU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACHV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACHV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACHC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACHC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACH_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACH_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACGLO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACGLO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACGL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACGL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACGBY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACGBY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACFN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACFN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACFL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACFL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACET_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACET_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACES_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACES_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACER_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACER_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACDVF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACDVF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACCD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACCD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACBI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACBI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACAD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACAD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACAC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACAC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ACA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ACA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABWN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABWN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABTX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABTX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABST_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABST_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABNB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABNB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABNAF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABNAF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABMD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABMD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABLZF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABLZF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABIO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABIO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABEV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABEV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABEO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABEO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABCL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABCL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABCB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABCB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABBV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABBV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving ABB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to ABB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAXJ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAXJ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAWW_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAWW_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAVXF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAVXF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAVVF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAVVF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AATRL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AATRL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AASP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AASP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAPL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAPL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAOI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAOI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAME_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAME_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAMC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAMC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAIIQ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAIIQ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAIGF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAIGF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AAIC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AAIC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AADR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AADR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AADEX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AADEX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AADAX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AADAX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AACQ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AACQ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AACG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AACG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n",
            "Saving AA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv to AA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns (3).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no8aDSVlApbi"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN2ViWdt_Mxc"
      },
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import *\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adagrad, Adadelta, RMSprop, Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy import stats"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9fE_16D7ZPO"
      },
      "source": [
        "## Get data at pattern for training & testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zrT35S6m0GZB",
        "outputId": "7f2c19b7-9dae-435c-85a2-f4b04ae5d633"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "price_patterns_df = pd.DataFrame()\n",
        "for filename in uploaded.keys():\n",
        "    print(filename)\n",
        "    if 'double_top_data_patterns.csv' not in filename:\n",
        "    # if 'data_patterns.csv' not in filename:\n",
        "        print('Skipping file', filename)\n",
        "        continue\n",
        "    try:\n",
        "        df = pd.read_csv(io.StringIO(uploaded[filename].decode('utf-8')),\n",
        "                        header=None)\n",
        "        price_patterns_df = price_patterns_df.append(df)\n",
        "    except:\n",
        "        print('Error', filename)\n",
        "\n",
        "price_patterns_df = price_patterns_df.sample(frac=1).reset_index(drop=True)\n",
        "price_patterns_df"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ZM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Z_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Y_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "XPEV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "XOM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "X_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "WMT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "WFC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "W_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "VZ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "VXRT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "VICI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "VIAC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "V_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "TSLA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "T_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "SPY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "SPCE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "SNP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "RIOT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "RIDE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error RIDE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "R_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "QYLD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "QQQ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "QCOM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "PLUG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "NVDA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "NOK_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "NFLX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "KO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "IBM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "GOOG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "GME_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "FB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "F_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ETH-USD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "DAX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "BTC-USD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "BB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "BABA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "BA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AMZN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AMD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AMC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFZ.SG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error AFZ.SG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFZ.MU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error AFZ.MU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFZ.F_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFZ.BE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFYA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFX.VI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFX.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFX.HA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFX.F_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFX.DE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFW.SG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFW.MU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFW.F_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFW.DU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFW.DE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFTPY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFTM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFSIP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFSIN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFSIM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFSIC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFSIB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFS.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFRM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFPW_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFP.SW_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFN.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFMD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFLYY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFLT.ME_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFLG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFL.SG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFKS.ME_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFK.OL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFK_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFJCX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFJAX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFINP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFIN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFIB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFHP.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFHL.TA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFHIF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFHBL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error AFHBL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFH.JO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFGC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFGD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFG.OL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFE.JO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFDVX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFDIX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFCG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFC.L_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFBA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error AFB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFAZX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AFAVX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEZS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEYE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEXE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEXAY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AETUF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AESC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AES_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AER_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEPT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEPPL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AENZ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEMD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEIS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEHR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEHL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEGY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error AEGY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEFC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error AEFC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AEB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADYYF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADYX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADYEY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error ADYEY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADXS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADXN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADVM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADTX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error ADTX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADTN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADTM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADSK_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADRNY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADPT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error ADPT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADNT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADMP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADMA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADIL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error ADIL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADHC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADES_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADDYY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADDDF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADCT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADBE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ADAP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACXIF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACWX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACWV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACWI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACTG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACST_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACTC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACRX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACRS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACRHF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error ACRHF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACRE_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACRDF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error ACP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACNB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACMR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACLS_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACLLF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACKAY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACIW_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACIU_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error ACI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACHV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACHC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACH_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACGLO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error ACGLO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACGL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACGBY_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACFN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACFL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "Error ACFL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACET_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACES_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACER_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACDVF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACCD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACBI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACAD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACAC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ACA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABWN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABTX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABST_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABNB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABNAF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABMD_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABM_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABLZF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABIO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABEV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABEO_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABCL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABCB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABBV_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "ABB_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAXJ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAWW_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAVXF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAVVF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AATRL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAT_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AASP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAPL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAP_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAOI_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAN_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAME_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAMC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAL_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAIIQ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAIGF_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AAIC_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AADR_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AADEX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AADAX_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AACQ_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AACG_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n",
            "AA_2000-01-01 00_00_00_2021-06-04_30_double_top_data_patterns.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42.340000</td>\n",
              "      <td>42.889999</td>\n",
              "      <td>42.630001</td>\n",
              "      <td>42.759998</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>43.849998</td>\n",
              "      <td>43.459999</td>\n",
              "      <td>43.230000</td>\n",
              "      <td>42.849998</td>\n",
              "      <td>42.439999</td>\n",
              "      <td>42.810001</td>\n",
              "      <td>44.320000</td>\n",
              "      <td>43.389999</td>\n",
              "      <td>42.610001</td>\n",
              "      <td>41.349998</td>\n",
              "      <td>40.959999</td>\n",
              "      <td>41.560001</td>\n",
              "      <td>41.900002</td>\n",
              "      <td>42.080002</td>\n",
              "      <td>40.990002</td>\n",
              "      <td>41.849998</td>\n",
              "      <td>42.320000</td>\n",
              "      <td>42.099998</td>\n",
              "      <td>41.669998</td>\n",
              "      <td>42.110001</td>\n",
              "      <td>43.730000</td>\n",
              "      <td>44.849998</td>\n",
              "      <td>43.939999</td>\n",
              "      <td>44.520000</td>\n",
              "      <td>43.450001</td>\n",
              "      <td>42.040001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11178.445312</td>\n",
              "      <td>11119.090820</td>\n",
              "      <td>11109.198242</td>\n",
              "      <td>11138.875977</td>\n",
              "      <td>11307.046875</td>\n",
              "      <td>11346.617188</td>\n",
              "      <td>11346.617188</td>\n",
              "      <td>11218.015625</td>\n",
              "      <td>10950.919922</td>\n",
              "      <td>10950.919922</td>\n",
              "      <td>10950.919922</td>\n",
              "      <td>10861.887695</td>\n",
              "      <td>11099.305664</td>\n",
              "      <td>11138.875977</td>\n",
              "      <td>11376.293945</td>\n",
              "      <td>11564.250000</td>\n",
              "      <td>11316.939453</td>\n",
              "      <td>11682.958984</td>\n",
              "      <td>11504.895508</td>\n",
              "      <td>11326.832031</td>\n",
              "      <td>11267.477539</td>\n",
              "      <td>11069.628906</td>\n",
              "      <td>11089.414062</td>\n",
              "      <td>11218.015625</td>\n",
              "      <td>11247.692383</td>\n",
              "      <td>11336.724609</td>\n",
              "      <td>11148.768555</td>\n",
              "      <td>11307.046875</td>\n",
              "      <td>11247.692383</td>\n",
              "      <td>10980.596680</td>\n",
              "      <td>10683.824219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>256.799988</td>\n",
              "      <td>248.559998</td>\n",
              "      <td>253.539993</td>\n",
              "      <td>259.130005</td>\n",
              "      <td>261.739990</td>\n",
              "      <td>261.000000</td>\n",
              "      <td>262.100006</td>\n",
              "      <td>266.320007</td>\n",
              "      <td>269.519989</td>\n",
              "      <td>275.869995</td>\n",
              "      <td>260.299988</td>\n",
              "      <td>260.010010</td>\n",
              "      <td>256.220001</td>\n",
              "      <td>248.539993</td>\n",
              "      <td>246.539993</td>\n",
              "      <td>268.579987</td>\n",
              "      <td>260.440002</td>\n",
              "      <td>261.179993</td>\n",
              "      <td>251.500000</td>\n",
              "      <td>246.559998</td>\n",
              "      <td>252.330002</td>\n",
              "      <td>252.179993</td>\n",
              "      <td>252.389999</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>253.910004</td>\n",
              "      <td>268.000000</td>\n",
              "      <td>267.339996</td>\n",
              "      <td>272.899994</td>\n",
              "      <td>268.250000</td>\n",
              "      <td>258.730011</td>\n",
              "      <td>250.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>99.019997</td>\n",
              "      <td>99.860001</td>\n",
              "      <td>98.639999</td>\n",
              "      <td>99.500000</td>\n",
              "      <td>100.250000</td>\n",
              "      <td>101.129997</td>\n",
              "      <td>103.320000</td>\n",
              "      <td>105.980003</td>\n",
              "      <td>96.639999</td>\n",
              "      <td>99.339996</td>\n",
              "      <td>103.290001</td>\n",
              "      <td>102.930000</td>\n",
              "      <td>104.480003</td>\n",
              "      <td>104.500000</td>\n",
              "      <td>103.459999</td>\n",
              "      <td>103.129997</td>\n",
              "      <td>107.400002</td>\n",
              "      <td>108.139999</td>\n",
              "      <td>109.089996</td>\n",
              "      <td>107.769997</td>\n",
              "      <td>108.129997</td>\n",
              "      <td>109.059998</td>\n",
              "      <td>109.849998</td>\n",
              "      <td>109.330002</td>\n",
              "      <td>108.559998</td>\n",
              "      <td>109.419998</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>110.349998</td>\n",
              "      <td>109.500000</td>\n",
              "      <td>108.400002</td>\n",
              "      <td>108.930000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23.360001</td>\n",
              "      <td>22.480000</td>\n",
              "      <td>22.379999</td>\n",
              "      <td>22.750000</td>\n",
              "      <td>25.559999</td>\n",
              "      <td>25.809999</td>\n",
              "      <td>26.250000</td>\n",
              "      <td>27.010000</td>\n",
              "      <td>26.010000</td>\n",
              "      <td>24.709999</td>\n",
              "      <td>25.100000</td>\n",
              "      <td>23.520000</td>\n",
              "      <td>23.950001</td>\n",
              "      <td>24.730000</td>\n",
              "      <td>25.330000</td>\n",
              "      <td>24.840000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>26.709999</td>\n",
              "      <td>27.480000</td>\n",
              "      <td>25.600000</td>\n",
              "      <td>26.850000</td>\n",
              "      <td>26.190001</td>\n",
              "      <td>26.059999</td>\n",
              "      <td>26.570000</td>\n",
              "      <td>26.730000</td>\n",
              "      <td>26.760000</td>\n",
              "      <td>27.459999</td>\n",
              "      <td>27.879999</td>\n",
              "      <td>27.959999</td>\n",
              "      <td>25.200001</td>\n",
              "      <td>25.719999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6896</th>\n",
              "      <td>38.709999</td>\n",
              "      <td>38.990002</td>\n",
              "      <td>39.029999</td>\n",
              "      <td>39.060001</td>\n",
              "      <td>39.360001</td>\n",
              "      <td>39.520000</td>\n",
              "      <td>39.779999</td>\n",
              "      <td>39.680000</td>\n",
              "      <td>39.389999</td>\n",
              "      <td>39.470001</td>\n",
              "      <td>39.500000</td>\n",
              "      <td>39.910000</td>\n",
              "      <td>40.750000</td>\n",
              "      <td>40.529999</td>\n",
              "      <td>40.680000</td>\n",
              "      <td>40.279999</td>\n",
              "      <td>40.430000</td>\n",
              "      <td>39.930000</td>\n",
              "      <td>39.660000</td>\n",
              "      <td>39.759998</td>\n",
              "      <td>39.340000</td>\n",
              "      <td>39.439999</td>\n",
              "      <td>40.919998</td>\n",
              "      <td>40.500000</td>\n",
              "      <td>40.259998</td>\n",
              "      <td>40.360001</td>\n",
              "      <td>40.759998</td>\n",
              "      <td>40.330002</td>\n",
              "      <td>40.470001</td>\n",
              "      <td>40.060001</td>\n",
              "      <td>40.709999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6897</th>\n",
              "      <td>117.230003</td>\n",
              "      <td>119.970001</td>\n",
              "      <td>119.809998</td>\n",
              "      <td>117.209999</td>\n",
              "      <td>120.330002</td>\n",
              "      <td>119.180000</td>\n",
              "      <td>117.250000</td>\n",
              "      <td>115.989998</td>\n",
              "      <td>117.320000</td>\n",
              "      <td>118.370003</td>\n",
              "      <td>118.430000</td>\n",
              "      <td>119.680000</td>\n",
              "      <td>119.480003</td>\n",
              "      <td>119.820000</td>\n",
              "      <td>120.739998</td>\n",
              "      <td>120.760002</td>\n",
              "      <td>120.940002</td>\n",
              "      <td>119.430000</td>\n",
              "      <td>117.660004</td>\n",
              "      <td>117.290001</td>\n",
              "      <td>117.720001</td>\n",
              "      <td>118.410004</td>\n",
              "      <td>121.570000</td>\n",
              "      <td>121.739998</td>\n",
              "      <td>123.309998</td>\n",
              "      <td>123.610001</td>\n",
              "      <td>125.400002</td>\n",
              "      <td>123.650002</td>\n",
              "      <td>124.550003</td>\n",
              "      <td>123.000000</td>\n",
              "      <td>123.650002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6898</th>\n",
              "      <td>195.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>172.500000</td>\n",
              "      <td>172.500000</td>\n",
              "      <td>180.000000</td>\n",
              "      <td>187.500000</td>\n",
              "      <td>187.500000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>247.500000</td>\n",
              "      <td>225.000000</td>\n",
              "      <td>225.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>202.500000</td>\n",
              "      <td>210.000000</td>\n",
              "      <td>217.500000</td>\n",
              "      <td>225.000000</td>\n",
              "      <td>210.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>202.500000</td>\n",
              "      <td>187.500000</td>\n",
              "      <td>217.500000</td>\n",
              "      <td>210.000000</td>\n",
              "      <td>210.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>187.500000</td>\n",
              "      <td>210.000000</td>\n",
              "      <td>202.500000</td>\n",
              "      <td>217.500000</td>\n",
              "      <td>202.500000</td>\n",
              "      <td>217.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6899</th>\n",
              "      <td>53.500000</td>\n",
              "      <td>54.849998</td>\n",
              "      <td>55.020000</td>\n",
              "      <td>55.139999</td>\n",
              "      <td>55.580002</td>\n",
              "      <td>53.630001</td>\n",
              "      <td>53.520000</td>\n",
              "      <td>52.779999</td>\n",
              "      <td>53.240002</td>\n",
              "      <td>55.200001</td>\n",
              "      <td>55.619999</td>\n",
              "      <td>55.150002</td>\n",
              "      <td>55.090000</td>\n",
              "      <td>54.560001</td>\n",
              "      <td>54.430000</td>\n",
              "      <td>55.669998</td>\n",
              "      <td>54.730000</td>\n",
              "      <td>54.119999</td>\n",
              "      <td>55.980000</td>\n",
              "      <td>54.900002</td>\n",
              "      <td>54.020000</td>\n",
              "      <td>54.549999</td>\n",
              "      <td>54.750000</td>\n",
              "      <td>55.299999</td>\n",
              "      <td>54.599998</td>\n",
              "      <td>55.400002</td>\n",
              "      <td>55.250000</td>\n",
              "      <td>54.700001</td>\n",
              "      <td>55.950001</td>\n",
              "      <td>53.200001</td>\n",
              "      <td>53.099998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6900</th>\n",
              "      <td>2958.000000</td>\n",
              "      <td>2988.000000</td>\n",
              "      <td>2976.000000</td>\n",
              "      <td>2970.000000</td>\n",
              "      <td>2970.000000</td>\n",
              "      <td>3012.000000</td>\n",
              "      <td>3012.000000</td>\n",
              "      <td>2916.000000</td>\n",
              "      <td>3024.000000</td>\n",
              "      <td>3078.000000</td>\n",
              "      <td>3060.000000</td>\n",
              "      <td>3120.000000</td>\n",
              "      <td>3198.000000</td>\n",
              "      <td>3150.000000</td>\n",
              "      <td>3204.000000</td>\n",
              "      <td>3360.000000</td>\n",
              "      <td>3522.000000</td>\n",
              "      <td>3792.000000</td>\n",
              "      <td>3696.000000</td>\n",
              "      <td>3594.000000</td>\n",
              "      <td>3660.000000</td>\n",
              "      <td>3636.000000</td>\n",
              "      <td>3642.000000</td>\n",
              "      <td>3660.000000</td>\n",
              "      <td>3702.000000</td>\n",
              "      <td>3684.000000</td>\n",
              "      <td>3636.000000</td>\n",
              "      <td>3630.000000</td>\n",
              "      <td>3612.000000</td>\n",
              "      <td>3576.000000</td>\n",
              "      <td>3618.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6901 rows  31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                0             1   ...            29            30\n",
              "0        42.340000     42.889999  ...     43.450001     42.040001\n",
              "1     11178.445312  11119.090820  ...  10980.596680  10683.824219\n",
              "2       256.799988    248.559998  ...    258.730011    250.000000\n",
              "3        99.019997     99.860001  ...    108.400002    108.930000\n",
              "4        23.360001     22.480000  ...     25.200001     25.719999\n",
              "...            ...           ...  ...           ...           ...\n",
              "6896     38.709999     38.990002  ...     40.060001     40.709999\n",
              "6897    117.230003    119.970001  ...    123.000000    123.650002\n",
              "6898    195.000000    195.000000  ...    202.500000    217.500000\n",
              "6899     53.500000     54.849998  ...     53.200001     53.099998\n",
              "6900   2958.000000   2988.000000  ...   3576.000000   3618.000000\n",
              "\n",
              "[6901 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3AloVK100Vr",
        "outputId": "8810ed1b-00eb-4e30-8b15-dfa40d9de532"
      },
      "source": [
        "#@title Separate into input and output columns: Get data: X and y\n",
        "pre_time_steps = 30 #@param {type:\"integer\"}\n",
        "\n",
        "X_df = price_patterns_df[price_patterns_df.columns[-(pre_time_steps + 1):-1]]\n",
        "y_df = price_patterns_df[price_patterns_df.columns[-1:]]\n",
        "print(X_df)\n",
        "print(y_df)\n",
        "\n",
        "X_data = X_df.values\n",
        "y_data = y_df.values\n",
        "print(X_data)\n",
        "print(y_data)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                0             1   ...            28            29\n",
            "0        42.340000     42.889999  ...     44.520000     43.450001\n",
            "1     11178.445312  11119.090820  ...  11247.692383  10980.596680\n",
            "2       256.799988    248.559998  ...    268.250000    258.730011\n",
            "3        99.019997     99.860001  ...    109.500000    108.400002\n",
            "4        23.360001     22.480000  ...     27.959999     25.200001\n",
            "...            ...           ...  ...           ...           ...\n",
            "6896     38.709999     38.990002  ...     40.470001     40.060001\n",
            "6897    117.230003    119.970001  ...    124.550003    123.000000\n",
            "6898    195.000000    195.000000  ...    217.500000    202.500000\n",
            "6899     53.500000     54.849998  ...     55.950001     53.200001\n",
            "6900   2958.000000   2988.000000  ...   3612.000000   3576.000000\n",
            "\n",
            "[6901 rows x 30 columns]\n",
            "                30\n",
            "0        42.040001\n",
            "1     10683.824219\n",
            "2       250.000000\n",
            "3       108.930000\n",
            "4        25.719999\n",
            "...            ...\n",
            "6896     40.709999\n",
            "6897    123.650002\n",
            "6898    217.500000\n",
            "6899     53.099998\n",
            "6900   3618.000000\n",
            "\n",
            "[6901 rows x 1 columns]\n",
            "[[   42.34000015    42.88999939    42.63000107 ...    43.93999863\n",
            "     44.52000046    43.45000076]\n",
            " [11178.4453125  11119.09082031 11109.19824219 ... 11307.046875\n",
            "  11247.69238281 10980.59667969]\n",
            " [  256.79998779   248.55999756   253.53999329 ...   272.8999939\n",
            "    268.25         258.73001099]\n",
            " ...\n",
            " [  195.           195.           172.5        ...   202.5\n",
            "    217.5          202.5       ]\n",
            " [   53.5           54.84999847    55.02000046 ...    54.70000076\n",
            "     55.95000076    53.20000076]\n",
            " [ 2958.          2988.          2976.         ...  3630.\n",
            "   3612.          3576.        ]]\n",
            "[[   42.04000092]\n",
            " [10683.82421875]\n",
            " [  250.        ]\n",
            " ...\n",
            " [  217.5       ]\n",
            " [   53.09999847]\n",
            " [ 3618.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r198r1bUuvR4"
      },
      "source": [
        "def get_direction(pre, cur, min_change=1):\n",
        "    if abs(pre - cur) < min_change:\n",
        "        return 0\n",
        "    else:\n",
        "        if pre < cur:\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "def get_binary_direction(pre, cur, min_change=0):\n",
        "    d = get_direction(pre, cur, min_change)\n",
        "    if d < 1:\n",
        "        d = 0\n",
        "    return d\n",
        "\n",
        "def y_preice_to_y_label(X, y, min_change=0, num_classes=2):\n",
        "    assert len(X) == len(y)\n",
        "    n = len(y)\n",
        "    if num_classes == 2:\n",
        "        y_label = np.array([get_binary_direction(X[i][-1], y[i], min_change) for i in range(n)])\n",
        "    else:\n",
        "        y_label = np.array([get_direction(X[i][-1], y[i], min_change) for i in range(n)])\n",
        "        # Convert from {-1; 0; 1} to label {0; 1; 2}\n",
        "        y_label = y_label + 1\n",
        "    return y_label"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncy0CeLoY-iX",
        "outputId": "9bcc9824-d0c3-49b3-a660-ce321d7a32dd"
      },
      "source": [
        "#@title Preprocess & split data for train model\n",
        "# Train, test split\n",
        "train_ratio = 0.8 #@param {type:\"number\"}\n",
        "if train_ratio < 0 or  train_ratio > 1:\n",
        "    train_ratio = 0.8\n",
        "n_data = len(X_data)\n",
        "n_train = int(train_ratio * n_data)\n",
        "n_test = n_data - n_train\n",
        "# Feature Scaling\n",
        "scale_method = \"MinMax\" #@param [\"None\", \"MinMax\"]\n",
        "min_price_change = 0 #@param {type:\"number\"}\n",
        "num_classes = 2 #@param {type:\"integer\"}\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, train_size=train_ratio, random_state=7)\n",
        "X_train, y_train = X_data[: n_train], y_data[: n_train]\n",
        "X_test, y_test = X_data[n_train:], y_data[n_train:]\n",
        "y_train_label = y_preice_to_y_label(X_train, y_train, min_price_change, num_classes=num_classes)\n",
        "y_train_categorical = to_categorical(y_train_label, num_classes=num_classes)\n",
        "y_test_label = y_preice_to_y_label(X_test, y_test, min_price_change, num_classes=num_classes)\n",
        "y_test_categorical = to_categorical(y_test_label, num_classes=num_classes)\n",
        "if scale_method == 'MinMax':\n",
        "    sc_train = MinMaxScaler(feature_range = (0, 1))\n",
        "    X_train_scaled = sc_train.fit_transform(X_train.T).T\n",
        "    y_train_scaled = sc_train.transform(y_train.T).T\n",
        "    y_train_scaled_label = y_preice_to_y_label(X_train_scaled, y_train_scaled, min_price_change, num_classes=num_classes)\n",
        "    y_train_scaled_categorical = to_categorical(y_train_scaled_label, num_classes=num_classes)\n",
        "    sc_test = MinMaxScaler(feature_range = (0, 1))\n",
        "    X_test_scaled = sc_test.fit_transform(X_test.T).T\n",
        "    y_test_scaled = sc_test.transform(y_test.T).T\n",
        "    y_test_scaled_label = y_preice_to_y_label(X_test_scaled, y_test_scaled, min_price_change, num_classes=num_classes)\n",
        "    y_test_scaled_categorical = to_categorical(y_test_scaled_label, num_classes=num_classes)\n",
        "    # X_train, y_train = X_train_scaled, y_train_scaled\n",
        "    # X_test, y_test = X_test_scaled, y_test_scaled\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_train_scaled.shape, y_train_scaled.shape)\n",
        "print(y_train_categorical.shape)\n",
        "print(y_train_scaled_categorical.shape)\n",
        "print('Stats y_train:', stats.describe(y_train))\n",
        "print('Stats y_train_scaled:', stats.describe(y_train_scaled))\n",
        "print('Stats y_train_label:', stats.describe(y_train_label))\n",
        "print(np.bincount(y_train_label))\n",
        "print('Stats y_train_categorical:', stats.describe(y_train_categorical))\n",
        "print('Stats y_train_scaled_label:', stats.describe(y_train_scaled_label))\n",
        "print(np.bincount(y_train_scaled_label))\n",
        "print('Stats y_train_scaled_categorical:', stats.describe(y_train_scaled_categorical))\n",
        "print(X_test.shape, y_test.shape)\n",
        "print(X_test_scaled.shape, y_test_scaled.shape)\n",
        "print(y_test_categorical.shape)\n",
        "print(y_test_scaled_categorical.shape)\n",
        "\n",
        "# print(X_train)\n",
        "# print(y_train)\n",
        "# print(y_train_label)\n",
        "# print(y_train_categorical)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5520, 30) (5520, 1)\n",
            "(5520, 30) (5520, 1)\n",
            "(5520, 2)\n",
            "(5520, 2)\n",
            "Stats y_train: DescribeResult(nobs=5520, minmax=(array([0.75999999]), array([2253825.])), mean=array([13704.50232168]), variance=array([8.89549561e+09]), skewness=array([14.79857766]), kurtosis=array([259.48203919]))\n",
            "Stats y_train_scaled: DescribeResult(nobs=5520, minmax=(array([-2.07671961]), array([6.2962963])), mean=array([0.43354356]), variance=array([0.13196952]), skewness=array([0.50006861]), kurtosis=array([12.59908896]))\n",
            "Stats y_train_label: DescribeResult(nobs=5520, minmax=(0, 1), mean=0.4971014492753623, variance=0.25003689494263565, skewness=0.011594397723085795, kurtosis=-1.9998655699414392)\n",
            "[2776 2744]\n",
            "Stats y_train_categorical: DescribeResult(nobs=5520, minmax=(array([0., 0.], dtype=float32), array([1., 1.], dtype=float32)), mean=array([0.5028986 , 0.49710146], dtype=float32), variance=array([0.25004208, 0.25004208], dtype=float32), skewness=array([-0.01159441,  0.01159438], dtype=float32), kurtosis=array([-1.9998745, -1.9998745], dtype=float32))\n",
            "Stats y_train_scaled_label: DescribeResult(nobs=5520, minmax=(0, 1), mean=0.4971014492753623, variance=0.25003689494263565, skewness=0.011594397723085795, kurtosis=-1.9998655699414392)\n",
            "[2776 2744]\n",
            "Stats y_train_scaled_categorical: DescribeResult(nobs=5520, minmax=(array([0., 0.], dtype=float32), array([1., 1.], dtype=float32)), mean=array([0.5028986 , 0.49710146], dtype=float32), variance=array([0.25004208, 0.25004208], dtype=float32), skewness=array([-0.01159441,  0.01159438], dtype=float32), kurtosis=array([-1.9998745, -1.9998745], dtype=float32))\n",
            "(1381, 30) (1381, 1)\n",
            "(1381, 30) (1381, 1)\n",
            "(1381, 2)\n",
            "(1381, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s82DA41L_xiz"
      },
      "source": [
        "## LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZfyb31yHUi4"
      },
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN8Yeavy-FV1"
      },
      "source": [
        "#@title Build & fit model\n",
        "lstm_units =  128#@param {type:\"integer\"}\n",
        "dropout_prob = 0.2 #@param {type:\"number\"}\n",
        "epochs = 5000 #@param {type:\"integer\"}\n",
        "batch_size = 32768 #@param {type:\"integer\"}\n",
        "lr = 0.001 #@param {type:\"number\"}\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jin-QL3uXUOg"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZKDU7Oy3XXHQ",
        "outputId": "f43d9768-0a6e-4de5-8e12-83eb65863053"
      },
      "source": [
        "def train_lstm(X_train, y_train, X_test, y_test, lstm_units=50, dropout_prob=0.5, n_classes=2, epochs=6000, batch_size=32768, lr=0.001):\n",
        "    # We have now reshaped the data into the following format (#values, #time-steps, #1 dimensional output).\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "    y_train = np.reshape(y_train, (y_train.shape[0], n_classes))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "    y_test = np.reshape(y_test, (y_test.shape[0], n_classes))\n",
        "\n",
        "    model = Sequential()\n",
        "    #Adding the first LSTM layer and some Dropout regularisation\n",
        "    model.add(LSTM(units=lstm_units,\n",
        "                   # return_sequences=True,\n",
        "                   input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "                   activation='relu'))\n",
        "    model.add(Dropout(dropout_prob))\n",
        "    # # Adding a second LSTM layer and some Dropout regularisation\n",
        "    # model.add(LSTM(units=lstm_units,\n",
        "    #             return_sequences=True))\n",
        "    # model.add(Dropout(dropout_prob))\n",
        "    # # Adding a third LSTM layer and some Dropout regularisation\n",
        "    # model.add(LSTM(units=lstm_units,\n",
        "    #             return_sequences=True))\n",
        "    # model.add(Dropout(dropout_prob))\n",
        "    # # Adding a fourth LSTM layer and some Dropout regularisation\n",
        "    # model.add(LSTM(units=lstm_units))\n",
        "    # model.add(Dropout(dropout_prob))\n",
        "    # Adding the output layer\n",
        "    # model.add(Dense(units=1))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    # model.add(Dense(n_classes, activation='softmax'))\n",
        "    model.add(Dense(n_classes))\n",
        "    if n_classes == 2:\n",
        "        model.add(Activation('sigmoid'))\n",
        "    else:\n",
        "        model.add(Activation('softmax'))\n",
        "\n",
        "    # Adam optimizer\n",
        "    opt = Adam(learning_rate=lr)\n",
        "\n",
        "    checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\",\n",
        "                                 monitor='val_accuracy', verbose=2,\n",
        "                                 save_best_only=True, mode='auto')\n",
        "\n",
        "    # Compiling the RNN\n",
        "    # model.compile(optimizer = opt, loss = 'mean_squared_error')\n",
        "    # model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "    # model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    if n_classes == 2:\n",
        "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # Fitting the RNN to the Training set\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        validation_data=(X_test, y_test),\n",
        "                        shuffle=False,\n",
        "                        epochs=epochs, batch_size=batch_size,\n",
        "                        callbacks=[checkpoint])\n",
        "\n",
        "    # evaluate the model\n",
        "    _, train_acc = model.evaluate(X_train, y_train)\n",
        "    _, test_acc = model.evaluate(X_test, y_test)\n",
        "    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "    # plot loss during training\n",
        "    plt.title('Accuracy')\n",
        "    plt.plot(history.history['accuracy'], label='train')\n",
        "    plt.plot(history.history['val_accuracy'], label='test')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "# model = train_lstm(X_train, y_train_categorical,\n",
        "#                    X_test, y_test_categorical,\n",
        "#                    lstm_units=lstm_units, dropout_prob=dropout_prob, n_classes=num_classes,\n",
        "#                    epochs=epochs, batch_size=batch_size, lr=lr)\n",
        "model = train_lstm(X_train_scaled, y_train_scaled_categorical,\n",
        "                   X_test_scaled, y_test_scaled_categorical,\n",
        "                   lstm_units=lstm_units, dropout_prob=dropout_prob, n_classes=num_classes,\n",
        "                   epochs=epochs, batch_size=batch_size, lr=lr)\n",
        "model.save('/content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_latest')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_13 (LSTM)               (None, 128)               66560     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 2)                 66        \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 70,754\n",
            "Trainable params: 70,754\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5000\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6935 - accuracy: 0.4967 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.50109, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6932 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.50109\n",
            "Epoch 3/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6931 - accuracy: 0.5074 - val_loss: 0.6933 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.50109\n",
            "Epoch 4/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6928 - accuracy: 0.5168 - val_loss: 0.6933 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.50109\n",
            "Epoch 5/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6932 - accuracy: 0.5033 - val_loss: 0.6934 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.50109\n",
            "Epoch 6/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.6931 - accuracy: 0.5036 - val_loss: 0.6934 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.50109\n",
            "Epoch 7/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6930 - accuracy: 0.5127 - val_loss: 0.6933 - val_accuracy: 0.4881\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.50109\n",
            "Epoch 8/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6929 - accuracy: 0.5109 - val_loss: 0.6933 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.50109\n",
            "Epoch 9/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6930 - accuracy: 0.5047 - val_loss: 0.6933 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.50109\n",
            "Epoch 10/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6931 - accuracy: 0.5069 - val_loss: 0.6933 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.50109 to 0.50181, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6928 - accuracy: 0.5109 - val_loss: 0.6934 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.50181 to 0.50326, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6927 - accuracy: 0.5181 - val_loss: 0.6934 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.50326\n",
            "Epoch 13/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6928 - accuracy: 0.5136 - val_loss: 0.6935 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.50326\n",
            "Epoch 14/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6927 - accuracy: 0.5111 - val_loss: 0.6935 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.50326\n",
            "Epoch 15/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6927 - accuracy: 0.5185 - val_loss: 0.6936 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.50326\n",
            "Epoch 16/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6928 - accuracy: 0.5143 - val_loss: 0.6936 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.50326\n",
            "Epoch 17/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6929 - accuracy: 0.5071 - val_loss: 0.6937 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.50326\n",
            "Epoch 18/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6928 - accuracy: 0.5100 - val_loss: 0.6938 - val_accuracy: 0.4873\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.50326\n",
            "Epoch 19/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6927 - accuracy: 0.5114 - val_loss: 0.6938 - val_accuracy: 0.4873\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.50326\n",
            "Epoch 20/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6928 - accuracy: 0.5067 - val_loss: 0.6939 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.50326\n",
            "Epoch 21/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6927 - accuracy: 0.5149 - val_loss: 0.6939 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.50326\n",
            "Epoch 22/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6926 - accuracy: 0.5172 - val_loss: 0.6939 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.50326\n",
            "Epoch 23/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6926 - accuracy: 0.5192 - val_loss: 0.6939 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.50326\n",
            "Epoch 24/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.6923 - accuracy: 0.5150 - val_loss: 0.6939 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.50326\n",
            "Epoch 25/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6930 - accuracy: 0.5118 - val_loss: 0.6939 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.50326\n",
            "Epoch 26/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6926 - accuracy: 0.5147 - val_loss: 0.6940 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.50326\n",
            "Epoch 27/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6926 - accuracy: 0.5127 - val_loss: 0.6940 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.50326\n",
            "Epoch 28/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6925 - accuracy: 0.5139 - val_loss: 0.6940 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.50326\n",
            "Epoch 29/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6926 - accuracy: 0.5111 - val_loss: 0.6940 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.50326\n",
            "Epoch 30/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6927 - accuracy: 0.5085 - val_loss: 0.6940 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.50326\n",
            "Epoch 31/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6925 - accuracy: 0.5187 - val_loss: 0.6940 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.50326\n",
            "Epoch 32/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6925 - accuracy: 0.5176 - val_loss: 0.6940 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.50326\n",
            "Epoch 33/5000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.6926 - accuracy: 0.5149 - val_loss: 0.6940 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.50326\n",
            "Epoch 34/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6928 - accuracy: 0.5092 - val_loss: 0.6940 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.50326\n",
            "Epoch 35/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6926 - accuracy: 0.5168 - val_loss: 0.6940 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.50326\n",
            "Epoch 36/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6924 - accuracy: 0.5165 - val_loss: 0.6939 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.50326\n",
            "Epoch 37/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6927 - accuracy: 0.5129 - val_loss: 0.6939 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.50326\n",
            "Epoch 38/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6926 - accuracy: 0.5187 - val_loss: 0.6938 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.50326\n",
            "Epoch 39/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6927 - accuracy: 0.5178 - val_loss: 0.6938 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.50326\n",
            "Epoch 40/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6927 - accuracy: 0.5188 - val_loss: 0.6938 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.50326\n",
            "Epoch 41/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6925 - accuracy: 0.5145 - val_loss: 0.6938 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.50326\n",
            "Epoch 42/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6925 - accuracy: 0.5149 - val_loss: 0.6938 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.50326\n",
            "Epoch 43/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6927 - accuracy: 0.5159 - val_loss: 0.6938 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.50326\n",
            "Epoch 44/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6925 - accuracy: 0.5158 - val_loss: 0.6937 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.50326\n",
            "Epoch 45/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6927 - accuracy: 0.5156 - val_loss: 0.6937 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.50326\n",
            "Epoch 46/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6926 - accuracy: 0.5178 - val_loss: 0.6937 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.50326\n",
            "Epoch 47/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6925 - accuracy: 0.5154 - val_loss: 0.6938 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.50326\n",
            "Epoch 48/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6925 - accuracy: 0.5136 - val_loss: 0.6938 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.50326\n",
            "Epoch 49/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6924 - accuracy: 0.5165 - val_loss: 0.6938 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.50326\n",
            "Epoch 50/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6926 - accuracy: 0.5138 - val_loss: 0.6938 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.50326\n",
            "Epoch 51/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6924 - accuracy: 0.5187 - val_loss: 0.6939 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.50326\n",
            "Epoch 52/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6925 - accuracy: 0.5141 - val_loss: 0.6939 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.50326\n",
            "Epoch 53/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6924 - accuracy: 0.5232 - val_loss: 0.6939 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.50326\n",
            "Epoch 54/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6924 - accuracy: 0.5194 - val_loss: 0.6939 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.50326\n",
            "Epoch 55/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6925 - accuracy: 0.5183 - val_loss: 0.6939 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.50326\n",
            "Epoch 56/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6924 - accuracy: 0.5179 - val_loss: 0.6939 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.50326\n",
            "Epoch 57/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6926 - accuracy: 0.5127 - val_loss: 0.6938 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.50326\n",
            "Epoch 58/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6926 - accuracy: 0.5172 - val_loss: 0.6938 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.50326\n",
            "Epoch 59/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6926 - accuracy: 0.5150 - val_loss: 0.6938 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.50326\n",
            "Epoch 60/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6925 - accuracy: 0.5168 - val_loss: 0.6938 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.50326\n",
            "Epoch 61/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6924 - accuracy: 0.5207 - val_loss: 0.6938 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00061: val_accuracy improved from 0.50326 to 0.50543, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 62/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6925 - accuracy: 0.5205 - val_loss: 0.6939 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.50543\n",
            "Epoch 63/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6923 - accuracy: 0.5159 - val_loss: 0.6938 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.50543\n",
            "Epoch 64/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6924 - accuracy: 0.5165 - val_loss: 0.6938 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.50543\n",
            "Epoch 65/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6923 - accuracy: 0.5192 - val_loss: 0.6938 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.50543\n",
            "Epoch 66/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6922 - accuracy: 0.5228 - val_loss: 0.6939 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.50543\n",
            "Epoch 67/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6918 - accuracy: 0.5232 - val_loss: 0.6941 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 00067: val_accuracy improved from 0.50543 to 0.50905, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 68/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6923 - accuracy: 0.5194 - val_loss: 0.6942 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.50905\n",
            "Epoch 69/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6921 - accuracy: 0.5237 - val_loss: 0.6943 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.50905\n",
            "Epoch 70/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6924 - accuracy: 0.5221 - val_loss: 0.6941 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.50905\n",
            "Epoch 71/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6923 - accuracy: 0.5170 - val_loss: 0.6940 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.50905\n",
            "Epoch 72/5000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.6922 - accuracy: 0.5188 - val_loss: 0.6939 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 00072: val_accuracy improved from 0.50905 to 0.51340, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 73/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6921 - accuracy: 0.5194 - val_loss: 0.6938 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.51340\n",
            "Epoch 74/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6920 - accuracy: 0.5245 - val_loss: 0.6939 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.51340\n",
            "Epoch 75/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6921 - accuracy: 0.5149 - val_loss: 0.6939 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.51340\n",
            "Epoch 76/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6921 - accuracy: 0.5170 - val_loss: 0.6939 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.51340\n",
            "Epoch 77/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6919 - accuracy: 0.5196 - val_loss: 0.6938 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.51340\n",
            "Epoch 78/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6920 - accuracy: 0.5196 - val_loss: 0.6940 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.51340\n",
            "Epoch 79/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6922 - accuracy: 0.5165 - val_loss: 0.6942 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.51340\n",
            "Epoch 80/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6921 - accuracy: 0.5185 - val_loss: 0.6941 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.51340\n",
            "Epoch 81/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6921 - accuracy: 0.5185 - val_loss: 0.6942 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.51340\n",
            "Epoch 82/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6920 - accuracy: 0.5217 - val_loss: 0.6945 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.51340\n",
            "Epoch 83/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6921 - accuracy: 0.5165 - val_loss: 0.6947 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.51340\n",
            "Epoch 84/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6922 - accuracy: 0.5192 - val_loss: 0.6945 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.51340\n",
            "Epoch 85/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6921 - accuracy: 0.5225 - val_loss: 0.6944 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.51340\n",
            "Epoch 86/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6921 - accuracy: 0.5259 - val_loss: 0.6944 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.51340\n",
            "Epoch 87/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6921 - accuracy: 0.5214 - val_loss: 0.6948 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.51340\n",
            "Epoch 88/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6919 - accuracy: 0.5201 - val_loss: 0.6947 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.51340\n",
            "Epoch 89/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6920 - accuracy: 0.5208 - val_loss: 0.6945 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.51340\n",
            "Epoch 90/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6918 - accuracy: 0.5219 - val_loss: 0.6944 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.51340\n",
            "Epoch 91/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6918 - accuracy: 0.5197 - val_loss: 0.6944 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.51340\n",
            "Epoch 92/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6918 - accuracy: 0.5214 - val_loss: 0.6947 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.51340\n",
            "Epoch 93/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6918 - accuracy: 0.5223 - val_loss: 0.6946 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.51340\n",
            "Epoch 94/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6919 - accuracy: 0.5214 - val_loss: 0.6944 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.51340\n",
            "Epoch 95/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6918 - accuracy: 0.5237 - val_loss: 0.6945 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.51340\n",
            "Epoch 96/5000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.6918 - accuracy: 0.5207 - val_loss: 0.6949 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.51340\n",
            "Epoch 97/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6919 - accuracy: 0.5221 - val_loss: 0.6947 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.51340\n",
            "Epoch 98/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6917 - accuracy: 0.5228 - val_loss: 0.6945 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.51340\n",
            "Epoch 99/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6917 - accuracy: 0.5234 - val_loss: 0.6945 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.51340\n",
            "Epoch 100/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6916 - accuracy: 0.5217 - val_loss: 0.6949 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.51340\n",
            "Epoch 101/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6918 - accuracy: 0.5207 - val_loss: 0.6948 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.51340\n",
            "Epoch 102/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6916 - accuracy: 0.5246 - val_loss: 0.6946 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.51340\n",
            "Epoch 103/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6916 - accuracy: 0.5225 - val_loss: 0.6946 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.51340\n",
            "Epoch 104/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6917 - accuracy: 0.5208 - val_loss: 0.6947 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.51340\n",
            "Epoch 105/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6917 - accuracy: 0.5219 - val_loss: 0.6947 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.51340\n",
            "Epoch 106/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6917 - accuracy: 0.5232 - val_loss: 0.6945 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.51340\n",
            "Epoch 107/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6915 - accuracy: 0.5208 - val_loss: 0.6946 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.51340\n",
            "Epoch 108/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6915 - accuracy: 0.5181 - val_loss: 0.6947 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.51340\n",
            "Epoch 109/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6915 - accuracy: 0.5203 - val_loss: 0.6946 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.51340\n",
            "Epoch 110/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6915 - accuracy: 0.5226 - val_loss: 0.6947 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.51340\n",
            "Epoch 111/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6915 - accuracy: 0.5208 - val_loss: 0.6952 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.51340\n",
            "Epoch 112/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6915 - accuracy: 0.5263 - val_loss: 0.6946 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.51340\n",
            "Epoch 113/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6913 - accuracy: 0.5246 - val_loss: 0.6947 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.51340\n",
            "Epoch 114/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6913 - accuracy: 0.5219 - val_loss: 0.6948 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.51340\n",
            "Epoch 115/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6913 - accuracy: 0.5199 - val_loss: 0.6947 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.51340\n",
            "Epoch 116/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6910 - accuracy: 0.5252 - val_loss: 0.6947 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.51340\n",
            "Epoch 117/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6910 - accuracy: 0.5270 - val_loss: 0.6946 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.51340\n",
            "Epoch 118/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6909 - accuracy: 0.5281 - val_loss: 0.6950 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.51340\n",
            "Epoch 119/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6912 - accuracy: 0.5245 - val_loss: 0.6941 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.51340\n",
            "Epoch 120/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6915 - accuracy: 0.5248 - val_loss: 0.6943 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.51340\n",
            "Epoch 121/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6910 - accuracy: 0.5270 - val_loss: 0.6956 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.51340\n",
            "Epoch 122/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6915 - accuracy: 0.5181 - val_loss: 0.6943 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.51340\n",
            "Epoch 123/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6911 - accuracy: 0.5255 - val_loss: 0.6940 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.51340\n",
            "Epoch 124/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6915 - accuracy: 0.5236 - val_loss: 0.6943 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.51340\n",
            "Epoch 125/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6912 - accuracy: 0.5205 - val_loss: 0.6951 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.51340\n",
            "Epoch 126/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6909 - accuracy: 0.5243 - val_loss: 0.6953 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.51340\n",
            "Epoch 127/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6911 - accuracy: 0.5236 - val_loss: 0.6945 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.51340\n",
            "Epoch 128/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6908 - accuracy: 0.5263 - val_loss: 0.6942 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.51340\n",
            "Epoch 129/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6913 - accuracy: 0.5239 - val_loss: 0.6944 - val_accuracy: 0.4888\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.51340\n",
            "Epoch 130/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6908 - accuracy: 0.5281 - val_loss: 0.6952 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.51340\n",
            "Epoch 131/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6907 - accuracy: 0.5261 - val_loss: 0.6959 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.51340\n",
            "Epoch 132/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6912 - accuracy: 0.5284 - val_loss: 0.6952 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.51340\n",
            "Epoch 133/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6908 - accuracy: 0.5254 - val_loss: 0.6947 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.51340\n",
            "Epoch 134/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6909 - accuracy: 0.5283 - val_loss: 0.6948 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.51340\n",
            "Epoch 135/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6907 - accuracy: 0.5290 - val_loss: 0.6958 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.51340\n",
            "Epoch 136/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6906 - accuracy: 0.5263 - val_loss: 0.6963 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.51340\n",
            "Epoch 137/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6909 - accuracy: 0.5264 - val_loss: 0.6954 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.51340\n",
            "Epoch 138/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6908 - accuracy: 0.5270 - val_loss: 0.6950 - val_accuracy: 0.4881\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.51340\n",
            "Epoch 139/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6905 - accuracy: 0.5254 - val_loss: 0.6951 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.51340\n",
            "Epoch 140/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6904 - accuracy: 0.5257 - val_loss: 0.6956 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.51340\n",
            "Epoch 141/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6908 - accuracy: 0.5243 - val_loss: 0.6949 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.51340\n",
            "Epoch 142/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6904 - accuracy: 0.5274 - val_loss: 0.6948 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.51340\n",
            "Epoch 143/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6904 - accuracy: 0.5250 - val_loss: 0.6952 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.51340\n",
            "Epoch 144/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6903 - accuracy: 0.5297 - val_loss: 0.6963 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.51340\n",
            "Epoch 145/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6906 - accuracy: 0.5254 - val_loss: 0.6953 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.51340\n",
            "Epoch 146/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6903 - accuracy: 0.5270 - val_loss: 0.6953 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.51340\n",
            "Epoch 147/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6906 - accuracy: 0.5223 - val_loss: 0.6960 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.51340\n",
            "Epoch 148/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6902 - accuracy: 0.5221 - val_loss: 0.6966 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.51340\n",
            "Epoch 149/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6903 - accuracy: 0.5279 - val_loss: 0.6953 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.51340\n",
            "Epoch 150/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6901 - accuracy: 0.5272 - val_loss: 0.6950 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.51340\n",
            "Epoch 151/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6899 - accuracy: 0.5301 - val_loss: 0.6953 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.51340\n",
            "Epoch 152/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6901 - accuracy: 0.5266 - val_loss: 0.6958 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.51340\n",
            "Epoch 153/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6906 - accuracy: 0.5250 - val_loss: 0.6950 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.51340\n",
            "Epoch 154/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6900 - accuracy: 0.5261 - val_loss: 0.6949 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.51340\n",
            "Epoch 155/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6902 - accuracy: 0.5225 - val_loss: 0.6957 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.51340\n",
            "Epoch 156/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6895 - accuracy: 0.5259 - val_loss: 0.6967 - val_accuracy: 0.4888\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.51340\n",
            "Epoch 157/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6902 - accuracy: 0.5268 - val_loss: 0.6948 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.51340\n",
            "Epoch 158/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6899 - accuracy: 0.5283 - val_loss: 0.6948 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.51340\n",
            "Epoch 159/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6900 - accuracy: 0.5312 - val_loss: 0.6956 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.51340\n",
            "Epoch 160/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6903 - accuracy: 0.5263 - val_loss: 0.6954 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.51340\n",
            "Epoch 161/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6896 - accuracy: 0.5270 - val_loss: 0.6948 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.51340\n",
            "Epoch 162/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6898 - accuracy: 0.5279 - val_loss: 0.6952 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.51340\n",
            "Epoch 163/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6897 - accuracy: 0.5263 - val_loss: 0.6966 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.51340\n",
            "Epoch 164/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6904 - accuracy: 0.5279 - val_loss: 0.6949 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.51340\n",
            "Epoch 165/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6895 - accuracy: 0.5246 - val_loss: 0.6949 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.51340\n",
            "Epoch 166/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6896 - accuracy: 0.5239 - val_loss: 0.6955 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.51340\n",
            "Epoch 167/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6904 - accuracy: 0.5149 - val_loss: 0.6949 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.51340\n",
            "Epoch 168/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6897 - accuracy: 0.5214 - val_loss: 0.6946 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.51340\n",
            "Epoch 169/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6900 - accuracy: 0.5275 - val_loss: 0.6951 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.51340\n",
            "Epoch 170/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6896 - accuracy: 0.5234 - val_loss: 0.6954 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.51340\n",
            "Epoch 171/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6893 - accuracy: 0.5284 - val_loss: 0.6945 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.51340\n",
            "Epoch 172/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6894 - accuracy: 0.5337 - val_loss: 0.6944 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.51340\n",
            "Epoch 173/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6894 - accuracy: 0.5290 - val_loss: 0.6951 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.51340\n",
            "Epoch 174/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6899 - accuracy: 0.5226 - val_loss: 0.6942 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.51340\n",
            "Epoch 175/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6898 - accuracy: 0.5270 - val_loss: 0.6944 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.51340\n",
            "Epoch 176/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6891 - accuracy: 0.5324 - val_loss: 0.6954 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.51340\n",
            "Epoch 177/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6897 - accuracy: 0.5232 - val_loss: 0.6942 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.51340\n",
            "Epoch 178/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6893 - accuracy: 0.5297 - val_loss: 0.6944 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.51340\n",
            "Epoch 179/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6891 - accuracy: 0.5299 - val_loss: 0.6970 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.51340\n",
            "Epoch 180/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6902 - accuracy: 0.5237 - val_loss: 0.6941 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.51340\n",
            "Epoch 181/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6893 - accuracy: 0.5301 - val_loss: 0.6942 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.51340\n",
            "Epoch 182/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6902 - accuracy: 0.5342 - val_loss: 0.6943 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.51340\n",
            "Epoch 183/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6893 - accuracy: 0.5293 - val_loss: 0.6970 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.51340\n",
            "Epoch 184/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6901 - accuracy: 0.5286 - val_loss: 0.6945 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00184: val_accuracy did not improve from 0.51340\n",
            "Epoch 185/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6889 - accuracy: 0.5317 - val_loss: 0.6943 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 00185: val_accuracy did not improve from 0.51340\n",
            "Epoch 186/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6900 - accuracy: 0.5263 - val_loss: 0.6943 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.51340\n",
            "Epoch 187/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6894 - accuracy: 0.5257 - val_loss: 0.6961 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.51340\n",
            "Epoch 188/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6893 - accuracy: 0.5254 - val_loss: 0.6949 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.51340\n",
            "Epoch 189/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6886 - accuracy: 0.5286 - val_loss: 0.6941 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.51340\n",
            "Epoch 190/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6889 - accuracy: 0.5339 - val_loss: 0.6941 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.51340\n",
            "Epoch 191/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6889 - accuracy: 0.5333 - val_loss: 0.6951 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.51340\n",
            "Epoch 192/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6892 - accuracy: 0.5252 - val_loss: 0.6942 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.51340\n",
            "Epoch 193/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6885 - accuracy: 0.5306 - val_loss: 0.6938 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.51340\n",
            "Epoch 194/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6890 - accuracy: 0.5290 - val_loss: 0.6939 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.51340\n",
            "Epoch 195/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6884 - accuracy: 0.5274 - val_loss: 0.6946 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.51340\n",
            "Epoch 196/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6888 - accuracy: 0.5292 - val_loss: 0.6939 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.51340\n",
            "Epoch 197/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6889 - accuracy: 0.5306 - val_loss: 0.6941 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.51340\n",
            "Epoch 198/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.6884 - accuracy: 0.5335 - val_loss: 0.6956 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.51340\n",
            "Epoch 199/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6886 - accuracy: 0.5283 - val_loss: 0.6942 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.51340\n",
            "Epoch 200/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6889 - accuracy: 0.5295 - val_loss: 0.6943 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.51340\n",
            "Epoch 201/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6886 - accuracy: 0.5310 - val_loss: 0.6953 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 00201: val_accuracy did not improve from 0.51340\n",
            "Epoch 202/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6884 - accuracy: 0.5245 - val_loss: 0.6942 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 00202: val_accuracy did not improve from 0.51340\n",
            "Epoch 203/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6882 - accuracy: 0.5288 - val_loss: 0.6941 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00203: val_accuracy did not improve from 0.51340\n",
            "Epoch 204/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6887 - accuracy: 0.5304 - val_loss: 0.6956 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 00204: val_accuracy did not improve from 0.51340\n",
            "Epoch 205/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6886 - accuracy: 0.5299 - val_loss: 0.6941 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 00205: val_accuracy did not improve from 0.51340\n",
            "Epoch 206/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6884 - accuracy: 0.5321 - val_loss: 0.6940 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 00206: val_accuracy did not improve from 0.51340\n",
            "Epoch 207/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6881 - accuracy: 0.5330 - val_loss: 0.6949 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 00207: val_accuracy did not improve from 0.51340\n",
            "Epoch 208/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6883 - accuracy: 0.5210 - val_loss: 0.6940 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 00208: val_accuracy did not improve from 0.51340\n",
            "Epoch 209/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6879 - accuracy: 0.5337 - val_loss: 0.6944 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 00209: val_accuracy did not improve from 0.51340\n",
            "Epoch 210/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6881 - accuracy: 0.5266 - val_loss: 0.6955 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00210: val_accuracy did not improve from 0.51340\n",
            "Epoch 211/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6879 - accuracy: 0.5317 - val_loss: 0.6943 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 00211: val_accuracy did not improve from 0.51340\n",
            "Epoch 212/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6888 - accuracy: 0.5303 - val_loss: 0.6943 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 00212: val_accuracy did not improve from 0.51340\n",
            "Epoch 213/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6883 - accuracy: 0.5315 - val_loss: 0.6971 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 00213: val_accuracy did not improve from 0.51340\n",
            "Epoch 214/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6894 - accuracy: 0.5234 - val_loss: 0.6945 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 00214: val_accuracy did not improve from 0.51340\n",
            "Epoch 215/5000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.6884 - accuracy: 0.5339 - val_loss: 0.6946 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 00215: val_accuracy did not improve from 0.51340\n",
            "Epoch 216/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6889 - accuracy: 0.5304 - val_loss: 0.6951 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 00216: val_accuracy did not improve from 0.51340\n",
            "Epoch 217/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6875 - accuracy: 0.5261 - val_loss: 0.6963 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00217: val_accuracy did not improve from 0.51340\n",
            "Epoch 218/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6888 - accuracy: 0.5268 - val_loss: 0.6946 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00218: val_accuracy did not improve from 0.51340\n",
            "Epoch 219/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6884 - accuracy: 0.5324 - val_loss: 0.6945 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00219: val_accuracy did not improve from 0.51340\n",
            "Epoch 220/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6885 - accuracy: 0.5308 - val_loss: 0.6954 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 00220: val_accuracy did not improve from 0.51340\n",
            "Epoch 221/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6876 - accuracy: 0.5252 - val_loss: 0.6959 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 00221: val_accuracy did not improve from 0.51340\n",
            "Epoch 222/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6873 - accuracy: 0.5299 - val_loss: 0.6944 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 00222: val_accuracy did not improve from 0.51340\n",
            "Epoch 223/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6881 - accuracy: 0.5321 - val_loss: 0.6944 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 00223: val_accuracy did not improve from 0.51340\n",
            "Epoch 224/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6882 - accuracy: 0.5313 - val_loss: 0.6956 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 00224: val_accuracy did not improve from 0.51340\n",
            "Epoch 225/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6881 - accuracy: 0.5328 - val_loss: 0.6944 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00225: val_accuracy did not improve from 0.51340\n",
            "Epoch 226/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6879 - accuracy: 0.5299 - val_loss: 0.6945 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 00226: val_accuracy did not improve from 0.51340\n",
            "Epoch 227/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6877 - accuracy: 0.5373 - val_loss: 0.6957 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00227: val_accuracy did not improve from 0.51340\n",
            "Epoch 228/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6875 - accuracy: 0.5295 - val_loss: 0.6949 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00228: val_accuracy did not improve from 0.51340\n",
            "Epoch 229/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6874 - accuracy: 0.5313 - val_loss: 0.6947 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 00229: val_accuracy did not improve from 0.51340\n",
            "Epoch 230/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6873 - accuracy: 0.5324 - val_loss: 0.6951 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00230: val_accuracy did not improve from 0.51340\n",
            "Epoch 231/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6875 - accuracy: 0.5341 - val_loss: 0.6958 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00231: val_accuracy did not improve from 0.51340\n",
            "Epoch 232/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6872 - accuracy: 0.5332 - val_loss: 0.6950 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00232: val_accuracy did not improve from 0.51340\n",
            "Epoch 233/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6870 - accuracy: 0.5361 - val_loss: 0.6966 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 00233: val_accuracy did not improve from 0.51340\n",
            "Epoch 234/5000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.6877 - accuracy: 0.5283 - val_loss: 0.6950 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 00234: val_accuracy did not improve from 0.51340\n",
            "Epoch 235/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6869 - accuracy: 0.5315 - val_loss: 0.6957 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00235: val_accuracy did not improve from 0.51340\n",
            "Epoch 236/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6867 - accuracy: 0.5339 - val_loss: 0.6962 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 00236: val_accuracy did not improve from 0.51340\n",
            "Epoch 237/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6869 - accuracy: 0.5308 - val_loss: 0.6950 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 00237: val_accuracy did not improve from 0.51340\n",
            "Epoch 238/5000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.6876 - accuracy: 0.5351 - val_loss: 0.6954 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00238: val_accuracy did not improve from 0.51340\n",
            "Epoch 239/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6867 - accuracy: 0.5299 - val_loss: 0.6964 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00239: val_accuracy did not improve from 0.51340\n",
            "Epoch 240/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6872 - accuracy: 0.5274 - val_loss: 0.6951 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 00240: val_accuracy did not improve from 0.51340\n",
            "Epoch 241/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6881 - accuracy: 0.5303 - val_loss: 0.6957 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00241: val_accuracy did not improve from 0.51340\n",
            "Epoch 242/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6868 - accuracy: 0.5254 - val_loss: 0.7000 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 00242: val_accuracy did not improve from 0.51340\n",
            "Epoch 243/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6891 - accuracy: 0.5319 - val_loss: 0.6956 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 00243: val_accuracy did not improve from 0.51340\n",
            "Epoch 244/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6906 - accuracy: 0.5308 - val_loss: 0.6953 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00244: val_accuracy did not improve from 0.51340\n",
            "Epoch 245/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6922 - accuracy: 0.5228 - val_loss: 0.6953 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00245: val_accuracy did not improve from 0.51340\n",
            "Epoch 246/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6927 - accuracy: 0.5170 - val_loss: 0.6951 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00246: val_accuracy did not improve from 0.51340\n",
            "Epoch 247/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6921 - accuracy: 0.5203 - val_loss: 0.6949 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00247: val_accuracy did not improve from 0.51340\n",
            "Epoch 248/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6918 - accuracy: 0.5268 - val_loss: 0.6947 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 00248: val_accuracy did not improve from 0.51340\n",
            "Epoch 249/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6912 - accuracy: 0.5252 - val_loss: 0.6949 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00249: val_accuracy did not improve from 0.51340\n",
            "Epoch 250/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6907 - accuracy: 0.5264 - val_loss: 0.6956 - val_accuracy: 0.4852\n",
            "\n",
            "Epoch 00250: val_accuracy did not improve from 0.51340\n",
            "Epoch 251/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6910 - accuracy: 0.5219 - val_loss: 0.6959 - val_accuracy: 0.4873\n",
            "\n",
            "Epoch 00251: val_accuracy did not improve from 0.51340\n",
            "Epoch 252/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6915 - accuracy: 0.5248 - val_loss: 0.6951 - val_accuracy: 0.4852\n",
            "\n",
            "Epoch 00252: val_accuracy did not improve from 0.51340\n",
            "Epoch 253/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6910 - accuracy: 0.5250 - val_loss: 0.6943 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00253: val_accuracy did not improve from 0.51340\n",
            "Epoch 254/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6909 - accuracy: 0.5228 - val_loss: 0.6939 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00254: val_accuracy did not improve from 0.51340\n",
            "Epoch 255/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6904 - accuracy: 0.5270 - val_loss: 0.6937 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00255: val_accuracy did not improve from 0.51340\n",
            "Epoch 256/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6909 - accuracy: 0.5246 - val_loss: 0.6936 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 00256: val_accuracy did not improve from 0.51340\n",
            "Epoch 257/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6907 - accuracy: 0.5255 - val_loss: 0.6936 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00257: val_accuracy did not improve from 0.51340\n",
            "Epoch 258/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6905 - accuracy: 0.5245 - val_loss: 0.6936 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00258: val_accuracy did not improve from 0.51340\n",
            "Epoch 259/5000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.6904 - accuracy: 0.5272 - val_loss: 0.6936 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00259: val_accuracy did not improve from 0.51340\n",
            "Epoch 260/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6901 - accuracy: 0.5290 - val_loss: 0.6938 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00260: val_accuracy did not improve from 0.51340\n",
            "Epoch 261/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6900 - accuracy: 0.5264 - val_loss: 0.6938 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00261: val_accuracy did not improve from 0.51340\n",
            "Epoch 262/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6900 - accuracy: 0.5232 - val_loss: 0.6938 - val_accuracy: 0.4873\n",
            "\n",
            "Epoch 00262: val_accuracy did not improve from 0.51340\n",
            "Epoch 263/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6893 - accuracy: 0.5286 - val_loss: 0.6940 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00263: val_accuracy did not improve from 0.51340\n",
            "Epoch 264/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6895 - accuracy: 0.5306 - val_loss: 0.6941 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00264: val_accuracy did not improve from 0.51340\n",
            "Epoch 265/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6895 - accuracy: 0.5321 - val_loss: 0.6943 - val_accuracy: 0.4859\n",
            "\n",
            "Epoch 00265: val_accuracy did not improve from 0.51340\n",
            "Epoch 266/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6893 - accuracy: 0.5310 - val_loss: 0.6949 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 00266: val_accuracy did not improve from 0.51340\n",
            "Epoch 267/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6888 - accuracy: 0.5344 - val_loss: 0.6950 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 00267: val_accuracy did not improve from 0.51340\n",
            "Epoch 268/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6892 - accuracy: 0.5308 - val_loss: 0.6944 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00268: val_accuracy did not improve from 0.51340\n",
            "Epoch 269/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6889 - accuracy: 0.5312 - val_loss: 0.6943 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00269: val_accuracy did not improve from 0.51340\n",
            "Epoch 270/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6887 - accuracy: 0.5275 - val_loss: 0.6943 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00270: val_accuracy did not improve from 0.51340\n",
            "Epoch 271/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6885 - accuracy: 0.5339 - val_loss: 0.6946 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00271: val_accuracy did not improve from 0.51340\n",
            "Epoch 272/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6882 - accuracy: 0.5304 - val_loss: 0.6954 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00272: val_accuracy did not improve from 0.51340\n",
            "Epoch 273/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6885 - accuracy: 0.5308 - val_loss: 0.6958 - val_accuracy: 0.4866\n",
            "\n",
            "Epoch 00273: val_accuracy did not improve from 0.51340\n",
            "Epoch 274/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6887 - accuracy: 0.5286 - val_loss: 0.6961 - val_accuracy: 0.4852\n",
            "\n",
            "Epoch 00274: val_accuracy did not improve from 0.51340\n",
            "Epoch 275/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6881 - accuracy: 0.5272 - val_loss: 0.6963 - val_accuracy: 0.4830\n",
            "\n",
            "Epoch 00275: val_accuracy did not improve from 0.51340\n",
            "Epoch 276/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6884 - accuracy: 0.5264 - val_loss: 0.6960 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 00276: val_accuracy did not improve from 0.51340\n",
            "Epoch 277/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6875 - accuracy: 0.5324 - val_loss: 0.6959 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00277: val_accuracy did not improve from 0.51340\n",
            "Epoch 278/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6877 - accuracy: 0.5319 - val_loss: 0.6956 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00278: val_accuracy did not improve from 0.51340\n",
            "Epoch 279/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6875 - accuracy: 0.5306 - val_loss: 0.6955 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00279: val_accuracy did not improve from 0.51340\n",
            "Epoch 280/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6869 - accuracy: 0.5322 - val_loss: 0.6964 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00280: val_accuracy did not improve from 0.51340\n",
            "Epoch 281/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6868 - accuracy: 0.5335 - val_loss: 0.6987 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00281: val_accuracy did not improve from 0.51340\n",
            "Epoch 282/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6874 - accuracy: 0.5326 - val_loss: 0.6986 - val_accuracy: 0.4881\n",
            "\n",
            "Epoch 00282: val_accuracy did not improve from 0.51340\n",
            "Epoch 283/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6867 - accuracy: 0.5388 - val_loss: 0.6974 - val_accuracy: 0.4881\n",
            "\n",
            "Epoch 00283: val_accuracy did not improve from 0.51340\n",
            "Epoch 284/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6868 - accuracy: 0.5319 - val_loss: 0.6966 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00284: val_accuracy did not improve from 0.51340\n",
            "Epoch 285/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6871 - accuracy: 0.5339 - val_loss: 0.6957 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00285: val_accuracy did not improve from 0.51340\n",
            "Epoch 286/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6867 - accuracy: 0.5315 - val_loss: 0.6955 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00286: val_accuracy did not improve from 0.51340\n",
            "Epoch 287/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6867 - accuracy: 0.5364 - val_loss: 0.6956 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00287: val_accuracy did not improve from 0.51340\n",
            "Epoch 288/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6863 - accuracy: 0.5299 - val_loss: 0.6966 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00288: val_accuracy did not improve from 0.51340\n",
            "Epoch 289/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6861 - accuracy: 0.5341 - val_loss: 0.6967 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00289: val_accuracy did not improve from 0.51340\n",
            "Epoch 290/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6861 - accuracy: 0.5357 - val_loss: 0.6964 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00290: val_accuracy did not improve from 0.51340\n",
            "Epoch 291/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6865 - accuracy: 0.5337 - val_loss: 0.6959 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00291: val_accuracy did not improve from 0.51340\n",
            "Epoch 292/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6864 - accuracy: 0.5292 - val_loss: 0.6957 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00292: val_accuracy did not improve from 0.51340\n",
            "Epoch 293/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6867 - accuracy: 0.5277 - val_loss: 0.6958 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00293: val_accuracy did not improve from 0.51340\n",
            "Epoch 294/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6866 - accuracy: 0.5264 - val_loss: 0.6963 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00294: val_accuracy did not improve from 0.51340\n",
            "Epoch 295/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6853 - accuracy: 0.5386 - val_loss: 0.6973 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 00295: val_accuracy did not improve from 0.51340\n",
            "Epoch 296/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6865 - accuracy: 0.5351 - val_loss: 0.6957 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 00296: val_accuracy did not improve from 0.51340\n",
            "Epoch 297/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6852 - accuracy: 0.5346 - val_loss: 0.6961 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 00297: val_accuracy did not improve from 0.51340\n",
            "Epoch 298/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6860 - accuracy: 0.5310 - val_loss: 0.6961 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00298: val_accuracy did not improve from 0.51340\n",
            "Epoch 299/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6855 - accuracy: 0.5361 - val_loss: 0.6964 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00299: val_accuracy did not improve from 0.51340\n",
            "Epoch 300/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6859 - accuracy: 0.5371 - val_loss: 0.6984 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 00300: val_accuracy did not improve from 0.51340\n",
            "Epoch 301/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6920 - accuracy: 0.5368 - val_loss: 0.6963 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00301: val_accuracy did not improve from 0.51340\n",
            "Epoch 302/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6927 - accuracy: 0.5127 - val_loss: 0.6964 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 00302: val_accuracy did not improve from 0.51340\n",
            "Epoch 303/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6938 - accuracy: 0.5092 - val_loss: 0.6964 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 00303: val_accuracy did not improve from 0.51340\n",
            "Epoch 304/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6945 - accuracy: 0.5033 - val_loss: 0.6967 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00304: val_accuracy did not improve from 0.51340\n",
            "Epoch 305/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6939 - accuracy: 0.5056 - val_loss: 0.6966 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00305: val_accuracy did not improve from 0.51340\n",
            "Epoch 306/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6941 - accuracy: 0.5033 - val_loss: 0.6959 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00306: val_accuracy did not improve from 0.51340\n",
            "Epoch 307/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6938 - accuracy: 0.5020 - val_loss: 0.6954 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 00307: val_accuracy did not improve from 0.51340\n",
            "Epoch 308/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6933 - accuracy: 0.5092 - val_loss: 0.6950 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00308: val_accuracy did not improve from 0.51340\n",
            "Epoch 309/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6931 - accuracy: 0.5111 - val_loss: 0.6948 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00309: val_accuracy did not improve from 0.51340\n",
            "Epoch 310/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6928 - accuracy: 0.5103 - val_loss: 0.6946 - val_accuracy: 0.4866\n",
            "\n",
            "Epoch 00310: val_accuracy did not improve from 0.51340\n",
            "Epoch 311/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6928 - accuracy: 0.5098 - val_loss: 0.6946 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 00311: val_accuracy did not improve from 0.51340\n",
            "Epoch 312/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6926 - accuracy: 0.5105 - val_loss: 0.6946 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00312: val_accuracy did not improve from 0.51340\n",
            "Epoch 313/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6928 - accuracy: 0.5058 - val_loss: 0.6946 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00313: val_accuracy did not improve from 0.51340\n",
            "Epoch 314/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6929 - accuracy: 0.5042 - val_loss: 0.6946 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00314: val_accuracy did not improve from 0.51340\n",
            "Epoch 315/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6929 - accuracy: 0.5067 - val_loss: 0.6945 - val_accuracy: 0.4866\n",
            "\n",
            "Epoch 00315: val_accuracy did not improve from 0.51340\n",
            "Epoch 316/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6927 - accuracy: 0.5033 - val_loss: 0.6944 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 00316: val_accuracy did not improve from 0.51340\n",
            "Epoch 317/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6928 - accuracy: 0.5123 - val_loss: 0.6943 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 00317: val_accuracy did not improve from 0.51340\n",
            "Epoch 318/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6928 - accuracy: 0.5018 - val_loss: 0.6943 - val_accuracy: 0.4873\n",
            "\n",
            "Epoch 00318: val_accuracy did not improve from 0.51340\n",
            "Epoch 319/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6926 - accuracy: 0.5071 - val_loss: 0.6942 - val_accuracy: 0.4837\n",
            "\n",
            "Epoch 00319: val_accuracy did not improve from 0.51340\n",
            "Epoch 320/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6926 - accuracy: 0.5127 - val_loss: 0.6943 - val_accuracy: 0.4837\n",
            "\n",
            "Epoch 00320: val_accuracy did not improve from 0.51340\n",
            "Epoch 321/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6924 - accuracy: 0.5114 - val_loss: 0.6943 - val_accuracy: 0.4779\n",
            "\n",
            "Epoch 00321: val_accuracy did not improve from 0.51340\n",
            "Epoch 322/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6927 - accuracy: 0.5074 - val_loss: 0.6943 - val_accuracy: 0.4823\n",
            "\n",
            "Epoch 00322: val_accuracy did not improve from 0.51340\n",
            "Epoch 323/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6927 - accuracy: 0.5163 - val_loss: 0.6944 - val_accuracy: 0.4844\n",
            "\n",
            "Epoch 00323: val_accuracy did not improve from 0.51340\n",
            "Epoch 324/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6928 - accuracy: 0.5072 - val_loss: 0.6944 - val_accuracy: 0.4837\n",
            "\n",
            "Epoch 00324: val_accuracy did not improve from 0.51340\n",
            "Epoch 325/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6925 - accuracy: 0.5136 - val_loss: 0.6944 - val_accuracy: 0.4859\n",
            "\n",
            "Epoch 00325: val_accuracy did not improve from 0.51340\n",
            "Epoch 326/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6926 - accuracy: 0.5071 - val_loss: 0.6944 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 00326: val_accuracy did not improve from 0.51340\n",
            "Epoch 327/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6926 - accuracy: 0.5029 - val_loss: 0.6945 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00327: val_accuracy did not improve from 0.51340\n",
            "Epoch 328/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6927 - accuracy: 0.5080 - val_loss: 0.6945 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 00328: val_accuracy did not improve from 0.51340\n",
            "Epoch 329/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6925 - accuracy: 0.5130 - val_loss: 0.6945 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00329: val_accuracy did not improve from 0.51340\n",
            "Epoch 330/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6923 - accuracy: 0.5143 - val_loss: 0.6945 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00330: val_accuracy did not improve from 0.51340\n",
            "Epoch 331/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6926 - accuracy: 0.5156 - val_loss: 0.6945 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00331: val_accuracy did not improve from 0.51340\n",
            "Epoch 332/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6925 - accuracy: 0.5062 - val_loss: 0.6945 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 00332: val_accuracy did not improve from 0.51340\n",
            "Epoch 333/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6925 - accuracy: 0.5176 - val_loss: 0.6945 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00333: val_accuracy did not improve from 0.51340\n",
            "Epoch 334/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6923 - accuracy: 0.5183 - val_loss: 0.6945 - val_accuracy: 0.4866\n",
            "\n",
            "Epoch 00334: val_accuracy did not improve from 0.51340\n",
            "Epoch 335/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6924 - accuracy: 0.5199 - val_loss: 0.6946 - val_accuracy: 0.4888\n",
            "\n",
            "Epoch 00335: val_accuracy did not improve from 0.51340\n",
            "Epoch 336/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6928 - accuracy: 0.5100 - val_loss: 0.6946 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 00336: val_accuracy did not improve from 0.51340\n",
            "Epoch 337/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6925 - accuracy: 0.5134 - val_loss: 0.6945 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00337: val_accuracy did not improve from 0.51340\n",
            "Epoch 338/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6926 - accuracy: 0.5165 - val_loss: 0.6945 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00338: val_accuracy did not improve from 0.51340\n",
            "Epoch 339/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6925 - accuracy: 0.5172 - val_loss: 0.6945 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00339: val_accuracy did not improve from 0.51340\n",
            "Epoch 340/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6924 - accuracy: 0.5185 - val_loss: 0.6945 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00340: val_accuracy did not improve from 0.51340\n",
            "Epoch 341/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6924 - accuracy: 0.5172 - val_loss: 0.6945 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00341: val_accuracy did not improve from 0.51340\n",
            "Epoch 342/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6924 - accuracy: 0.5145 - val_loss: 0.6946 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00342: val_accuracy did not improve from 0.51340\n",
            "Epoch 343/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6926 - accuracy: 0.5134 - val_loss: 0.6947 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00343: val_accuracy did not improve from 0.51340\n",
            "Epoch 344/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6923 - accuracy: 0.5172 - val_loss: 0.6947 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00344: val_accuracy did not improve from 0.51340\n",
            "Epoch 345/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6925 - accuracy: 0.5130 - val_loss: 0.6948 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00345: val_accuracy did not improve from 0.51340\n",
            "Epoch 346/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6924 - accuracy: 0.5152 - val_loss: 0.6948 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00346: val_accuracy did not improve from 0.51340\n",
            "Epoch 347/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6926 - accuracy: 0.5158 - val_loss: 0.6948 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00347: val_accuracy did not improve from 0.51340\n",
            "Epoch 348/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6924 - accuracy: 0.5168 - val_loss: 0.6949 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00348: val_accuracy did not improve from 0.51340\n",
            "Epoch 349/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6928 - accuracy: 0.5100 - val_loss: 0.6948 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00349: val_accuracy did not improve from 0.51340\n",
            "Epoch 350/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6925 - accuracy: 0.5147 - val_loss: 0.6948 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00350: val_accuracy did not improve from 0.51340\n",
            "Epoch 351/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6923 - accuracy: 0.5132 - val_loss: 0.6947 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 00351: val_accuracy did not improve from 0.51340\n",
            "Epoch 352/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6923 - accuracy: 0.5165 - val_loss: 0.6947 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 00352: val_accuracy did not improve from 0.51340\n",
            "Epoch 353/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6923 - accuracy: 0.5183 - val_loss: 0.6947 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00353: val_accuracy did not improve from 0.51340\n",
            "Epoch 354/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6923 - accuracy: 0.5138 - val_loss: 0.6947 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00354: val_accuracy did not improve from 0.51340\n",
            "Epoch 355/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6924 - accuracy: 0.5239 - val_loss: 0.6947 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00355: val_accuracy did not improve from 0.51340\n",
            "Epoch 356/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6924 - accuracy: 0.5178 - val_loss: 0.6947 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00356: val_accuracy did not improve from 0.51340\n",
            "Epoch 357/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6922 - accuracy: 0.5143 - val_loss: 0.6948 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00357: val_accuracy did not improve from 0.51340\n",
            "Epoch 358/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6921 - accuracy: 0.5172 - val_loss: 0.6949 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00358: val_accuracy did not improve from 0.51340\n",
            "Epoch 359/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6921 - accuracy: 0.5183 - val_loss: 0.6950 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00359: val_accuracy did not improve from 0.51340\n",
            "Epoch 360/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6918 - accuracy: 0.5190 - val_loss: 0.6951 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 00360: val_accuracy did not improve from 0.51340\n",
            "Epoch 361/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6919 - accuracy: 0.5214 - val_loss: 0.6952 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00361: val_accuracy did not improve from 0.51340\n",
            "Epoch 362/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6918 - accuracy: 0.5197 - val_loss: 0.6952 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00362: val_accuracy did not improve from 0.51340\n",
            "Epoch 363/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6919 - accuracy: 0.5150 - val_loss: 0.6951 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00363: val_accuracy did not improve from 0.51340\n",
            "Epoch 364/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6916 - accuracy: 0.5228 - val_loss: 0.6950 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00364: val_accuracy did not improve from 0.51340\n",
            "Epoch 365/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6918 - accuracy: 0.5234 - val_loss: 0.6950 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 00365: val_accuracy did not improve from 0.51340\n",
            "Epoch 366/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6918 - accuracy: 0.5239 - val_loss: 0.6950 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00366: val_accuracy did not improve from 0.51340\n",
            "Epoch 367/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6917 - accuracy: 0.5205 - val_loss: 0.6952 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00367: val_accuracy did not improve from 0.51340\n",
            "Epoch 368/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6914 - accuracy: 0.5212 - val_loss: 0.6954 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00368: val_accuracy did not improve from 0.51340\n",
            "Epoch 369/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6913 - accuracy: 0.5138 - val_loss: 0.6954 - val_accuracy: 0.4866\n",
            "\n",
            "Epoch 00369: val_accuracy did not improve from 0.51340\n",
            "Epoch 370/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6912 - accuracy: 0.5203 - val_loss: 0.6949 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 00370: val_accuracy did not improve from 0.51340\n",
            "Epoch 371/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6910 - accuracy: 0.5214 - val_loss: 0.6947 - val_accuracy: 0.4837\n",
            "\n",
            "Epoch 00371: val_accuracy did not improve from 0.51340\n",
            "Epoch 372/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6910 - accuracy: 0.5263 - val_loss: 0.6952 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00372: val_accuracy did not improve from 0.51340\n",
            "Epoch 373/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6907 - accuracy: 0.5281 - val_loss: 0.6958 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 00373: val_accuracy did not improve from 0.51340\n",
            "Epoch 374/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6909 - accuracy: 0.5246 - val_loss: 0.6950 - val_accuracy: 0.4881\n",
            "\n",
            "Epoch 00374: val_accuracy did not improve from 0.51340\n",
            "Epoch 375/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6906 - accuracy: 0.5286 - val_loss: 0.6948 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00375: val_accuracy did not improve from 0.51340\n",
            "Epoch 376/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6905 - accuracy: 0.5257 - val_loss: 0.6953 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00376: val_accuracy did not improve from 0.51340\n",
            "Epoch 377/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6902 - accuracy: 0.5290 - val_loss: 0.6960 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00377: val_accuracy did not improve from 0.51340\n",
            "Epoch 378/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6904 - accuracy: 0.5275 - val_loss: 0.6958 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 00378: val_accuracy did not improve from 0.51340\n",
            "Epoch 379/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6900 - accuracy: 0.5364 - val_loss: 0.6957 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00379: val_accuracy did not improve from 0.51340\n",
            "Epoch 380/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6902 - accuracy: 0.5284 - val_loss: 0.6960 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00380: val_accuracy did not improve from 0.51340\n",
            "Epoch 381/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6903 - accuracy: 0.5250 - val_loss: 0.6965 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 00381: val_accuracy did not improve from 0.51340\n",
            "Epoch 382/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6901 - accuracy: 0.5326 - val_loss: 0.6963 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 00382: val_accuracy did not improve from 0.51340\n",
            "Epoch 383/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6899 - accuracy: 0.5299 - val_loss: 0.6964 - val_accuracy: 0.4888\n",
            "\n",
            "Epoch 00383: val_accuracy did not improve from 0.51340\n",
            "Epoch 384/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6898 - accuracy: 0.5333 - val_loss: 0.6966 - val_accuracy: 0.4866\n",
            "\n",
            "Epoch 00384: val_accuracy did not improve from 0.51340\n",
            "Epoch 385/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6897 - accuracy: 0.5279 - val_loss: 0.6966 - val_accuracy: 0.4888\n",
            "\n",
            "Epoch 00385: val_accuracy did not improve from 0.51340\n",
            "Epoch 386/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6898 - accuracy: 0.5297 - val_loss: 0.6969 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 00386: val_accuracy did not improve from 0.51340\n",
            "Epoch 387/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6895 - accuracy: 0.5297 - val_loss: 0.6974 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00387: val_accuracy did not improve from 0.51340\n",
            "Epoch 388/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6900 - accuracy: 0.5322 - val_loss: 0.6976 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 00388: val_accuracy did not improve from 0.51340\n",
            "Epoch 389/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6900 - accuracy: 0.5241 - val_loss: 0.6973 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00389: val_accuracy did not improve from 0.51340\n",
            "Epoch 390/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6896 - accuracy: 0.5330 - val_loss: 0.6968 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00390: val_accuracy did not improve from 0.51340\n",
            "Epoch 391/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6891 - accuracy: 0.5326 - val_loss: 0.6970 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00391: val_accuracy did not improve from 0.51340\n",
            "Epoch 392/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6890 - accuracy: 0.5281 - val_loss: 0.6978 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 00392: val_accuracy did not improve from 0.51340\n",
            "Epoch 393/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6890 - accuracy: 0.5301 - val_loss: 0.6978 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00393: val_accuracy did not improve from 0.51340\n",
            "Epoch 394/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6886 - accuracy: 0.5313 - val_loss: 0.6977 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 00394: val_accuracy did not improve from 0.51340\n",
            "Epoch 395/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6895 - accuracy: 0.5270 - val_loss: 0.6976 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00395: val_accuracy did not improve from 0.51340\n",
            "Epoch 396/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6891 - accuracy: 0.5332 - val_loss: 0.6989 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00396: val_accuracy did not improve from 0.51340\n",
            "Epoch 397/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6889 - accuracy: 0.5332 - val_loss: 0.6976 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 00397: val_accuracy did not improve from 0.51340\n",
            "Epoch 398/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6889 - accuracy: 0.5330 - val_loss: 0.6976 - val_accuracy: 0.4881\n",
            "\n",
            "Epoch 00398: val_accuracy did not improve from 0.51340\n",
            "Epoch 399/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6888 - accuracy: 0.5274 - val_loss: 0.6986 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00399: val_accuracy did not improve from 0.51340\n",
            "Epoch 400/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6887 - accuracy: 0.5319 - val_loss: 0.6991 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00400: val_accuracy did not improve from 0.51340\n",
            "Epoch 401/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6890 - accuracy: 0.5310 - val_loss: 0.6977 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00401: val_accuracy did not improve from 0.51340\n",
            "Epoch 402/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6884 - accuracy: 0.5330 - val_loss: 0.6976 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00402: val_accuracy did not improve from 0.51340\n",
            "Epoch 403/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6890 - accuracy: 0.5283 - val_loss: 0.6981 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00403: val_accuracy did not improve from 0.51340\n",
            "Epoch 404/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6884 - accuracy: 0.5306 - val_loss: 0.6981 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00404: val_accuracy did not improve from 0.51340\n",
            "Epoch 405/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6879 - accuracy: 0.5321 - val_loss: 0.6982 - val_accuracy: 0.4888\n",
            "\n",
            "Epoch 00405: val_accuracy did not improve from 0.51340\n",
            "Epoch 406/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6878 - accuracy: 0.5328 - val_loss: 0.6984 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00406: val_accuracy did not improve from 0.51340\n",
            "Epoch 407/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6877 - accuracy: 0.5359 - val_loss: 0.6981 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 00407: val_accuracy did not improve from 0.51340\n",
            "Epoch 408/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6879 - accuracy: 0.5310 - val_loss: 0.6979 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 00408: val_accuracy did not improve from 0.51340\n",
            "Epoch 409/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6874 - accuracy: 0.5319 - val_loss: 0.6982 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00409: val_accuracy did not improve from 0.51340\n",
            "Epoch 410/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6870 - accuracy: 0.5344 - val_loss: 0.6978 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 00410: val_accuracy did not improve from 0.51340\n",
            "Epoch 411/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6874 - accuracy: 0.5304 - val_loss: 0.6979 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 00411: val_accuracy did not improve from 0.51340\n",
            "Epoch 412/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6870 - accuracy: 0.5368 - val_loss: 0.6988 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 00412: val_accuracy did not improve from 0.51340\n",
            "Epoch 413/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6874 - accuracy: 0.5322 - val_loss: 0.6970 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 00413: val_accuracy did not improve from 0.51340\n",
            "Epoch 414/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6872 - accuracy: 0.5332 - val_loss: 0.6973 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00414: val_accuracy did not improve from 0.51340\n",
            "Epoch 415/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6864 - accuracy: 0.5389 - val_loss: 0.6980 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00415: val_accuracy did not improve from 0.51340\n",
            "Epoch 416/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6863 - accuracy: 0.5315 - val_loss: 0.6975 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00416: val_accuracy did not improve from 0.51340\n",
            "Epoch 417/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6862 - accuracy: 0.5330 - val_loss: 0.6978 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 00417: val_accuracy did not improve from 0.51340\n",
            "Epoch 418/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6865 - accuracy: 0.5326 - val_loss: 0.6962 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00418: val_accuracy did not improve from 0.51340\n",
            "Epoch 419/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6864 - accuracy: 0.5379 - val_loss: 0.6972 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 00419: val_accuracy did not improve from 0.51340\n",
            "Epoch 420/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6856 - accuracy: 0.5361 - val_loss: 0.6995 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00420: val_accuracy did not improve from 0.51340\n",
            "Epoch 421/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6866 - accuracy: 0.5335 - val_loss: 0.6963 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 00421: val_accuracy did not improve from 0.51340\n",
            "Epoch 422/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6883 - accuracy: 0.5342 - val_loss: 0.6959 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 00422: val_accuracy did not improve from 0.51340\n",
            "Epoch 423/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6867 - accuracy: 0.5373 - val_loss: 0.7019 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00423: val_accuracy did not improve from 0.51340\n",
            "Epoch 424/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6890 - accuracy: 0.5297 - val_loss: 0.6952 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 00424: val_accuracy did not improve from 0.51340\n",
            "Epoch 425/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6864 - accuracy: 0.5351 - val_loss: 0.6954 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 00425: val_accuracy did not improve from 0.51340\n",
            "Epoch 426/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6876 - accuracy: 0.5380 - val_loss: 0.6957 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 00426: val_accuracy did not improve from 0.51340\n",
            "Epoch 427/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6866 - accuracy: 0.5397 - val_loss: 0.6992 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 00427: val_accuracy improved from 0.51340 to 0.51557, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 428/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6865 - accuracy: 0.5319 - val_loss: 0.7000 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 00428: val_accuracy did not improve from 0.51557\n",
            "Epoch 429/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6862 - accuracy: 0.5388 - val_loss: 0.6955 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 00429: val_accuracy did not improve from 0.51557\n",
            "Epoch 430/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6854 - accuracy: 0.5335 - val_loss: 0.6949 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 00430: val_accuracy did not improve from 0.51557\n",
            "Epoch 431/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6857 - accuracy: 0.5391 - val_loss: 0.6952 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 00431: val_accuracy did not improve from 0.51557\n",
            "Epoch 432/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6853 - accuracy: 0.5389 - val_loss: 0.6988 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 00432: val_accuracy improved from 0.51557 to 0.51774, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 433/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6860 - accuracy: 0.5368 - val_loss: 0.6966 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 00433: val_accuracy did not improve from 0.51774\n",
            "Epoch 434/5000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.6850 - accuracy: 0.5355 - val_loss: 0.6953 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 00434: val_accuracy did not improve from 0.51774\n",
            "Epoch 435/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6853 - accuracy: 0.5366 - val_loss: 0.6953 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 00435: val_accuracy did not improve from 0.51774\n",
            "Epoch 436/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6851 - accuracy: 0.5368 - val_loss: 0.6981 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 00436: val_accuracy improved from 0.51774 to 0.52136, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 437/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6851 - accuracy: 0.5391 - val_loss: 0.6975 - val_accuracy: 0.5264\n",
            "\n",
            "Epoch 00437: val_accuracy improved from 0.52136 to 0.52643, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 438/5000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.6852 - accuracy: 0.5337 - val_loss: 0.6959 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 00438: val_accuracy did not improve from 0.52643\n",
            "Epoch 439/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6847 - accuracy: 0.5400 - val_loss: 0.6967 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 00439: val_accuracy did not improve from 0.52643\n",
            "Epoch 440/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6840 - accuracy: 0.5375 - val_loss: 0.6991 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 00440: val_accuracy did not improve from 0.52643\n",
            "Epoch 441/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6845 - accuracy: 0.5424 - val_loss: 0.6979 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00441: val_accuracy did not improve from 0.52643\n",
            "Epoch 442/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6828 - accuracy: 0.5460 - val_loss: 0.6971 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 00442: val_accuracy did not improve from 0.52643\n",
            "Epoch 443/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6839 - accuracy: 0.5428 - val_loss: 0.6978 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 00443: val_accuracy did not improve from 0.52643\n",
            "Epoch 444/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6832 - accuracy: 0.5368 - val_loss: 0.6991 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 00444: val_accuracy did not improve from 0.52643\n",
            "Epoch 445/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6835 - accuracy: 0.5433 - val_loss: 0.6982 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 00445: val_accuracy did not improve from 0.52643\n",
            "Epoch 446/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6837 - accuracy: 0.5409 - val_loss: 0.6985 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 00446: val_accuracy did not improve from 0.52643\n",
            "Epoch 447/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6829 - accuracy: 0.5404 - val_loss: 0.7010 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 00447: val_accuracy did not improve from 0.52643\n",
            "Epoch 448/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6831 - accuracy: 0.5402 - val_loss: 0.7007 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 00448: val_accuracy did not improve from 0.52643\n",
            "Epoch 449/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6835 - accuracy: 0.5380 - val_loss: 0.6991 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 00449: val_accuracy did not improve from 0.52643\n",
            "Epoch 450/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6827 - accuracy: 0.5476 - val_loss: 0.7030 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 00450: val_accuracy did not improve from 0.52643\n",
            "Epoch 451/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6827 - accuracy: 0.5386 - val_loss: 0.7046 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 00451: val_accuracy did not improve from 0.52643\n",
            "Epoch 452/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6837 - accuracy: 0.5415 - val_loss: 0.6976 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 00452: val_accuracy did not improve from 0.52643\n",
            "Epoch 453/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6832 - accuracy: 0.5444 - val_loss: 0.7016 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 00453: val_accuracy did not improve from 0.52643\n",
            "Epoch 454/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6829 - accuracy: 0.5395 - val_loss: 0.7048 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 00454: val_accuracy did not improve from 0.52643\n",
            "Epoch 455/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6833 - accuracy: 0.5404 - val_loss: 0.6976 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 00455: val_accuracy did not improve from 0.52643\n",
            "Epoch 456/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6849 - accuracy: 0.5408 - val_loss: 0.6980 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 00456: val_accuracy did not improve from 0.52643\n",
            "Epoch 457/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6835 - accuracy: 0.5411 - val_loss: 0.7018 - val_accuracy: 0.5206\n",
            "\n",
            "Epoch 00457: val_accuracy did not improve from 0.52643\n",
            "Epoch 458/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6819 - accuracy: 0.5464 - val_loss: 0.6972 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 00458: val_accuracy did not improve from 0.52643\n",
            "Epoch 459/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6829 - accuracy: 0.5408 - val_loss: 0.6983 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 00459: val_accuracy did not improve from 0.52643\n",
            "Epoch 460/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6820 - accuracy: 0.5435 - val_loss: 0.7023 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 00460: val_accuracy did not improve from 0.52643\n",
            "Epoch 461/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6825 - accuracy: 0.5442 - val_loss: 0.7007 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 00461: val_accuracy did not improve from 0.52643\n",
            "Epoch 462/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6812 - accuracy: 0.5446 - val_loss: 0.7040 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 00462: val_accuracy did not improve from 0.52643\n",
            "Epoch 463/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6811 - accuracy: 0.5424 - val_loss: 0.7089 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 00463: val_accuracy did not improve from 0.52643\n",
            "Epoch 464/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6824 - accuracy: 0.5442 - val_loss: 0.7097 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 00464: val_accuracy did not improve from 0.52643\n",
            "Epoch 465/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6815 - accuracy: 0.5505 - val_loss: 0.7255 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 00465: val_accuracy did not improve from 0.52643\n",
            "Epoch 466/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6825 - accuracy: 0.5464 - val_loss: 0.7053 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 00466: val_accuracy did not improve from 0.52643\n",
            "Epoch 467/5000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.6830 - accuracy: 0.5428 - val_loss: 0.7061 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 00467: val_accuracy did not improve from 0.52643\n",
            "Epoch 468/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6814 - accuracy: 0.5471 - val_loss: 0.7105 - val_accuracy: 0.5257\n",
            "\n",
            "Epoch 00468: val_accuracy did not improve from 0.52643\n",
            "Epoch 469/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6816 - accuracy: 0.5442 - val_loss: 0.7020 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 00469: val_accuracy did not improve from 0.52643\n",
            "Epoch 470/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6812 - accuracy: 0.5458 - val_loss: 0.7055 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 00470: val_accuracy did not improve from 0.52643\n",
            "Epoch 471/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6805 - accuracy: 0.5469 - val_loss: 0.7169 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 00471: val_accuracy did not improve from 0.52643\n",
            "Epoch 472/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6829 - accuracy: 0.5460 - val_loss: 0.7002 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 00472: val_accuracy did not improve from 0.52643\n",
            "Epoch 473/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6849 - accuracy: 0.5406 - val_loss: 0.6998 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 00473: val_accuracy did not improve from 0.52643\n",
            "Epoch 474/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6843 - accuracy: 0.5404 - val_loss: 0.7072 - val_accuracy: 0.5279\n",
            "\n",
            "Epoch 00474: val_accuracy improved from 0.52643 to 0.52788, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 475/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6833 - accuracy: 0.5400 - val_loss: 0.7060 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 00475: val_accuracy did not improve from 0.52788\n",
            "Epoch 476/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6820 - accuracy: 0.5424 - val_loss: 0.6997 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 00476: val_accuracy did not improve from 0.52788\n",
            "Epoch 477/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6822 - accuracy: 0.5458 - val_loss: 0.7025 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 00477: val_accuracy did not improve from 0.52788\n",
            "Epoch 478/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6833 - accuracy: 0.5395 - val_loss: 0.7158 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00478: val_accuracy improved from 0.52788 to 0.53222, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 479/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6815 - accuracy: 0.5462 - val_loss: 0.7115 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 00479: val_accuracy did not improve from 0.53222\n",
            "Epoch 480/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6805 - accuracy: 0.5516 - val_loss: 0.7037 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 00480: val_accuracy did not improve from 0.53222\n",
            "Epoch 481/5000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.6805 - accuracy: 0.5444 - val_loss: 0.7062 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 00481: val_accuracy did not improve from 0.53222\n",
            "Epoch 482/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6811 - accuracy: 0.5464 - val_loss: 0.7133 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 00482: val_accuracy did not improve from 0.53222\n",
            "Epoch 483/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6803 - accuracy: 0.5464 - val_loss: 0.7079 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 00483: val_accuracy did not improve from 0.53222\n",
            "Epoch 484/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6802 - accuracy: 0.5486 - val_loss: 0.7141 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 00484: val_accuracy did not improve from 0.53222\n",
            "Epoch 485/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6796 - accuracy: 0.5466 - val_loss: 0.7330 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 00485: val_accuracy did not improve from 0.53222\n",
            "Epoch 486/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6804 - accuracy: 0.5482 - val_loss: 0.7153 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 00486: val_accuracy did not improve from 0.53222\n",
            "Epoch 487/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6791 - accuracy: 0.5531 - val_loss: 0.7093 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 00487: val_accuracy did not improve from 0.53222\n",
            "Epoch 488/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6790 - accuracy: 0.5514 - val_loss: 0.7129 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 00488: val_accuracy did not improve from 0.53222\n",
            "Epoch 489/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6795 - accuracy: 0.5525 - val_loss: 0.7123 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 00489: val_accuracy did not improve from 0.53222\n",
            "Epoch 490/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6788 - accuracy: 0.5487 - val_loss: 0.7187 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 00490: val_accuracy did not improve from 0.53222\n",
            "Epoch 491/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6794 - accuracy: 0.5511 - val_loss: 0.7236 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 00491: val_accuracy did not improve from 0.53222\n",
            "Epoch 492/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6783 - accuracy: 0.5500 - val_loss: 0.7147 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 00492: val_accuracy did not improve from 0.53222\n",
            "Epoch 493/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6777 - accuracy: 0.5491 - val_loss: 0.7143 - val_accuracy: 0.5264\n",
            "\n",
            "Epoch 00493: val_accuracy did not improve from 0.53222\n",
            "Epoch 494/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6776 - accuracy: 0.5567 - val_loss: 0.7106 - val_accuracy: 0.5206\n",
            "\n",
            "Epoch 00494: val_accuracy did not improve from 0.53222\n",
            "Epoch 495/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6783 - accuracy: 0.5543 - val_loss: 0.7192 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 00495: val_accuracy did not improve from 0.53222\n",
            "Epoch 496/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6797 - accuracy: 0.5496 - val_loss: 0.7066 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 00496: val_accuracy did not improve from 0.53222\n",
            "Epoch 497/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6840 - accuracy: 0.5431 - val_loss: 0.7112 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 00497: val_accuracy did not improve from 0.53222\n",
            "Epoch 498/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6793 - accuracy: 0.5514 - val_loss: 0.7171 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 00498: val_accuracy did not improve from 0.53222\n",
            "Epoch 499/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6819 - accuracy: 0.5513 - val_loss: 0.7006 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 00499: val_accuracy did not improve from 0.53222\n",
            "Epoch 500/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6816 - accuracy: 0.5453 - val_loss: 0.7003 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 00500: val_accuracy did not improve from 0.53222\n",
            "Epoch 501/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6830 - accuracy: 0.5466 - val_loss: 0.7033 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 00501: val_accuracy did not improve from 0.53222\n",
            "Epoch 502/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6816 - accuracy: 0.5554 - val_loss: 0.7047 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 00502: val_accuracy did not improve from 0.53222\n",
            "Epoch 503/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6811 - accuracy: 0.5480 - val_loss: 0.6995 - val_accuracy: 0.5279\n",
            "\n",
            "Epoch 00503: val_accuracy did not improve from 0.53222\n",
            "Epoch 504/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6806 - accuracy: 0.5458 - val_loss: 0.6960 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 00504: val_accuracy did not improve from 0.53222\n",
            "Epoch 505/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6804 - accuracy: 0.5502 - val_loss: 0.6979 - val_accuracy: 0.5206\n",
            "\n",
            "Epoch 00505: val_accuracy did not improve from 0.53222\n",
            "Epoch 506/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6804 - accuracy: 0.5511 - val_loss: 0.6953 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 00506: val_accuracy did not improve from 0.53222\n",
            "Epoch 507/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6800 - accuracy: 0.5466 - val_loss: 0.6963 - val_accuracy: 0.5206\n",
            "\n",
            "Epoch 00507: val_accuracy did not improve from 0.53222\n",
            "Epoch 508/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6795 - accuracy: 0.5502 - val_loss: 0.6988 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 00508: val_accuracy did not improve from 0.53222\n",
            "Epoch 509/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6798 - accuracy: 0.5554 - val_loss: 0.6963 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 00509: val_accuracy did not improve from 0.53222\n",
            "Epoch 510/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6790 - accuracy: 0.5516 - val_loss: 0.6979 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 00510: val_accuracy did not improve from 0.53222\n",
            "Epoch 511/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6786 - accuracy: 0.5540 - val_loss: 0.7028 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 00511: val_accuracy did not improve from 0.53222\n",
            "Epoch 512/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6789 - accuracy: 0.5572 - val_loss: 0.7006 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 00512: val_accuracy did not improve from 0.53222\n",
            "Epoch 513/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6796 - accuracy: 0.5524 - val_loss: 0.6992 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 00513: val_accuracy did not improve from 0.53222\n",
            "Epoch 514/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6788 - accuracy: 0.5538 - val_loss: 0.6998 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 00514: val_accuracy did not improve from 0.53222\n",
            "Epoch 515/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6770 - accuracy: 0.5560 - val_loss: 0.7020 - val_accuracy: 0.5257\n",
            "\n",
            "Epoch 00515: val_accuracy did not improve from 0.53222\n",
            "Epoch 516/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6779 - accuracy: 0.5600 - val_loss: 0.6979 - val_accuracy: 0.5206\n",
            "\n",
            "Epoch 00516: val_accuracy did not improve from 0.53222\n",
            "Epoch 517/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6776 - accuracy: 0.5596 - val_loss: 0.7002 - val_accuracy: 0.5329\n",
            "\n",
            "Epoch 00517: val_accuracy improved from 0.53222 to 0.53295, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 518/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6765 - accuracy: 0.5569 - val_loss: 0.7035 - val_accuracy: 0.5344\n",
            "\n",
            "Epoch 00518: val_accuracy improved from 0.53295 to 0.53440, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 519/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6780 - accuracy: 0.5527 - val_loss: 0.6981 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 00519: val_accuracy did not improve from 0.53440\n",
            "Epoch 520/5000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.6785 - accuracy: 0.5522 - val_loss: 0.6993 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 00520: val_accuracy did not improve from 0.53440\n",
            "Epoch 521/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6778 - accuracy: 0.5549 - val_loss: 0.7094 - val_accuracy: 0.5344\n",
            "\n",
            "Epoch 00521: val_accuracy did not improve from 0.53440\n",
            "Epoch 522/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6794 - accuracy: 0.5565 - val_loss: 0.6985 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 00522: val_accuracy did not improve from 0.53440\n",
            "Epoch 523/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6827 - accuracy: 0.5455 - val_loss: 0.6973 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 00523: val_accuracy did not improve from 0.53440\n",
            "Epoch 524/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6802 - accuracy: 0.5438 - val_loss: 0.7054 - val_accuracy: 0.5308\n",
            "\n",
            "Epoch 00524: val_accuracy did not improve from 0.53440\n",
            "Epoch 525/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6816 - accuracy: 0.5453 - val_loss: 0.6970 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 00525: val_accuracy did not improve from 0.53440\n",
            "Epoch 526/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6779 - accuracy: 0.5469 - val_loss: 0.6955 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 00526: val_accuracy did not improve from 0.53440\n",
            "Epoch 527/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6798 - accuracy: 0.5482 - val_loss: 0.6980 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 00527: val_accuracy did not improve from 0.53440\n",
            "Epoch 528/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6789 - accuracy: 0.5527 - val_loss: 0.7020 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 00528: val_accuracy did not improve from 0.53440\n",
            "Epoch 529/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6783 - accuracy: 0.5524 - val_loss: 0.6984 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00529: val_accuracy did not improve from 0.53440\n",
            "Epoch 530/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6783 - accuracy: 0.5498 - val_loss: 0.7004 - val_accuracy: 0.5264\n",
            "\n",
            "Epoch 00530: val_accuracy did not improve from 0.53440\n",
            "Epoch 531/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6776 - accuracy: 0.5623 - val_loss: 0.7014 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 00531: val_accuracy did not improve from 0.53440\n",
            "Epoch 532/5000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.6778 - accuracy: 0.5569 - val_loss: 0.7019 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 00532: val_accuracy did not improve from 0.53440\n",
            "Epoch 533/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6765 - accuracy: 0.5612 - val_loss: 0.7090 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 00533: val_accuracy did not improve from 0.53440\n",
            "Epoch 534/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6770 - accuracy: 0.5614 - val_loss: 0.7028 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 00534: val_accuracy did not improve from 0.53440\n",
            "Epoch 535/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6764 - accuracy: 0.5580 - val_loss: 0.7026 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 00535: val_accuracy did not improve from 0.53440\n",
            "Epoch 536/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6769 - accuracy: 0.5576 - val_loss: 0.7015 - val_accuracy: 0.5243\n",
            "\n",
            "Epoch 00536: val_accuracy did not improve from 0.53440\n",
            "Epoch 537/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6766 - accuracy: 0.5563 - val_loss: 0.7010 - val_accuracy: 0.5279\n",
            "\n",
            "Epoch 00537: val_accuracy did not improve from 0.53440\n",
            "Epoch 538/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6758 - accuracy: 0.5533 - val_loss: 0.7029 - val_accuracy: 0.5351\n",
            "\n",
            "Epoch 00538: val_accuracy improved from 0.53440 to 0.53512, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 539/5000\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.6756 - accuracy: 0.5556 - val_loss: 0.7077 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 00539: val_accuracy did not improve from 0.53512\n",
            "Epoch 540/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.6760 - accuracy: 0.5558 - val_loss: 0.7004 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 00540: val_accuracy did not improve from 0.53512\n",
            "Epoch 541/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6775 - accuracy: 0.5551 - val_loss: 0.7073 - val_accuracy: 0.5301\n",
            "\n",
            "Epoch 00541: val_accuracy did not improve from 0.53512\n",
            "Epoch 542/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6747 - accuracy: 0.5601 - val_loss: 0.7191 - val_accuracy: 0.5337\n",
            "\n",
            "Epoch 00542: val_accuracy did not improve from 0.53512\n",
            "Epoch 543/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6782 - accuracy: 0.5480 - val_loss: 0.6982 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 00543: val_accuracy did not improve from 0.53512\n",
            "Epoch 544/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6818 - accuracy: 0.5524 - val_loss: 0.6986 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 00544: val_accuracy did not improve from 0.53512\n",
            "Epoch 545/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6832 - accuracy: 0.5473 - val_loss: 0.7011 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 00545: val_accuracy did not improve from 0.53512\n",
            "Epoch 546/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6783 - accuracy: 0.5571 - val_loss: 0.7098 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 00546: val_accuracy did not improve from 0.53512\n",
            "Epoch 547/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6826 - accuracy: 0.5458 - val_loss: 0.7027 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 00547: val_accuracy did not improve from 0.53512\n",
            "Epoch 548/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6766 - accuracy: 0.5518 - val_loss: 0.7013 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 00548: val_accuracy did not improve from 0.53512\n",
            "Epoch 549/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6787 - accuracy: 0.5473 - val_loss: 0.7026 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 00549: val_accuracy did not improve from 0.53512\n",
            "Epoch 550/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6762 - accuracy: 0.5545 - val_loss: 0.7118 - val_accuracy: 0.5301\n",
            "\n",
            "Epoch 00550: val_accuracy did not improve from 0.53512\n",
            "Epoch 551/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6796 - accuracy: 0.5583 - val_loss: 0.6984 - val_accuracy: 0.5206\n",
            "\n",
            "Epoch 00551: val_accuracy did not improve from 0.53512\n",
            "Epoch 552/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6750 - accuracy: 0.5549 - val_loss: 0.6975 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 00552: val_accuracy did not improve from 0.53512\n",
            "Epoch 553/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6767 - accuracy: 0.5567 - val_loss: 0.7014 - val_accuracy: 0.5286\n",
            "\n",
            "Epoch 00553: val_accuracy did not improve from 0.53512\n",
            "Epoch 554/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6758 - accuracy: 0.5562 - val_loss: 0.7043 - val_accuracy: 0.5373\n",
            "\n",
            "Epoch 00554: val_accuracy improved from 0.53512 to 0.53729, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 555/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6760 - accuracy: 0.5576 - val_loss: 0.6982 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 00555: val_accuracy did not improve from 0.53729\n",
            "Epoch 556/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6750 - accuracy: 0.5605 - val_loss: 0.6992 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 00556: val_accuracy did not improve from 0.53729\n",
            "Epoch 557/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6759 - accuracy: 0.5542 - val_loss: 0.7017 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 00557: val_accuracy did not improve from 0.53729\n",
            "Epoch 558/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6756 - accuracy: 0.5574 - val_loss: 0.7032 - val_accuracy: 0.5358\n",
            "\n",
            "Epoch 00558: val_accuracy did not improve from 0.53729\n",
            "Epoch 559/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6757 - accuracy: 0.5596 - val_loss: 0.7034 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00559: val_accuracy did not improve from 0.53729\n",
            "Epoch 560/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6759 - accuracy: 0.5647 - val_loss: 0.6989 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 00560: val_accuracy did not improve from 0.53729\n",
            "Epoch 561/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6759 - accuracy: 0.5643 - val_loss: 0.7001 - val_accuracy: 0.5329\n",
            "\n",
            "Epoch 00561: val_accuracy did not improve from 0.53729\n",
            "Epoch 562/5000\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.6746 - accuracy: 0.5647 - val_loss: 0.7059 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 00562: val_accuracy did not improve from 0.53729\n",
            "Epoch 563/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6765 - accuracy: 0.5572 - val_loss: 0.6998 - val_accuracy: 0.5351\n",
            "\n",
            "Epoch 00563: val_accuracy did not improve from 0.53729\n",
            "Epoch 564/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6768 - accuracy: 0.5618 - val_loss: 0.7036 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 00564: val_accuracy did not improve from 0.53729\n",
            "Epoch 565/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6755 - accuracy: 0.5591 - val_loss: 0.7121 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 00565: val_accuracy did not improve from 0.53729\n",
            "Epoch 566/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6769 - accuracy: 0.5556 - val_loss: 0.7005 - val_accuracy: 0.5264\n",
            "\n",
            "Epoch 00566: val_accuracy did not improve from 0.53729\n",
            "Epoch 567/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6772 - accuracy: 0.5533 - val_loss: 0.6998 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 00567: val_accuracy did not improve from 0.53729\n",
            "Epoch 568/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6759 - accuracy: 0.5629 - val_loss: 0.7036 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 00568: val_accuracy did not improve from 0.53729\n",
            "Epoch 569/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6763 - accuracy: 0.5574 - val_loss: 0.7034 - val_accuracy: 0.5286\n",
            "\n",
            "Epoch 00569: val_accuracy did not improve from 0.53729\n",
            "Epoch 570/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6741 - accuracy: 0.5641 - val_loss: 0.7048 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 00570: val_accuracy did not improve from 0.53729\n",
            "Epoch 571/5000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.6752 - accuracy: 0.5553 - val_loss: 0.7084 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 00571: val_accuracy did not improve from 0.53729\n",
            "Epoch 572/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6773 - accuracy: 0.5580 - val_loss: 0.6980 - val_accuracy: 0.5293\n",
            "\n",
            "Epoch 00572: val_accuracy did not improve from 0.53729\n",
            "Epoch 573/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6765 - accuracy: 0.5654 - val_loss: 0.6986 - val_accuracy: 0.5272\n",
            "\n",
            "Epoch 00573: val_accuracy did not improve from 0.53729\n",
            "Epoch 574/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6761 - accuracy: 0.5578 - val_loss: 0.7151 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00574: val_accuracy did not improve from 0.53729\n",
            "Epoch 575/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6767 - accuracy: 0.5592 - val_loss: 0.7033 - val_accuracy: 0.5337\n",
            "\n",
            "Epoch 00575: val_accuracy did not improve from 0.53729\n",
            "Epoch 576/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6729 - accuracy: 0.5634 - val_loss: 0.7023 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 00576: val_accuracy did not improve from 0.53729\n",
            "Epoch 577/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6754 - accuracy: 0.5560 - val_loss: 0.7043 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 00577: val_accuracy did not improve from 0.53729\n",
            "Epoch 578/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6749 - accuracy: 0.5583 - val_loss: 0.7052 - val_accuracy: 0.5257\n",
            "\n",
            "Epoch 00578: val_accuracy did not improve from 0.53729\n",
            "Epoch 579/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6733 - accuracy: 0.5580 - val_loss: 0.7018 - val_accuracy: 0.5337\n",
            "\n",
            "Epoch 00579: val_accuracy did not improve from 0.53729\n",
            "Epoch 580/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6735 - accuracy: 0.5592 - val_loss: 0.7035 - val_accuracy: 0.5329\n",
            "\n",
            "Epoch 00580: val_accuracy did not improve from 0.53729\n",
            "Epoch 581/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6733 - accuracy: 0.5656 - val_loss: 0.7036 - val_accuracy: 0.5279\n",
            "\n",
            "Epoch 00581: val_accuracy did not improve from 0.53729\n",
            "Epoch 582/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6729 - accuracy: 0.5620 - val_loss: 0.7075 - val_accuracy: 0.5293\n",
            "\n",
            "Epoch 00582: val_accuracy did not improve from 0.53729\n",
            "Epoch 583/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6731 - accuracy: 0.5591 - val_loss: 0.7033 - val_accuracy: 0.5293\n",
            "\n",
            "Epoch 00583: val_accuracy did not improve from 0.53729\n",
            "Epoch 584/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6718 - accuracy: 0.5663 - val_loss: 0.7046 - val_accuracy: 0.5351\n",
            "\n",
            "Epoch 00584: val_accuracy did not improve from 0.53729\n",
            "Epoch 585/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6726 - accuracy: 0.5643 - val_loss: 0.7028 - val_accuracy: 0.5308\n",
            "\n",
            "Epoch 00585: val_accuracy did not improve from 0.53729\n",
            "Epoch 586/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6743 - accuracy: 0.5632 - val_loss: 0.7025 - val_accuracy: 0.5373\n",
            "\n",
            "Epoch 00586: val_accuracy did not improve from 0.53729\n",
            "Epoch 587/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6732 - accuracy: 0.5620 - val_loss: 0.7014 - val_accuracy: 0.5308\n",
            "\n",
            "Epoch 00587: val_accuracy did not improve from 0.53729\n",
            "Epoch 588/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6715 - accuracy: 0.5665 - val_loss: 0.7024 - val_accuracy: 0.5344\n",
            "\n",
            "Epoch 00588: val_accuracy did not improve from 0.53729\n",
            "Epoch 589/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6729 - accuracy: 0.5649 - val_loss: 0.7001 - val_accuracy: 0.5366\n",
            "\n",
            "Epoch 00589: val_accuracy did not improve from 0.53729\n",
            "Epoch 590/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6713 - accuracy: 0.5647 - val_loss: 0.7031 - val_accuracy: 0.5293\n",
            "\n",
            "Epoch 00590: val_accuracy did not improve from 0.53729\n",
            "Epoch 591/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6724 - accuracy: 0.5683 - val_loss: 0.6980 - val_accuracy: 0.5344\n",
            "\n",
            "Epoch 00591: val_accuracy did not improve from 0.53729\n",
            "Epoch 592/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6734 - accuracy: 0.5620 - val_loss: 0.7063 - val_accuracy: 0.5206\n",
            "\n",
            "Epoch 00592: val_accuracy did not improve from 0.53729\n",
            "Epoch 593/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6728 - accuracy: 0.5639 - val_loss: 0.7076 - val_accuracy: 0.5351\n",
            "\n",
            "Epoch 00593: val_accuracy did not improve from 0.53729\n",
            "Epoch 594/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6767 - accuracy: 0.5545 - val_loss: 0.6984 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 00594: val_accuracy did not improve from 0.53729\n",
            "Epoch 595/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6748 - accuracy: 0.5614 - val_loss: 0.7010 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 00595: val_accuracy did not improve from 0.53729\n",
            "Epoch 596/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6749 - accuracy: 0.5609 - val_loss: 0.6969 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 00596: val_accuracy did not improve from 0.53729\n",
            "Epoch 597/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6736 - accuracy: 0.5565 - val_loss: 0.7024 - val_accuracy: 0.5308\n",
            "\n",
            "Epoch 00597: val_accuracy did not improve from 0.53729\n",
            "Epoch 598/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6745 - accuracy: 0.5565 - val_loss: 0.6996 - val_accuracy: 0.5272\n",
            "\n",
            "Epoch 00598: val_accuracy did not improve from 0.53729\n",
            "Epoch 599/5000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.6719 - accuracy: 0.5676 - val_loss: 0.6995 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 00599: val_accuracy did not improve from 0.53729\n",
            "Epoch 600/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6725 - accuracy: 0.5716 - val_loss: 0.7025 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 00600: val_accuracy did not improve from 0.53729\n",
            "Epoch 601/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6725 - accuracy: 0.5667 - val_loss: 0.6988 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 00601: val_accuracy did not improve from 0.53729\n",
            "Epoch 602/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6735 - accuracy: 0.5652 - val_loss: 0.7057 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 00602: val_accuracy did not improve from 0.53729\n",
            "Epoch 603/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6731 - accuracy: 0.5705 - val_loss: 0.7042 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 00603: val_accuracy did not improve from 0.53729\n",
            "Epoch 604/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6710 - accuracy: 0.5721 - val_loss: 0.7062 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 00604: val_accuracy did not improve from 0.53729\n",
            "Epoch 605/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6700 - accuracy: 0.5716 - val_loss: 0.7065 - val_accuracy: 0.5308\n",
            "\n",
            "Epoch 00605: val_accuracy did not improve from 0.53729\n",
            "Epoch 606/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6715 - accuracy: 0.5676 - val_loss: 0.6983 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 00606: val_accuracy did not improve from 0.53729\n",
            "Epoch 607/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6756 - accuracy: 0.5574 - val_loss: 0.7037 - val_accuracy: 0.5366\n",
            "\n",
            "Epoch 00607: val_accuracy did not improve from 0.53729\n",
            "Epoch 608/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6685 - accuracy: 0.5725 - val_loss: 0.7076 - val_accuracy: 0.5264\n",
            "\n",
            "Epoch 00608: val_accuracy did not improve from 0.53729\n",
            "Epoch 609/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6723 - accuracy: 0.5607 - val_loss: 0.6976 - val_accuracy: 0.5279\n",
            "\n",
            "Epoch 00609: val_accuracy did not improve from 0.53729\n",
            "Epoch 610/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6753 - accuracy: 0.5618 - val_loss: 0.7005 - val_accuracy: 0.5373\n",
            "\n",
            "Epoch 00610: val_accuracy did not improve from 0.53729\n",
            "Epoch 611/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6725 - accuracy: 0.5643 - val_loss: 0.7238 - val_accuracy: 0.5402\n",
            "\n",
            "Epoch 00611: val_accuracy improved from 0.53729 to 0.54019, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 612/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6807 - accuracy: 0.5496 - val_loss: 0.7054 - val_accuracy: 0.5279\n",
            "\n",
            "Epoch 00612: val_accuracy did not improve from 0.54019\n",
            "Epoch 613/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6763 - accuracy: 0.5600 - val_loss: 0.7031 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 00613: val_accuracy did not improve from 0.54019\n",
            "Epoch 614/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6801 - accuracy: 0.5556 - val_loss: 0.7026 - val_accuracy: 0.5286\n",
            "\n",
            "Epoch 00614: val_accuracy did not improve from 0.54019\n",
            "Epoch 615/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6731 - accuracy: 0.5585 - val_loss: 0.7164 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 00615: val_accuracy did not improve from 0.54019\n",
            "Epoch 616/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6828 - accuracy: 0.5467 - val_loss: 0.7061 - val_accuracy: 0.5257\n",
            "\n",
            "Epoch 00616: val_accuracy did not improve from 0.54019\n",
            "Epoch 617/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6722 - accuracy: 0.5688 - val_loss: 0.7071 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 00617: val_accuracy did not improve from 0.54019\n",
            "Epoch 618/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6756 - accuracy: 0.5596 - val_loss: 0.7100 - val_accuracy: 0.5279\n",
            "\n",
            "Epoch 00618: val_accuracy did not improve from 0.54019\n",
            "Epoch 619/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6788 - accuracy: 0.5478 - val_loss: 0.7087 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00619: val_accuracy did not improve from 0.54019\n",
            "Epoch 620/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6771 - accuracy: 0.5545 - val_loss: 0.7089 - val_accuracy: 0.5272\n",
            "\n",
            "Epoch 00620: val_accuracy did not improve from 0.54019\n",
            "Epoch 621/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6736 - accuracy: 0.5665 - val_loss: 0.7112 - val_accuracy: 0.5279\n",
            "\n",
            "Epoch 00621: val_accuracy did not improve from 0.54019\n",
            "Epoch 622/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6765 - accuracy: 0.5607 - val_loss: 0.7015 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00622: val_accuracy did not improve from 0.54019\n",
            "Epoch 623/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6722 - accuracy: 0.5696 - val_loss: 0.6996 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00623: val_accuracy did not improve from 0.54019\n",
            "Epoch 624/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6734 - accuracy: 0.5634 - val_loss: 0.7043 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 00624: val_accuracy did not improve from 0.54019\n",
            "Epoch 625/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6740 - accuracy: 0.5611 - val_loss: 0.7185 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 00625: val_accuracy did not improve from 0.54019\n",
            "Epoch 626/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6735 - accuracy: 0.5639 - val_loss: 0.7193 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 00626: val_accuracy did not improve from 0.54019\n",
            "Epoch 627/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6726 - accuracy: 0.5621 - val_loss: 0.7064 - val_accuracy: 0.5279\n",
            "\n",
            "Epoch 00627: val_accuracy did not improve from 0.54019\n",
            "Epoch 628/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6709 - accuracy: 0.5629 - val_loss: 0.7055 - val_accuracy: 0.5264\n",
            "\n",
            "Epoch 00628: val_accuracy did not improve from 0.54019\n",
            "Epoch 629/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6700 - accuracy: 0.5676 - val_loss: 0.7172 - val_accuracy: 0.5337\n",
            "\n",
            "Epoch 00629: val_accuracy did not improve from 0.54019\n",
            "Epoch 630/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6716 - accuracy: 0.5656 - val_loss: 0.7244 - val_accuracy: 0.5337\n",
            "\n",
            "Epoch 00630: val_accuracy did not improve from 0.54019\n",
            "Epoch 631/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6712 - accuracy: 0.5668 - val_loss: 0.7102 - val_accuracy: 0.5286\n",
            "\n",
            "Epoch 00631: val_accuracy did not improve from 0.54019\n",
            "Epoch 632/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6688 - accuracy: 0.5683 - val_loss: 0.7086 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00632: val_accuracy did not improve from 0.54019\n",
            "Epoch 633/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6685 - accuracy: 0.5726 - val_loss: 0.7247 - val_accuracy: 0.5337\n",
            "\n",
            "Epoch 00633: val_accuracy did not improve from 0.54019\n",
            "Epoch 634/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6704 - accuracy: 0.5688 - val_loss: 0.7272 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00634: val_accuracy did not improve from 0.54019\n",
            "Epoch 635/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6691 - accuracy: 0.5685 - val_loss: 0.7276 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 00635: val_accuracy did not improve from 0.54019\n",
            "Epoch 636/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6698 - accuracy: 0.5659 - val_loss: 0.7301 - val_accuracy: 0.5366\n",
            "\n",
            "Epoch 00636: val_accuracy did not improve from 0.54019\n",
            "Epoch 637/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6679 - accuracy: 0.5694 - val_loss: 0.7482 - val_accuracy: 0.5380\n",
            "\n",
            "Epoch 00637: val_accuracy did not improve from 0.54019\n",
            "Epoch 638/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6685 - accuracy: 0.5716 - val_loss: 0.7404 - val_accuracy: 0.5329\n",
            "\n",
            "Epoch 00638: val_accuracy did not improve from 0.54019\n",
            "Epoch 639/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6687 - accuracy: 0.5683 - val_loss: 0.7251 - val_accuracy: 0.5337\n",
            "\n",
            "Epoch 00639: val_accuracy did not improve from 0.54019\n",
            "Epoch 640/5000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.6671 - accuracy: 0.5745 - val_loss: 0.7264 - val_accuracy: 0.5329\n",
            "\n",
            "Epoch 00640: val_accuracy did not improve from 0.54019\n",
            "Epoch 641/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6660 - accuracy: 0.5766 - val_loss: 0.7353 - val_accuracy: 0.5257\n",
            "\n",
            "Epoch 00641: val_accuracy did not improve from 0.54019\n",
            "Epoch 642/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6687 - accuracy: 0.5678 - val_loss: 0.7416 - val_accuracy: 0.5308\n",
            "\n",
            "Epoch 00642: val_accuracy did not improve from 0.54019\n",
            "Epoch 643/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6672 - accuracy: 0.5781 - val_loss: 0.7317 - val_accuracy: 0.5395\n",
            "\n",
            "Epoch 00643: val_accuracy did not improve from 0.54019\n",
            "Epoch 644/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6693 - accuracy: 0.5732 - val_loss: 0.7357 - val_accuracy: 0.5431\n",
            "\n",
            "Epoch 00644: val_accuracy improved from 0.54019 to 0.54308, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 645/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6693 - accuracy: 0.5710 - val_loss: 0.7437 - val_accuracy: 0.5395\n",
            "\n",
            "Epoch 00645: val_accuracy did not improve from 0.54308\n",
            "Epoch 646/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6680 - accuracy: 0.5750 - val_loss: 0.7247 - val_accuracy: 0.5351\n",
            "\n",
            "Epoch 00646: val_accuracy did not improve from 0.54308\n",
            "Epoch 647/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6701 - accuracy: 0.5707 - val_loss: 0.7048 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 00647: val_accuracy did not improve from 0.54308\n",
            "Epoch 648/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6694 - accuracy: 0.5694 - val_loss: 0.7082 - val_accuracy: 0.5286\n",
            "\n",
            "Epoch 00648: val_accuracy did not improve from 0.54308\n",
            "Epoch 649/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6656 - accuracy: 0.5761 - val_loss: 0.7300 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00649: val_accuracy did not improve from 0.54308\n",
            "Epoch 650/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6721 - accuracy: 0.5611 - val_loss: 0.7008 - val_accuracy: 0.5358\n",
            "\n",
            "Epoch 00650: val_accuracy did not improve from 0.54308\n",
            "Epoch 651/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6732 - accuracy: 0.5627 - val_loss: 0.7027 - val_accuracy: 0.5293\n",
            "\n",
            "Epoch 00651: val_accuracy did not improve from 0.54308\n",
            "Epoch 652/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6717 - accuracy: 0.5632 - val_loss: 0.7202 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 00652: val_accuracy did not improve from 0.54308\n",
            "Epoch 653/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6694 - accuracy: 0.5745 - val_loss: 0.7348 - val_accuracy: 0.5424\n",
            "\n",
            "Epoch 00653: val_accuracy did not improve from 0.54308\n",
            "Epoch 654/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6716 - accuracy: 0.5643 - val_loss: 0.7237 - val_accuracy: 0.5395\n",
            "\n",
            "Epoch 00654: val_accuracy did not improve from 0.54308\n",
            "Epoch 655/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6676 - accuracy: 0.5683 - val_loss: 0.7140 - val_accuracy: 0.5286\n",
            "\n",
            "Epoch 00655: val_accuracy did not improve from 0.54308\n",
            "Epoch 656/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6684 - accuracy: 0.5665 - val_loss: 0.7209 - val_accuracy: 0.5373\n",
            "\n",
            "Epoch 00656: val_accuracy did not improve from 0.54308\n",
            "Epoch 657/5000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.6655 - accuracy: 0.5766 - val_loss: 0.7334 - val_accuracy: 0.5387\n",
            "\n",
            "Epoch 00657: val_accuracy did not improve from 0.54308\n",
            "Epoch 658/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6692 - accuracy: 0.5701 - val_loss: 0.7286 - val_accuracy: 0.5431\n",
            "\n",
            "Epoch 00658: val_accuracy did not improve from 0.54308\n",
            "Epoch 659/5000\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.6664 - accuracy: 0.5813 - val_loss: 0.7209 - val_accuracy: 0.5329\n",
            "\n",
            "Epoch 00659: val_accuracy did not improve from 0.54308\n",
            "Epoch 660/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6689 - accuracy: 0.5746 - val_loss: 0.7385 - val_accuracy: 0.5344\n",
            "\n",
            "Epoch 00660: val_accuracy did not improve from 0.54308\n",
            "Epoch 661/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6674 - accuracy: 0.5667 - val_loss: 0.7574 - val_accuracy: 0.5431\n",
            "\n",
            "Epoch 00661: val_accuracy did not improve from 0.54308\n",
            "Epoch 662/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6669 - accuracy: 0.5763 - val_loss: 0.7345 - val_accuracy: 0.5373\n",
            "\n",
            "Epoch 00662: val_accuracy did not improve from 0.54308\n",
            "Epoch 663/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6670 - accuracy: 0.5748 - val_loss: 0.7116 - val_accuracy: 0.5395\n",
            "\n",
            "Epoch 00663: val_accuracy did not improve from 0.54308\n",
            "Epoch 664/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6689 - accuracy: 0.5714 - val_loss: 0.7174 - val_accuracy: 0.5402\n",
            "\n",
            "Epoch 00664: val_accuracy did not improve from 0.54308\n",
            "Epoch 665/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6662 - accuracy: 0.5803 - val_loss: 0.7519 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 00665: val_accuracy did not improve from 0.54308\n",
            "Epoch 666/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6706 - accuracy: 0.5667 - val_loss: 0.7500 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 00666: val_accuracy did not improve from 0.54308\n",
            "Epoch 667/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6690 - accuracy: 0.5650 - val_loss: 0.7379 - val_accuracy: 0.5279\n",
            "\n",
            "Epoch 00667: val_accuracy did not improve from 0.54308\n",
            "Epoch 668/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6681 - accuracy: 0.5726 - val_loss: 0.7399 - val_accuracy: 0.5286\n",
            "\n",
            "Epoch 00668: val_accuracy did not improve from 0.54308\n",
            "Epoch 669/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6711 - accuracy: 0.5639 - val_loss: 0.7307 - val_accuracy: 0.5358\n",
            "\n",
            "Epoch 00669: val_accuracy did not improve from 0.54308\n",
            "Epoch 670/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6651 - accuracy: 0.5828 - val_loss: 0.7201 - val_accuracy: 0.5373\n",
            "\n",
            "Epoch 00670: val_accuracy did not improve from 0.54308\n",
            "Epoch 671/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6674 - accuracy: 0.5658 - val_loss: 0.7339 - val_accuracy: 0.5286\n",
            "\n",
            "Epoch 00671: val_accuracy did not improve from 0.54308\n",
            "Epoch 672/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6635 - accuracy: 0.5817 - val_loss: 0.7532 - val_accuracy: 0.5366\n",
            "\n",
            "Epoch 00672: val_accuracy did not improve from 0.54308\n",
            "Epoch 673/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6649 - accuracy: 0.5764 - val_loss: 0.7620 - val_accuracy: 0.5344\n",
            "\n",
            "Epoch 00673: val_accuracy did not improve from 0.54308\n",
            "Epoch 674/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6660 - accuracy: 0.5707 - val_loss: 0.7554 - val_accuracy: 0.5402\n",
            "\n",
            "Epoch 00674: val_accuracy did not improve from 0.54308\n",
            "Epoch 675/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6616 - accuracy: 0.5830 - val_loss: 0.7600 - val_accuracy: 0.5301\n",
            "\n",
            "Epoch 00675: val_accuracy did not improve from 0.54308\n",
            "Epoch 676/5000\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.6673 - accuracy: 0.5743 - val_loss: 0.8126 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 00676: val_accuracy did not improve from 0.54308\n",
            "Epoch 677/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6724 - accuracy: 0.5696 - val_loss: 0.8038 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 00677: val_accuracy did not improve from 0.54308\n",
            "Epoch 678/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6745 - accuracy: 0.5716 - val_loss: 0.7675 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00678: val_accuracy did not improve from 0.54308\n",
            "Epoch 679/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6667 - accuracy: 0.5777 - val_loss: 0.7352 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 00679: val_accuracy did not improve from 0.54308\n",
            "Epoch 680/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6705 - accuracy: 0.5621 - val_loss: 0.7112 - val_accuracy: 0.5351\n",
            "\n",
            "Epoch 00680: val_accuracy did not improve from 0.54308\n",
            "Epoch 681/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6679 - accuracy: 0.5712 - val_loss: 0.7011 - val_accuracy: 0.5351\n",
            "\n",
            "Epoch 00681: val_accuracy did not improve from 0.54308\n",
            "Epoch 682/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6700 - accuracy: 0.5730 - val_loss: 0.7090 - val_accuracy: 0.5322\n",
            "\n",
            "Epoch 00682: val_accuracy did not improve from 0.54308\n",
            "Epoch 683/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6658 - accuracy: 0.5830 - val_loss: 0.7292 - val_accuracy: 0.5308\n",
            "\n",
            "Epoch 00683: val_accuracy did not improve from 0.54308\n",
            "Epoch 684/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6686 - accuracy: 0.5766 - val_loss: 0.7296 - val_accuracy: 0.5474\n",
            "\n",
            "Epoch 00684: val_accuracy improved from 0.54308 to 0.54743, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 685/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6696 - accuracy: 0.5659 - val_loss: 0.7382 - val_accuracy: 0.5329\n",
            "\n",
            "Epoch 00685: val_accuracy did not improve from 0.54743\n",
            "Epoch 686/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6676 - accuracy: 0.5681 - val_loss: 0.7427 - val_accuracy: 0.5402\n",
            "\n",
            "Epoch 00686: val_accuracy did not improve from 0.54743\n",
            "Epoch 687/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6676 - accuracy: 0.5717 - val_loss: 0.7339 - val_accuracy: 0.5351\n",
            "\n",
            "Epoch 00687: val_accuracy did not improve from 0.54743\n",
            "Epoch 688/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6655 - accuracy: 0.5797 - val_loss: 0.7198 - val_accuracy: 0.5402\n",
            "\n",
            "Epoch 00688: val_accuracy did not improve from 0.54743\n",
            "Epoch 689/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6663 - accuracy: 0.5725 - val_loss: 0.7301 - val_accuracy: 0.5380\n",
            "\n",
            "Epoch 00689: val_accuracy did not improve from 0.54743\n",
            "Epoch 690/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6641 - accuracy: 0.5837 - val_loss: 0.7656 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 00690: val_accuracy did not improve from 0.54743\n",
            "Epoch 691/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6642 - accuracy: 0.5822 - val_loss: 0.7679 - val_accuracy: 0.5366\n",
            "\n",
            "Epoch 00691: val_accuracy did not improve from 0.54743\n",
            "Epoch 692/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6636 - accuracy: 0.5797 - val_loss: 0.7640 - val_accuracy: 0.5366\n",
            "\n",
            "Epoch 00692: val_accuracy did not improve from 0.54743\n",
            "Epoch 693/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6637 - accuracy: 0.5763 - val_loss: 0.7730 - val_accuracy: 0.5293\n",
            "\n",
            "Epoch 00693: val_accuracy did not improve from 0.54743\n",
            "Epoch 694/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6620 - accuracy: 0.5779 - val_loss: 0.7699 - val_accuracy: 0.5366\n",
            "\n",
            "Epoch 00694: val_accuracy did not improve from 0.54743\n",
            "Epoch 695/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6645 - accuracy: 0.5792 - val_loss: 0.7954 - val_accuracy: 0.5380\n",
            "\n",
            "Epoch 00695: val_accuracy did not improve from 0.54743\n",
            "Epoch 696/5000\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.6594 - accuracy: 0.5853 - val_loss: 0.7864 - val_accuracy: 0.5431\n",
            "\n",
            "Epoch 00696: val_accuracy did not improve from 0.54743\n",
            "Epoch 697/5000\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.6645 - accuracy: 0.5812 - val_loss: 0.7231 - val_accuracy: 0.5344\n",
            "\n",
            "Epoch 00697: val_accuracy did not improve from 0.54743\n",
            "Epoch 698/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6643 - accuracy: 0.5801 - val_loss: 0.7151 - val_accuracy: 0.5257\n",
            "\n",
            "Epoch 00698: val_accuracy did not improve from 0.54743\n",
            "Epoch 699/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6625 - accuracy: 0.5819 - val_loss: 0.7129 - val_accuracy: 0.5395\n",
            "\n",
            "Epoch 00699: val_accuracy did not improve from 0.54743\n",
            "Epoch 700/5000\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6628 - accuracy: 0.5864 - val_loss: 0.7039 - val_accuracy: 0.5272\n",
            "\n",
            "Epoch 00700: val_accuracy did not improve from 0.54743\n",
            "Epoch 701/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6655 - accuracy: 0.5815 - val_loss: 0.7068 - val_accuracy: 0.5351\n",
            "\n",
            "Epoch 00701: val_accuracy did not improve from 0.54743\n",
            "Epoch 702/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6622 - accuracy: 0.5868 - val_loss: 0.7121 - val_accuracy: 0.5344\n",
            "\n",
            "Epoch 00702: val_accuracy did not improve from 0.54743\n",
            "Epoch 703/5000\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.6628 - accuracy: 0.5853 - val_loss: 0.7067 - val_accuracy: 0.5301\n",
            "\n",
            "Epoch 00703: val_accuracy did not improve from 0.54743\n",
            "Epoch 704/5000\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.6654 - accuracy: 0.5699 - val_loss: 0.7117 - val_accuracy: 0.5308\n",
            "\n",
            "Epoch 00704: val_accuracy did not improve from 0.54743\n",
            "Epoch 705/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6633 - accuracy: 0.5790 - val_loss: 0.7233 - val_accuracy: 0.5358\n",
            "\n",
            "Epoch 00705: val_accuracy did not improve from 0.54743\n",
            "Epoch 706/5000\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.6618 - accuracy: 0.5861 - val_loss: 0.7219 - val_accuracy: 0.5409\n",
            "\n",
            "Epoch 00706: val_accuracy did not improve from 0.54743\n",
            "Epoch 707/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6614 - accuracy: 0.5830 - val_loss: 0.7134 - val_accuracy: 0.5358\n",
            "\n",
            "Epoch 00707: val_accuracy did not improve from 0.54743\n",
            "Epoch 708/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6595 - accuracy: 0.5866 - val_loss: 0.7174 - val_accuracy: 0.5373\n",
            "\n",
            "Epoch 00708: val_accuracy did not improve from 0.54743\n",
            "Epoch 709/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6629 - accuracy: 0.5851 - val_loss: 0.7103 - val_accuracy: 0.5308\n",
            "\n",
            "Epoch 00709: val_accuracy did not improve from 0.54743\n",
            "Epoch 710/5000\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.6668 - accuracy: 0.5783 - val_loss: 0.7183 - val_accuracy: 0.5344\n",
            "\n",
            "Epoch 00710: val_accuracy did not improve from 0.54743\n",
            "Epoch 711/5000\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.6633 - accuracy: 0.5819 - val_loss: 0.7210 - val_accuracy: 0.5416\n",
            "\n",
            "Epoch 00711: val_accuracy did not improve from 0.54743\n",
            "Epoch 712/5000\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.6628 - accuracy: 0.5806 - val_loss: 0.7115 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 00712: val_accuracy did not improve from 0.54743\n",
            "Epoch 713/5000\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6673 - accuracy: 0.5712 - val_loss: 0.7264 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 00713: val_accuracy did not improve from 0.54743\n",
            "Epoch 714/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6660 - accuracy: 0.5799 - val_loss: 0.7619 - val_accuracy: 0.5358\n",
            "\n",
            "Epoch 00714: val_accuracy did not improve from 0.54743\n",
            "Epoch 715/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6704 - accuracy: 0.5806 - val_loss: 0.7360 - val_accuracy: 0.5387\n",
            "\n",
            "Epoch 00715: val_accuracy did not improve from 0.54743\n",
            "Epoch 716/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6641 - accuracy: 0.5790 - val_loss: 0.7097 - val_accuracy: 0.5344\n",
            "\n",
            "Epoch 00716: val_accuracy did not improve from 0.54743\n",
            "Epoch 717/5000\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6645 - accuracy: 0.5766 - val_loss: 0.7194 - val_accuracy: 0.5583\n",
            "\n",
            "Epoch 00717: val_accuracy improved from 0.54743 to 0.55829, saving model to /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 03751: val_accuracy did not improve from 0.55829\n",
            "Epoch 3752/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.5185 - accuracy: 0.7120 - val_loss: 1.6627 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03752: val_accuracy did not improve from 0.55829\n",
            "Epoch 3753/5000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.5154 - accuracy: 0.7170 - val_loss: 1.7554 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 03753: val_accuracy did not improve from 0.55829\n",
            "Epoch 3754/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.5688 - accuracy: 0.6893 - val_loss: 1.7802 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 03754: val_accuracy did not improve from 0.55829\n",
            "Epoch 3755/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.5159 - accuracy: 0.7170 - val_loss: 1.9741 - val_accuracy: 0.4859\n",
            "\n",
            "Epoch 03755: val_accuracy did not improve from 0.55829\n",
            "Epoch 3756/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.5338 - accuracy: 0.7187 - val_loss: 1.8745 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 03756: val_accuracy did not improve from 0.55829\n",
            "Epoch 3757/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.5052 - accuracy: 0.7212 - val_loss: 1.7495 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03757: val_accuracy did not improve from 0.55829\n",
            "Epoch 3758/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.5011 - accuracy: 0.7221 - val_loss: 1.6509 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 03758: val_accuracy did not improve from 0.55829\n",
            "Epoch 3759/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.5062 - accuracy: 0.7078 - val_loss: 1.6023 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 03759: val_accuracy did not improve from 0.55829\n",
            "Epoch 3760/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.5078 - accuracy: 0.7127 - val_loss: 1.8323 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 03760: val_accuracy did not improve from 0.55829\n",
            "Epoch 3761/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4907 - accuracy: 0.7216 - val_loss: 2.0356 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 03761: val_accuracy did not improve from 0.55829\n",
            "Epoch 3762/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.5078 - accuracy: 0.7194 - val_loss: 1.8973 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 03762: val_accuracy did not improve from 0.55829\n",
            "Epoch 3763/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4822 - accuracy: 0.7279 - val_loss: 1.6352 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 03763: val_accuracy did not improve from 0.55829\n",
            "Epoch 3764/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4792 - accuracy: 0.7350 - val_loss: 1.6019 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 03764: val_accuracy did not improve from 0.55829\n",
            "Epoch 3765/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4735 - accuracy: 0.7375 - val_loss: 1.7733 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 03765: val_accuracy did not improve from 0.55829\n",
            "Epoch 3766/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4749 - accuracy: 0.7400 - val_loss: 2.0221 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 03766: val_accuracy did not improve from 0.55829\n",
            "Epoch 3767/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4742 - accuracy: 0.7379 - val_loss: 2.1386 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03767: val_accuracy did not improve from 0.55829\n",
            "Epoch 3768/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4699 - accuracy: 0.7371 - val_loss: 2.2013 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 03768: val_accuracy did not improve from 0.55829\n",
            "Epoch 3769/5000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.4638 - accuracy: 0.7442 - val_loss: 2.1368 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 03769: val_accuracy did not improve from 0.55829\n",
            "Epoch 3770/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.4570 - accuracy: 0.7429 - val_loss: 1.9359 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 03770: val_accuracy did not improve from 0.55829\n",
            "Epoch 3771/5000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.4614 - accuracy: 0.7375 - val_loss: 1.6737 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 03771: val_accuracy did not improve from 0.55829\n",
            "Epoch 3772/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4681 - accuracy: 0.7388 - val_loss: 1.8032 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 03772: val_accuracy did not improve from 0.55829\n",
            "Epoch 3773/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4555 - accuracy: 0.7475 - val_loss: 2.0523 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03773: val_accuracy did not improve from 0.55829\n",
            "Epoch 3774/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4577 - accuracy: 0.7384 - val_loss: 2.2482 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 03774: val_accuracy did not improve from 0.55829\n",
            "Epoch 3775/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4598 - accuracy: 0.7406 - val_loss: 2.3026 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 03775: val_accuracy did not improve from 0.55829\n",
            "Epoch 3776/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4511 - accuracy: 0.7429 - val_loss: 2.3244 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 03776: val_accuracy did not improve from 0.55829\n",
            "Epoch 3777/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4585 - accuracy: 0.7395 - val_loss: 2.4043 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 03777: val_accuracy did not improve from 0.55829\n",
            "Epoch 3778/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4846 - accuracy: 0.7245 - val_loss: 2.4039 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 03778: val_accuracy did not improve from 0.55829\n",
            "Epoch 3779/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4503 - accuracy: 0.7451 - val_loss: 2.4372 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 03779: val_accuracy did not improve from 0.55829\n",
            "Epoch 3780/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4611 - accuracy: 0.7411 - val_loss: 2.3332 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 03780: val_accuracy did not improve from 0.55829\n",
            "Epoch 3781/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4642 - accuracy: 0.7408 - val_loss: 2.2055 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 03781: val_accuracy did not improve from 0.55829\n",
            "Epoch 3782/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4491 - accuracy: 0.7489 - val_loss: 2.1377 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 03782: val_accuracy did not improve from 0.55829\n",
            "Epoch 3783/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4573 - accuracy: 0.7409 - val_loss: 1.9909 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 03783: val_accuracy did not improve from 0.55829\n",
            "Epoch 3784/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4445 - accuracy: 0.7536 - val_loss: 2.0331 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03784: val_accuracy did not improve from 0.55829\n",
            "Epoch 3785/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4500 - accuracy: 0.7491 - val_loss: 2.1020 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 03785: val_accuracy did not improve from 0.55829\n",
            "Epoch 3786/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4502 - accuracy: 0.7460 - val_loss: 2.0508 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 03786: val_accuracy did not improve from 0.55829\n",
            "Epoch 3787/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4447 - accuracy: 0.7460 - val_loss: 1.9788 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 03787: val_accuracy did not improve from 0.55829\n",
            "Epoch 3788/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4518 - accuracy: 0.7431 - val_loss: 2.0612 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 03788: val_accuracy did not improve from 0.55829\n",
            "Epoch 3789/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4447 - accuracy: 0.7502 - val_loss: 2.0692 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 03789: val_accuracy did not improve from 0.55829\n",
            "Epoch 3790/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.4418 - accuracy: 0.7496 - val_loss: 2.0179 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 03790: val_accuracy did not improve from 0.55829\n",
            "Epoch 3791/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4425 - accuracy: 0.7540 - val_loss: 2.0785 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 03791: val_accuracy did not improve from 0.55829\n",
            "Epoch 3792/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4370 - accuracy: 0.7587 - val_loss: 2.1824 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 03792: val_accuracy did not improve from 0.55829\n",
            "Epoch 3793/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4393 - accuracy: 0.7538 - val_loss: 2.1594 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 03793: val_accuracy did not improve from 0.55829\n",
            "Epoch 3794/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4442 - accuracy: 0.7493 - val_loss: 2.0972 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 03794: val_accuracy did not improve from 0.55829\n",
            "Epoch 3795/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4392 - accuracy: 0.7520 - val_loss: 2.0192 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 03795: val_accuracy did not improve from 0.55829\n",
            "Epoch 3796/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4355 - accuracy: 0.7558 - val_loss: 2.0116 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 03796: val_accuracy did not improve from 0.55829\n",
            "Epoch 3797/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4360 - accuracy: 0.7514 - val_loss: 2.1201 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03797: val_accuracy did not improve from 0.55829\n",
            "Epoch 3798/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4331 - accuracy: 0.7578 - val_loss: 2.2670 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 03798: val_accuracy did not improve from 0.55829\n",
            "Epoch 3799/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4401 - accuracy: 0.7518 - val_loss: 2.1611 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03799: val_accuracy did not improve from 0.55829\n",
            "Epoch 3800/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4331 - accuracy: 0.7569 - val_loss: 2.1526 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 03800: val_accuracy did not improve from 0.55829\n",
            "Epoch 3801/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4314 - accuracy: 0.7569 - val_loss: 2.1628 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 03801: val_accuracy did not improve from 0.55829\n",
            "Epoch 3802/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4357 - accuracy: 0.7580 - val_loss: 2.1798 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 03802: val_accuracy did not improve from 0.55829\n",
            "Epoch 3803/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4353 - accuracy: 0.7565 - val_loss: 2.2938 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 03803: val_accuracy did not improve from 0.55829\n",
            "Epoch 3804/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4290 - accuracy: 0.7601 - val_loss: 2.3797 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 03804: val_accuracy did not improve from 0.55829\n",
            "Epoch 3805/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4287 - accuracy: 0.7601 - val_loss: 2.3515 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 03805: val_accuracy did not improve from 0.55829\n",
            "Epoch 3806/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4289 - accuracy: 0.7587 - val_loss: 2.2348 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 03806: val_accuracy did not improve from 0.55829\n",
            "Epoch 3807/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4344 - accuracy: 0.7567 - val_loss: 2.2432 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 03807: val_accuracy did not improve from 0.55829\n",
            "Epoch 3808/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4342 - accuracy: 0.7540 - val_loss: 2.2635 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03808: val_accuracy did not improve from 0.55829\n",
            "Epoch 3809/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4285 - accuracy: 0.7582 - val_loss: 2.2886 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03809: val_accuracy did not improve from 0.55829\n",
            "Epoch 3810/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4320 - accuracy: 0.7576 - val_loss: 2.2953 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03810: val_accuracy did not improve from 0.55829\n",
            "Epoch 3811/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4326 - accuracy: 0.7592 - val_loss: 2.3042 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03811: val_accuracy did not improve from 0.55829\n",
            "Epoch 3812/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4309 - accuracy: 0.7578 - val_loss: 2.3927 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03812: val_accuracy did not improve from 0.55829\n",
            "Epoch 3813/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4265 - accuracy: 0.7605 - val_loss: 2.4003 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 03813: val_accuracy did not improve from 0.55829\n",
            "Epoch 3814/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4278 - accuracy: 0.7600 - val_loss: 2.2927 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 03814: val_accuracy did not improve from 0.55829\n",
            "Epoch 3815/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.4298 - accuracy: 0.7594 - val_loss: 2.3466 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03815: val_accuracy did not improve from 0.55829\n",
            "Epoch 3816/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4297 - accuracy: 0.7605 - val_loss: 2.3096 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 03816: val_accuracy did not improve from 0.55829\n",
            "Epoch 3817/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4284 - accuracy: 0.7614 - val_loss: 2.2901 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03817: val_accuracy did not improve from 0.55829\n",
            "Epoch 3818/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4269 - accuracy: 0.7620 - val_loss: 2.3497 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 03818: val_accuracy did not improve from 0.55829\n",
            "Epoch 3819/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4274 - accuracy: 0.7572 - val_loss: 2.4169 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 03819: val_accuracy did not improve from 0.55829\n",
            "Epoch 3820/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4234 - accuracy: 0.7667 - val_loss: 2.4227 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 03820: val_accuracy did not improve from 0.55829\n",
            "Epoch 3821/5000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.4226 - accuracy: 0.7649 - val_loss: 2.4018 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 03821: val_accuracy did not improve from 0.55829\n",
            "Epoch 3822/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4271 - accuracy: 0.7625 - val_loss: 2.4413 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 03822: val_accuracy did not improve from 0.55829\n",
            "Epoch 3823/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4303 - accuracy: 0.7582 - val_loss: 2.4329 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 03823: val_accuracy did not improve from 0.55829\n",
            "Epoch 3824/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4277 - accuracy: 0.7625 - val_loss: 2.5013 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 03824: val_accuracy did not improve from 0.55829\n",
            "Epoch 3825/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.4368 - accuracy: 0.7549 - val_loss: 2.4348 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 03825: val_accuracy did not improve from 0.55829\n",
            "Epoch 3826/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4297 - accuracy: 0.7592 - val_loss: 2.5862 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 03826: val_accuracy did not improve from 0.55829\n",
            "Epoch 3827/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4219 - accuracy: 0.7618 - val_loss: 2.6096 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 03827: val_accuracy did not improve from 0.55829\n",
            "Epoch 3828/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4281 - accuracy: 0.7574 - val_loss: 2.4601 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 03828: val_accuracy did not improve from 0.55829\n",
            "Epoch 3829/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4347 - accuracy: 0.7578 - val_loss: 2.4308 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 03829: val_accuracy did not improve from 0.55829\n",
            "Epoch 3830/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4350 - accuracy: 0.7630 - val_loss: 2.5046 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03830: val_accuracy did not improve from 0.55829\n",
            "Epoch 3831/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4376 - accuracy: 0.7543 - val_loss: 2.4684 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 03831: val_accuracy did not improve from 0.55829\n",
            "Epoch 3832/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4423 - accuracy: 0.7534 - val_loss: 2.3990 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 03832: val_accuracy did not improve from 0.55829\n",
            "Epoch 3833/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4339 - accuracy: 0.7578 - val_loss: 2.4676 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 03833: val_accuracy did not improve from 0.55829\n",
            "Epoch 3834/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4281 - accuracy: 0.7611 - val_loss: 2.7536 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 03834: val_accuracy did not improve from 0.55829\n",
            "Epoch 3835/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4414 - accuracy: 0.7527 - val_loss: 2.6734 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03835: val_accuracy did not improve from 0.55829\n",
            "Epoch 3836/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4594 - accuracy: 0.7415 - val_loss: 2.7484 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03836: val_accuracy did not improve from 0.55829\n",
            "Epoch 3837/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.5521 - accuracy: 0.7069 - val_loss: 2.3871 - val_accuracy: 0.5243\n",
            "\n",
            "Epoch 03837: val_accuracy did not improve from 0.55829\n",
            "Epoch 3838/5000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.5182 - accuracy: 0.7149 - val_loss: 2.4155 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 03838: val_accuracy did not improve from 0.55829\n",
            "Epoch 3839/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.5019 - accuracy: 0.7303 - val_loss: 2.2186 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 03839: val_accuracy did not improve from 0.55829\n",
            "Epoch 3840/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4735 - accuracy: 0.7437 - val_loss: 2.2629 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 03840: val_accuracy did not improve from 0.55829\n",
            "Epoch 3841/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.5026 - accuracy: 0.7384 - val_loss: 2.3016 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03841: val_accuracy did not improve from 0.55829\n",
            "Epoch 3842/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.5503 - accuracy: 0.7121 - val_loss: 1.5637 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 03842: val_accuracy did not improve from 0.55829\n",
            "Epoch 3843/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6234 - accuracy: 0.6630 - val_loss: 1.3059 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 03843: val_accuracy did not improve from 0.55829\n",
            "Epoch 3844/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6086 - accuracy: 0.6656 - val_loss: 1.2259 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 03844: val_accuracy did not improve from 0.55829\n",
            "Epoch 3845/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.5890 - accuracy: 0.6752 - val_loss: 1.2906 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03845: val_accuracy did not improve from 0.55829\n",
            "Epoch 3846/5000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.6015 - accuracy: 0.6630 - val_loss: 1.5552 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 03846: val_accuracy did not improve from 0.55829\n",
            "Epoch 3847/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6097 - accuracy: 0.6736 - val_loss: 1.6846 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 03847: val_accuracy did not improve from 0.55829\n",
            "Epoch 3848/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.5476 - accuracy: 0.6926 - val_loss: 1.8410 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 03848: val_accuracy did not improve from 0.55829\n",
            "Epoch 3849/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.5483 - accuracy: 0.6953 - val_loss: 1.7583 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03849: val_accuracy did not improve from 0.55829\n",
            "Epoch 3850/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.5351 - accuracy: 0.7094 - val_loss: 1.7020 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 03850: val_accuracy did not improve from 0.55829\n",
            "Epoch 3851/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.5327 - accuracy: 0.7080 - val_loss: 1.7930 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 03851: val_accuracy did not improve from 0.55829\n",
            "Epoch 3852/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.5479 - accuracy: 0.7004 - val_loss: 1.8347 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 03852: val_accuracy did not improve from 0.55829\n",
            "Epoch 3853/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.5185 - accuracy: 0.7136 - val_loss: 1.8338 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 03853: val_accuracy did not improve from 0.55829\n",
            "Epoch 3854/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4972 - accuracy: 0.7315 - val_loss: 1.5725 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 03854: val_accuracy did not improve from 0.55829\n",
            "Epoch 3855/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.5005 - accuracy: 0.7245 - val_loss: 1.4741 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 03855: val_accuracy did not improve from 0.55829\n",
            "Epoch 3856/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4895 - accuracy: 0.7274 - val_loss: 1.5181 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 03856: val_accuracy did not improve from 0.55829\n",
            "Epoch 3857/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4907 - accuracy: 0.7310 - val_loss: 1.6076 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 03857: val_accuracy did not improve from 0.55829\n",
            "Epoch 3858/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.4837 - accuracy: 0.7310 - val_loss: 1.6421 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 03858: val_accuracy did not improve from 0.55829\n",
            "Epoch 3859/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4911 - accuracy: 0.7261 - val_loss: 1.6310 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 03859: val_accuracy did not improve from 0.55829\n",
            "Epoch 3860/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4782 - accuracy: 0.7319 - val_loss: 1.6366 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 03860: val_accuracy did not improve from 0.55829\n",
            "Epoch 3861/5000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.4746 - accuracy: 0.7299 - val_loss: 1.6709 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03861: val_accuracy did not improve from 0.55829\n",
            "Epoch 3862/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4721 - accuracy: 0.7382 - val_loss: 1.7787 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 03862: val_accuracy did not improve from 0.55829\n",
            "Epoch 3863/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4699 - accuracy: 0.7379 - val_loss: 1.7543 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 03863: val_accuracy did not improve from 0.55829\n",
            "Epoch 3864/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4657 - accuracy: 0.7411 - val_loss: 1.9355 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 03864: val_accuracy did not improve from 0.55829\n",
            "Epoch 3865/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4715 - accuracy: 0.7382 - val_loss: 2.0995 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 03865: val_accuracy did not improve from 0.55829\n",
            "Epoch 3866/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4577 - accuracy: 0.7462 - val_loss: 2.1549 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 03866: val_accuracy did not improve from 0.55829\n",
            "Epoch 3867/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4595 - accuracy: 0.7453 - val_loss: 1.9280 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 03867: val_accuracy did not improve from 0.55829\n",
            "Epoch 3868/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4566 - accuracy: 0.7451 - val_loss: 1.7926 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 03868: val_accuracy did not improve from 0.55829\n",
            "Epoch 3869/5000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.4560 - accuracy: 0.7426 - val_loss: 1.7847 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 03869: val_accuracy did not improve from 0.55829\n",
            "Epoch 3870/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.4538 - accuracy: 0.7466 - val_loss: 1.8465 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 03870: val_accuracy did not improve from 0.55829\n",
            "Epoch 3871/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4553 - accuracy: 0.7482 - val_loss: 1.9388 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03871: val_accuracy did not improve from 0.55829\n",
            "Epoch 3872/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.4421 - accuracy: 0.7538 - val_loss: 2.0266 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 03872: val_accuracy did not improve from 0.55829\n",
            "Epoch 3873/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4421 - accuracy: 0.7574 - val_loss: 2.1308 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 03873: val_accuracy did not improve from 0.55829\n",
            "Epoch 3874/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4418 - accuracy: 0.7545 - val_loss: 2.1569 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 03874: val_accuracy did not improve from 0.55829\n",
            "Epoch 3875/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4414 - accuracy: 0.7504 - val_loss: 2.1438 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 03875: val_accuracy did not improve from 0.55829\n",
            "Epoch 3876/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.4383 - accuracy: 0.7536 - val_loss: 2.1637 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 03876: val_accuracy did not improve from 0.55829\n",
            "Epoch 3877/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4397 - accuracy: 0.7540 - val_loss: 2.2062 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 03877: val_accuracy did not improve from 0.55829\n",
            "Epoch 3878/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4378 - accuracy: 0.7553 - val_loss: 2.1970 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 03878: val_accuracy did not improve from 0.55829\n",
            "Epoch 3879/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4366 - accuracy: 0.7536 - val_loss: 2.0856 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 03879: val_accuracy did not improve from 0.55829\n",
            "Epoch 3880/5000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.4374 - accuracy: 0.7504 - val_loss: 2.0988 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 03880: val_accuracy did not improve from 0.55829\n",
            "Epoch 3881/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4375 - accuracy: 0.7623 - val_loss: 2.2630 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 03881: val_accuracy did not improve from 0.55829\n",
            "Epoch 3882/5000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.4341 - accuracy: 0.7594 - val_loss: 2.3860 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 03882: val_accuracy did not improve from 0.55829\n",
            "Epoch 3883/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4362 - accuracy: 0.7549 - val_loss: 2.3728 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 03883: val_accuracy did not improve from 0.55829\n",
            "Epoch 3884/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4334 - accuracy: 0.7567 - val_loss: 2.4386 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 03884: val_accuracy did not improve from 0.55829\n",
            "Epoch 3885/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4347 - accuracy: 0.7562 - val_loss: 2.5349 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 03885: val_accuracy did not improve from 0.55829\n",
            "Epoch 3886/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4315 - accuracy: 0.7587 - val_loss: 2.4896 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03886: val_accuracy did not improve from 0.55829\n",
            "Epoch 3887/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4304 - accuracy: 0.7647 - val_loss: 2.3651 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 03887: val_accuracy did not improve from 0.55829\n",
            "Epoch 3888/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.4304 - accuracy: 0.7558 - val_loss: 2.3461 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 03888: val_accuracy did not improve from 0.55829\n",
            "Epoch 3889/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4272 - accuracy: 0.7674 - val_loss: 2.4559 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 03889: val_accuracy did not improve from 0.55829\n",
            "Epoch 3890/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4279 - accuracy: 0.7618 - val_loss: 2.5932 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 03890: val_accuracy did not improve from 0.55829\n",
            "Epoch 3891/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4290 - accuracy: 0.7621 - val_loss: 2.6528 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 03891: val_accuracy did not improve from 0.55829\n",
            "Epoch 3892/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4208 - accuracy: 0.7683 - val_loss: 2.7189 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 03892: val_accuracy did not improve from 0.55829\n",
            "Epoch 3893/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4237 - accuracy: 0.7652 - val_loss: 2.7706 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 03893: val_accuracy did not improve from 0.55829\n",
            "Epoch 3894/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.4323 - accuracy: 0.7598 - val_loss: 2.6707 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 03894: val_accuracy did not improve from 0.55829\n",
            "Epoch 3895/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4259 - accuracy: 0.7652 - val_loss: 2.5487 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 03895: val_accuracy did not improve from 0.55829\n",
            "Epoch 3896/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4282 - accuracy: 0.7681 - val_loss: 2.5304 - val_accuracy: 0.5206\n",
            "\n",
            "Epoch 03896: val_accuracy did not improve from 0.55829\n",
            "Epoch 3897/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4350 - accuracy: 0.7574 - val_loss: 2.6414 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 03897: val_accuracy did not improve from 0.55829\n",
            "Epoch 3898/5000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.4273 - accuracy: 0.7592 - val_loss: 2.6277 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 03898: val_accuracy did not improve from 0.55829\n",
            "Epoch 3899/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4293 - accuracy: 0.7592 - val_loss: 2.6042 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03899: val_accuracy did not improve from 0.55829\n",
            "Epoch 3900/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4560 - accuracy: 0.7514 - val_loss: 2.5397 - val_accuracy: 0.4888\n",
            "\n",
            "Epoch 03900: val_accuracy did not improve from 0.55829\n",
            "Epoch 3901/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.6089 - accuracy: 0.6848 - val_loss: 2.2022 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 03901: val_accuracy did not improve from 0.55829\n",
            "Epoch 3902/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.5099 - accuracy: 0.7199 - val_loss: 2.2950 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03902: val_accuracy did not improve from 0.55829\n",
            "Epoch 3903/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.5668 - accuracy: 0.6837 - val_loss: 2.2599 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 03903: val_accuracy did not improve from 0.55829\n",
            "Epoch 3904/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.5126 - accuracy: 0.7111 - val_loss: 2.3390 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 03904: val_accuracy did not improve from 0.55829\n",
            "Epoch 3905/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.5635 - accuracy: 0.6967 - val_loss: 2.1630 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 03905: val_accuracy did not improve from 0.55829\n",
            "Epoch 3906/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.5187 - accuracy: 0.7134 - val_loss: 2.1365 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 03906: val_accuracy did not improve from 0.55829\n",
            "Epoch 3907/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.5241 - accuracy: 0.6975 - val_loss: 1.9428 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 03907: val_accuracy did not improve from 0.55829\n",
            "Epoch 3908/5000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.5021 - accuracy: 0.7245 - val_loss: 1.9047 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 03908: val_accuracy did not improve from 0.55829\n",
            "Epoch 3909/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.5004 - accuracy: 0.7263 - val_loss: 2.0581 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03909: val_accuracy did not improve from 0.55829\n",
            "Epoch 3910/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.5142 - accuracy: 0.7141 - val_loss: 2.2470 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 03910: val_accuracy did not improve from 0.55829\n",
            "Epoch 3911/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4772 - accuracy: 0.7386 - val_loss: 2.4450 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 03911: val_accuracy did not improve from 0.55829\n",
            "Epoch 3912/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4924 - accuracy: 0.7344 - val_loss: 2.4019 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 03912: val_accuracy did not improve from 0.55829\n",
            "Epoch 3913/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4968 - accuracy: 0.7274 - val_loss: 2.3955 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 03913: val_accuracy did not improve from 0.55829\n",
            "Epoch 3914/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.5059 - accuracy: 0.7183 - val_loss: 2.5798 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03914: val_accuracy did not improve from 0.55829\n",
            "Epoch 3915/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.5079 - accuracy: 0.7236 - val_loss: 2.4332 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03915: val_accuracy did not improve from 0.55829\n",
            "Epoch 3916/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4747 - accuracy: 0.7313 - val_loss: 2.2383 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 03916: val_accuracy did not improve from 0.55829\n",
            "Epoch 3917/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4949 - accuracy: 0.7306 - val_loss: 2.0389 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03917: val_accuracy did not improve from 0.55829\n",
            "Epoch 3918/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4591 - accuracy: 0.7451 - val_loss: 2.1191 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03918: val_accuracy did not improve from 0.55829\n",
            "Epoch 3919/5000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.4797 - accuracy: 0.7341 - val_loss: 2.1620 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 03919: val_accuracy did not improve from 0.55829\n",
            "Epoch 3920/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4661 - accuracy: 0.7337 - val_loss: 2.0481 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 03920: val_accuracy did not improve from 0.55829\n",
            "Epoch 3921/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4717 - accuracy: 0.7397 - val_loss: 1.9614 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 03921: val_accuracy did not improve from 0.55829\n",
            "Epoch 3922/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4664 - accuracy: 0.7382 - val_loss: 2.0102 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 03922: val_accuracy did not improve from 0.55829\n",
            "Epoch 3923/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4620 - accuracy: 0.7362 - val_loss: 1.9564 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 03923: val_accuracy did not improve from 0.55829\n",
            "Epoch 3924/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4616 - accuracy: 0.7406 - val_loss: 2.0335 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03924: val_accuracy did not improve from 0.55829\n",
            "Epoch 3925/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4651 - accuracy: 0.7446 - val_loss: 2.1220 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 03925: val_accuracy did not improve from 0.55829\n",
            "Epoch 3926/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4510 - accuracy: 0.7522 - val_loss: 2.2910 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 03926: val_accuracy did not improve from 0.55829\n",
            "Epoch 3927/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4753 - accuracy: 0.7348 - val_loss: 2.3860 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 03927: val_accuracy did not improve from 0.55829\n",
            "Epoch 3928/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4432 - accuracy: 0.7496 - val_loss: 2.2591 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 03928: val_accuracy did not improve from 0.55829\n",
            "Epoch 3929/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4632 - accuracy: 0.7438 - val_loss: 1.8978 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 03929: val_accuracy did not improve from 0.55829\n",
            "Epoch 3930/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4453 - accuracy: 0.7480 - val_loss: 1.7443 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 03930: val_accuracy did not improve from 0.55829\n",
            "Epoch 3931/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.4655 - accuracy: 0.7455 - val_loss: 1.8730 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 03931: val_accuracy did not improve from 0.55829\n",
            "Epoch 3932/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4384 - accuracy: 0.7576 - val_loss: 2.0967 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 03932: val_accuracy did not improve from 0.55829\n",
            "Epoch 3933/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4615 - accuracy: 0.7437 - val_loss: 1.9933 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 03933: val_accuracy did not improve from 0.55829\n",
            "Epoch 3934/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4384 - accuracy: 0.7534 - val_loss: 1.9323 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 03934: val_accuracy did not improve from 0.55829\n",
            "Epoch 3935/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4555 - accuracy: 0.7438 - val_loss: 1.9199 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 03935: val_accuracy did not improve from 0.55829\n",
            "Epoch 3936/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4416 - accuracy: 0.7565 - val_loss: 2.0115 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 03936: val_accuracy did not improve from 0.55829\n",
            "Epoch 3937/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4519 - accuracy: 0.7426 - val_loss: 2.1590 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 03937: val_accuracy did not improve from 0.55829\n",
            "Epoch 3938/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4387 - accuracy: 0.7621 - val_loss: 2.1258 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 03938: val_accuracy did not improve from 0.55829\n",
            "Epoch 3939/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4533 - accuracy: 0.7408 - val_loss: 2.3087 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 03939: val_accuracy did not improve from 0.55829\n",
            "Epoch 3940/5000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.4343 - accuracy: 0.7554 - val_loss: 2.5724 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 03940: val_accuracy did not improve from 0.55829\n",
            "Epoch 3941/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4665 - accuracy: 0.7361 - val_loss: 2.8119 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 03941: val_accuracy did not improve from 0.55829\n",
            "Epoch 3942/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4459 - accuracy: 0.7453 - val_loss: 2.9003 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03942: val_accuracy did not improve from 0.55829\n",
            "Epoch 3943/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4595 - accuracy: 0.7455 - val_loss: 2.7320 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 03943: val_accuracy did not improve from 0.55829\n",
            "Epoch 3944/5000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.4450 - accuracy: 0.7471 - val_loss: 2.6970 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 03944: val_accuracy did not improve from 0.55829\n",
            "Epoch 3945/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4629 - accuracy: 0.7409 - val_loss: 2.6918 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 03945: val_accuracy did not improve from 0.55829\n",
            "Epoch 3946/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4461 - accuracy: 0.7502 - val_loss: 2.5474 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 03946: val_accuracy did not improve from 0.55829\n",
            "Epoch 3947/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4588 - accuracy: 0.7446 - val_loss: 2.3567 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 03947: val_accuracy did not improve from 0.55829\n",
            "Epoch 3948/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4438 - accuracy: 0.7558 - val_loss: 2.2946 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03948: val_accuracy did not improve from 0.55829\n",
            "Epoch 3949/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4484 - accuracy: 0.7509 - val_loss: 2.2230 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 03949: val_accuracy did not improve from 0.55829\n",
            "Epoch 3950/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4385 - accuracy: 0.7529 - val_loss: 2.1095 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 03950: val_accuracy did not improve from 0.55829\n",
            "Epoch 3951/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4486 - accuracy: 0.7482 - val_loss: 2.0411 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03951: val_accuracy did not improve from 0.55829\n",
            "Epoch 3952/5000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.4367 - accuracy: 0.7592 - val_loss: 2.1286 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 03952: val_accuracy did not improve from 0.55829\n",
            "Epoch 3953/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4439 - accuracy: 0.7471 - val_loss: 2.2629 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03953: val_accuracy did not improve from 0.55829\n",
            "Epoch 3954/5000\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.4374 - accuracy: 0.7567 - val_loss: 2.3361 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 03954: val_accuracy did not improve from 0.55829\n",
            "Epoch 3955/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.4338 - accuracy: 0.7592 - val_loss: 2.2160 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 03955: val_accuracy did not improve from 0.55829\n",
            "Epoch 3956/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4378 - accuracy: 0.7543 - val_loss: 2.2278 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 03956: val_accuracy did not improve from 0.55829\n",
            "Epoch 3957/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4370 - accuracy: 0.7592 - val_loss: 2.2879 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 03957: val_accuracy did not improve from 0.55829\n",
            "Epoch 3958/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4416 - accuracy: 0.7547 - val_loss: 2.5030 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 03958: val_accuracy did not improve from 0.55829\n",
            "Epoch 3959/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4425 - accuracy: 0.7525 - val_loss: 2.5891 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 03959: val_accuracy did not improve from 0.55829\n",
            "Epoch 3960/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4271 - accuracy: 0.7623 - val_loss: 2.5723 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 03960: val_accuracy did not improve from 0.55829\n",
            "Epoch 3961/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4321 - accuracy: 0.7572 - val_loss: 2.5083 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03961: val_accuracy did not improve from 0.55829\n",
            "Epoch 3962/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4335 - accuracy: 0.7600 - val_loss: 2.4894 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 03962: val_accuracy did not improve from 0.55829\n",
            "Epoch 3963/5000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.4274 - accuracy: 0.7609 - val_loss: 2.5391 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 03963: val_accuracy did not improve from 0.55829\n",
            "Epoch 3964/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4341 - accuracy: 0.7571 - val_loss: 2.6050 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03964: val_accuracy did not improve from 0.55829\n",
            "Epoch 3965/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4301 - accuracy: 0.7630 - val_loss: 2.6556 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 03965: val_accuracy did not improve from 0.55829\n",
            "Epoch 3966/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4295 - accuracy: 0.7616 - val_loss: 2.6119 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03966: val_accuracy did not improve from 0.55829\n",
            "Epoch 3967/5000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.4322 - accuracy: 0.7576 - val_loss: 2.5271 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 03967: val_accuracy did not improve from 0.55829\n",
            "Epoch 3968/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4279 - accuracy: 0.7601 - val_loss: 2.6577 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 03968: val_accuracy did not improve from 0.55829\n",
            "Epoch 3969/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4288 - accuracy: 0.7636 - val_loss: 2.7571 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 03969: val_accuracy did not improve from 0.55829\n",
            "Epoch 3970/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4355 - accuracy: 0.7562 - val_loss: 2.6276 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 03970: val_accuracy did not improve from 0.55829\n",
            "Epoch 3971/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4213 - accuracy: 0.7647 - val_loss: 2.5976 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03971: val_accuracy did not improve from 0.55829\n",
            "Epoch 3972/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4378 - accuracy: 0.7585 - val_loss: 2.6197 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 03972: val_accuracy did not improve from 0.55829\n",
            "Epoch 3973/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4583 - accuracy: 0.7418 - val_loss: 2.4420 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 03973: val_accuracy did not improve from 0.55829\n",
            "Epoch 3974/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4414 - accuracy: 0.7553 - val_loss: 2.3304 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 03974: val_accuracy did not improve from 0.55829\n",
            "Epoch 3975/5000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.4552 - accuracy: 0.7480 - val_loss: 2.4342 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 03975: val_accuracy did not improve from 0.55829\n",
            "Epoch 3976/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4373 - accuracy: 0.7527 - val_loss: 2.5797 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03976: val_accuracy did not improve from 0.55829\n",
            "Epoch 3977/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4520 - accuracy: 0.7540 - val_loss: 2.4588 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 03977: val_accuracy did not improve from 0.55829\n",
            "Epoch 3978/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.4578 - accuracy: 0.7489 - val_loss: 2.3318 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 03978: val_accuracy did not improve from 0.55829\n",
            "Epoch 3979/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4408 - accuracy: 0.7553 - val_loss: 2.4173 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 03979: val_accuracy did not improve from 0.55829\n",
            "Epoch 3980/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4573 - accuracy: 0.7431 - val_loss: 2.5078 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 03980: val_accuracy did not improve from 0.55829\n",
            "Epoch 3981/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4437 - accuracy: 0.7560 - val_loss: 2.5835 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 03981: val_accuracy did not improve from 0.55829\n",
            "Epoch 3982/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4340 - accuracy: 0.7538 - val_loss: 2.7278 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 03982: val_accuracy did not improve from 0.55829\n",
            "Epoch 3983/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4547 - accuracy: 0.7379 - val_loss: 2.6054 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 03983: val_accuracy did not improve from 0.55829\n",
            "Epoch 3984/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4449 - accuracy: 0.7567 - val_loss: 2.5032 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 03984: val_accuracy did not improve from 0.55829\n",
            "Epoch 3985/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4490 - accuracy: 0.7594 - val_loss: 2.6646 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 03985: val_accuracy did not improve from 0.55829\n",
            "Epoch 3986/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.4467 - accuracy: 0.7489 - val_loss: 2.8269 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 03986: val_accuracy did not improve from 0.55829\n",
            "Epoch 3987/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4337 - accuracy: 0.7600 - val_loss: 2.7000 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03987: val_accuracy did not improve from 0.55829\n",
            "Epoch 3988/5000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.4545 - accuracy: 0.7444 - val_loss: 2.6585 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 03988: val_accuracy did not improve from 0.55829\n",
            "Epoch 3989/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4474 - accuracy: 0.7520 - val_loss: 2.7215 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 03989: val_accuracy did not improve from 0.55829\n",
            "Epoch 3990/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4502 - accuracy: 0.7484 - val_loss: 2.7744 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 03990: val_accuracy did not improve from 0.55829\n",
            "Epoch 3991/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4515 - accuracy: 0.7491 - val_loss: 2.7307 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 03991: val_accuracy did not improve from 0.55829\n",
            "Epoch 3992/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4237 - accuracy: 0.7658 - val_loss: 2.7990 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 03992: val_accuracy did not improve from 0.55829\n",
            "Epoch 3993/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4511 - accuracy: 0.7466 - val_loss: 2.5976 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 03993: val_accuracy did not improve from 0.55829\n",
            "Epoch 3994/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4435 - accuracy: 0.7496 - val_loss: 2.5675 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 03994: val_accuracy did not improve from 0.55829\n",
            "Epoch 3995/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4337 - accuracy: 0.7611 - val_loss: 2.6890 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03995: val_accuracy did not improve from 0.55829\n",
            "Epoch 3996/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4593 - accuracy: 0.7504 - val_loss: 2.5817 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 03996: val_accuracy did not improve from 0.55829\n",
            "Epoch 3997/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4766 - accuracy: 0.7337 - val_loss: 2.6345 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 03997: val_accuracy did not improve from 0.55829\n",
            "Epoch 3998/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4755 - accuracy: 0.7473 - val_loss: 2.6144 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 03998: val_accuracy did not improve from 0.55829\n",
            "Epoch 3999/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4618 - accuracy: 0.7453 - val_loss: 2.5202 - val_accuracy: 0.4888\n",
            "\n",
            "Epoch 03999: val_accuracy did not improve from 0.55829\n",
            "Epoch 4000/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.4536 - accuracy: 0.7489 - val_loss: 2.5420 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04000: val_accuracy did not improve from 0.55829\n",
            "Epoch 4001/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4696 - accuracy: 0.7422 - val_loss: 2.4728 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04001: val_accuracy did not improve from 0.55829\n",
            "Epoch 4002/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4429 - accuracy: 0.7533 - val_loss: 2.5068 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04002: val_accuracy did not improve from 0.55829\n",
            "Epoch 4003/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4471 - accuracy: 0.7540 - val_loss: 2.5051 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04003: val_accuracy did not improve from 0.55829\n",
            "Epoch 4004/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4314 - accuracy: 0.7592 - val_loss: 2.5787 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04004: val_accuracy did not improve from 0.55829\n",
            "Epoch 4005/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4280 - accuracy: 0.7574 - val_loss: 2.6298 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04005: val_accuracy did not improve from 0.55829\n",
            "Epoch 4006/5000\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.4380 - accuracy: 0.7558 - val_loss: 2.5676 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04006: val_accuracy did not improve from 0.55829\n",
            "Epoch 4007/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4255 - accuracy: 0.7574 - val_loss: 2.5253 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04007: val_accuracy did not improve from 0.55829\n",
            "Epoch 4008/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4328 - accuracy: 0.7525 - val_loss: 2.5773 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04008: val_accuracy did not improve from 0.55829\n",
            "Epoch 4009/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4263 - accuracy: 0.7614 - val_loss: 2.5822 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04009: val_accuracy did not improve from 0.55829\n",
            "Epoch 4010/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4200 - accuracy: 0.7650 - val_loss: 2.7022 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04010: val_accuracy did not improve from 0.55829\n",
            "Epoch 4011/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4255 - accuracy: 0.7650 - val_loss: 2.8285 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04011: val_accuracy did not improve from 0.55829\n",
            "Epoch 4012/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4185 - accuracy: 0.7656 - val_loss: 2.8817 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04012: val_accuracy did not improve from 0.55829\n",
            "Epoch 4013/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4198 - accuracy: 0.7703 - val_loss: 2.9384 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04013: val_accuracy did not improve from 0.55829\n",
            "Epoch 4014/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4235 - accuracy: 0.7605 - val_loss: 3.0078 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04014: val_accuracy did not improve from 0.55829\n",
            "Epoch 4015/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.4231 - accuracy: 0.7638 - val_loss: 3.0089 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04015: val_accuracy did not improve from 0.55829\n",
            "Epoch 4016/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4184 - accuracy: 0.7690 - val_loss: 2.9229 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04016: val_accuracy did not improve from 0.55829\n",
            "Epoch 4017/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4182 - accuracy: 0.7645 - val_loss: 2.8368 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04017: val_accuracy did not improve from 0.55829\n",
            "Epoch 4018/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4193 - accuracy: 0.7678 - val_loss: 2.8184 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04018: val_accuracy did not improve from 0.55829\n",
            "Epoch 4019/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4161 - accuracy: 0.7679 - val_loss: 2.8440 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04019: val_accuracy did not improve from 0.55829\n",
            "Epoch 4020/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4202 - accuracy: 0.7716 - val_loss: 2.8474 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04020: val_accuracy did not improve from 0.55829\n",
            "Epoch 4021/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4186 - accuracy: 0.7643 - val_loss: 2.8833 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04021: val_accuracy did not improve from 0.55829\n",
            "Epoch 4022/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4151 - accuracy: 0.7708 - val_loss: 2.8903 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04022: val_accuracy did not improve from 0.55829\n",
            "Epoch 4023/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4152 - accuracy: 0.7726 - val_loss: 2.9471 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04023: val_accuracy did not improve from 0.55829\n",
            "Epoch 4024/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4210 - accuracy: 0.7690 - val_loss: 3.1172 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04024: val_accuracy did not improve from 0.55829\n",
            "Epoch 4025/5000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.4164 - accuracy: 0.7639 - val_loss: 3.1780 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04025: val_accuracy did not improve from 0.55829\n",
            "Epoch 4026/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4150 - accuracy: 0.7688 - val_loss: 2.9183 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04026: val_accuracy did not improve from 0.55829\n",
            "Epoch 4027/5000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.4149 - accuracy: 0.7676 - val_loss: 2.7816 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04027: val_accuracy did not improve from 0.55829\n",
            "Epoch 4028/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4160 - accuracy: 0.7690 - val_loss: 2.8144 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04028: val_accuracy did not improve from 0.55829\n",
            "Epoch 4029/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4119 - accuracy: 0.7743 - val_loss: 2.9852 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04029: val_accuracy did not improve from 0.55829\n",
            "Epoch 4030/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4130 - accuracy: 0.7739 - val_loss: 3.1098 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04030: val_accuracy did not improve from 0.55829\n",
            "Epoch 4031/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4093 - accuracy: 0.7728 - val_loss: 3.0885 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04031: val_accuracy did not improve from 0.55829\n",
            "Epoch 4032/5000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.4132 - accuracy: 0.7714 - val_loss: 3.0538 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04032: val_accuracy did not improve from 0.55829\n",
            "Epoch 4033/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4114 - accuracy: 0.7725 - val_loss: 3.0681 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04033: val_accuracy did not improve from 0.55829\n",
            "Epoch 4034/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.4100 - accuracy: 0.7685 - val_loss: 3.0635 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04034: val_accuracy did not improve from 0.55829\n",
            "Epoch 4035/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4087 - accuracy: 0.7710 - val_loss: 3.0906 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04035: val_accuracy did not improve from 0.55829\n",
            "Epoch 4036/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4117 - accuracy: 0.7763 - val_loss: 3.1014 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04036: val_accuracy did not improve from 0.55829\n",
            "Epoch 4037/5000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.4072 - accuracy: 0.7723 - val_loss: 3.1017 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04037: val_accuracy did not improve from 0.55829\n",
            "Epoch 4038/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4096 - accuracy: 0.7707 - val_loss: 3.1151 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04038: val_accuracy did not improve from 0.55829\n",
            "Epoch 4039/5000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.4049 - accuracy: 0.7786 - val_loss: 3.1574 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04039: val_accuracy did not improve from 0.55829\n",
            "Epoch 4040/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4151 - accuracy: 0.7678 - val_loss: 3.2691 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04040: val_accuracy did not improve from 0.55829\n",
            "Epoch 4041/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4119 - accuracy: 0.7681 - val_loss: 3.2200 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04041: val_accuracy did not improve from 0.55829\n",
            "Epoch 4042/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4085 - accuracy: 0.7783 - val_loss: 3.0628 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04042: val_accuracy did not improve from 0.55829\n",
            "Epoch 4043/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4060 - accuracy: 0.7757 - val_loss: 3.0664 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04043: val_accuracy did not improve from 0.55829\n",
            "Epoch 4044/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4067 - accuracy: 0.7815 - val_loss: 3.2014 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04044: val_accuracy did not improve from 0.55829\n",
            "Epoch 4045/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4080 - accuracy: 0.7692 - val_loss: 3.2285 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04045: val_accuracy did not improve from 0.55829\n",
            "Epoch 4046/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4071 - accuracy: 0.7746 - val_loss: 3.0793 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04046: val_accuracy did not improve from 0.55829\n",
            "Epoch 4047/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4065 - accuracy: 0.7817 - val_loss: 2.9452 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04047: val_accuracy did not improve from 0.55829\n",
            "Epoch 4048/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4014 - accuracy: 0.7826 - val_loss: 3.0690 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04048: val_accuracy did not improve from 0.55829\n",
            "Epoch 4049/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4069 - accuracy: 0.7745 - val_loss: 3.2009 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04049: val_accuracy did not improve from 0.55829\n",
            "Epoch 4050/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4047 - accuracy: 0.7723 - val_loss: 3.2246 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04050: val_accuracy did not improve from 0.55829\n",
            "Epoch 4051/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4054 - accuracy: 0.7777 - val_loss: 3.1761 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04051: val_accuracy did not improve from 0.55829\n",
            "Epoch 4052/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4061 - accuracy: 0.7755 - val_loss: 3.0791 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04052: val_accuracy did not improve from 0.55829\n",
            "Epoch 4053/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4060 - accuracy: 0.7717 - val_loss: 3.1082 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04053: val_accuracy did not improve from 0.55829\n",
            "Epoch 4054/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4054 - accuracy: 0.7741 - val_loss: 3.2602 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04054: val_accuracy did not improve from 0.55829\n",
            "Epoch 4055/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4017 - accuracy: 0.7781 - val_loss: 3.2759 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04055: val_accuracy did not improve from 0.55829\n",
            "Epoch 4056/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4059 - accuracy: 0.7812 - val_loss: 3.1662 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04056: val_accuracy did not improve from 0.55829\n",
            "Epoch 4057/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4028 - accuracy: 0.7737 - val_loss: 3.1560 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 04057: val_accuracy did not improve from 0.55829\n",
            "Epoch 4058/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4050 - accuracy: 0.7777 - val_loss: 3.2361 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04058: val_accuracy did not improve from 0.55829\n",
            "Epoch 4059/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4016 - accuracy: 0.7788 - val_loss: 3.2579 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04059: val_accuracy did not improve from 0.55829\n",
            "Epoch 4060/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4002 - accuracy: 0.7821 - val_loss: 3.1843 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04060: val_accuracy did not improve from 0.55829\n",
            "Epoch 4061/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4032 - accuracy: 0.7810 - val_loss: 3.2369 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04061: val_accuracy did not improve from 0.55829\n",
            "Epoch 4062/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.3981 - accuracy: 0.7799 - val_loss: 3.2473 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04062: val_accuracy did not improve from 0.55829\n",
            "Epoch 4063/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4019 - accuracy: 0.7779 - val_loss: 3.2541 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04063: val_accuracy did not improve from 0.55829\n",
            "Epoch 4064/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3994 - accuracy: 0.7786 - val_loss: 3.3266 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04064: val_accuracy did not improve from 0.55829\n",
            "Epoch 4065/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4047 - accuracy: 0.7696 - val_loss: 3.3206 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04065: val_accuracy did not improve from 0.55829\n",
            "Epoch 4066/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4024 - accuracy: 0.7783 - val_loss: 3.2183 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04066: val_accuracy did not improve from 0.55829\n",
            "Epoch 4067/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3979 - accuracy: 0.7837 - val_loss: 3.2440 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04067: val_accuracy did not improve from 0.55829\n",
            "Epoch 4068/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4005 - accuracy: 0.7781 - val_loss: 3.4010 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04068: val_accuracy did not improve from 0.55829\n",
            "Epoch 4069/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4115 - accuracy: 0.7725 - val_loss: 3.4476 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04069: val_accuracy did not improve from 0.55829\n",
            "Epoch 4070/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4519 - accuracy: 0.7504 - val_loss: 2.8463 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04070: val_accuracy did not improve from 0.55829\n",
            "Epoch 4071/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4935 - accuracy: 0.7286 - val_loss: 2.9620 - val_accuracy: 0.5243\n",
            "\n",
            "Epoch 04071: val_accuracy did not improve from 0.55829\n",
            "Epoch 4072/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4500 - accuracy: 0.7509 - val_loss: 3.2910 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04072: val_accuracy did not improve from 0.55829\n",
            "Epoch 4073/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4664 - accuracy: 0.7409 - val_loss: 3.0775 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 04073: val_accuracy did not improve from 0.55829\n",
            "Epoch 4074/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4796 - accuracy: 0.7404 - val_loss: 2.9620 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04074: val_accuracy did not improve from 0.55829\n",
            "Epoch 4075/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.5031 - accuracy: 0.7330 - val_loss: 3.3006 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04075: val_accuracy did not improve from 0.55829\n",
            "Epoch 4076/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4897 - accuracy: 0.7420 - val_loss: 3.3949 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 04076: val_accuracy did not improve from 0.55829\n",
            "Epoch 4077/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4544 - accuracy: 0.7582 - val_loss: 3.1890 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04077: val_accuracy did not improve from 0.55829\n",
            "Epoch 4078/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.5009 - accuracy: 0.7402 - val_loss: 2.7798 - val_accuracy: 0.4823\n",
            "\n",
            "Epoch 04078: val_accuracy did not improve from 0.55829\n",
            "Epoch 4079/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6286 - accuracy: 0.6846 - val_loss: 2.1662 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04079: val_accuracy did not improve from 0.55829\n",
            "Epoch 4080/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.5984 - accuracy: 0.6801 - val_loss: 1.9676 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04080: val_accuracy did not improve from 0.55829\n",
            "Epoch 4081/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.5864 - accuracy: 0.6819 - val_loss: 1.9965 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04081: val_accuracy did not improve from 0.55829\n",
            "Epoch 4082/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6001 - accuracy: 0.6755 - val_loss: 2.2512 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04082: val_accuracy did not improve from 0.55829\n",
            "Epoch 4083/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.5807 - accuracy: 0.6793 - val_loss: 2.4877 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04083: val_accuracy did not improve from 0.55829\n",
            "Epoch 4084/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.5851 - accuracy: 0.6893 - val_loss: 2.7329 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 04084: val_accuracy did not improve from 0.55829\n",
            "Epoch 4085/5000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.5440 - accuracy: 0.7053 - val_loss: 3.1273 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04085: val_accuracy did not improve from 0.55829\n",
            "Epoch 4086/5000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.5803 - accuracy: 0.6940 - val_loss: 2.8354 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04086: val_accuracy did not improve from 0.55829\n",
            "Epoch 4087/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.5658 - accuracy: 0.6920 - val_loss: 2.7624 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04087: val_accuracy did not improve from 0.55829\n",
            "Epoch 4088/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.5402 - accuracy: 0.7116 - val_loss: 2.4702 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04088: val_accuracy did not improve from 0.55829\n",
            "Epoch 4089/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.5020 - accuracy: 0.7279 - val_loss: 2.4110 - val_accuracy: 0.4873\n",
            "\n",
            "Epoch 04089: val_accuracy did not improve from 0.55829\n",
            "Epoch 4090/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.5153 - accuracy: 0.7234 - val_loss: 1.7847 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04090: val_accuracy did not improve from 0.55829\n",
            "Epoch 4091/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4994 - accuracy: 0.7263 - val_loss: 1.4175 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04091: val_accuracy did not improve from 0.55829\n",
            "Epoch 4092/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4922 - accuracy: 0.7304 - val_loss: 1.4224 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04092: val_accuracy did not improve from 0.55829\n",
            "Epoch 4093/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.5169 - accuracy: 0.7179 - val_loss: 1.5384 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04093: val_accuracy did not improve from 0.55829\n",
            "Epoch 4094/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4723 - accuracy: 0.7464 - val_loss: 1.7072 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 04094: val_accuracy did not improve from 0.55829\n",
            "Epoch 4095/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.5107 - accuracy: 0.7393 - val_loss: 3.2226 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 04095: val_accuracy did not improve from 0.55829\n",
            "Epoch 4096/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.5441 - accuracy: 0.7091 - val_loss: 7.2939 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04096: val_accuracy did not improve from 0.55829\n",
            "Epoch 4097/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.9691 - accuracy: 0.7159 - val_loss: 2.5960 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04097: val_accuracy did not improve from 0.55829\n",
            "Epoch 4098/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.5879 - accuracy: 0.6871 - val_loss: 1.2313 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04098: val_accuracy did not improve from 0.55829\n",
            "Epoch 4099/5000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.6304 - accuracy: 0.6571 - val_loss: 0.9757 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04099: val_accuracy did not improve from 0.55829\n",
            "Epoch 4100/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6214 - accuracy: 0.6525 - val_loss: 0.9048 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 04100: val_accuracy did not improve from 0.55829\n",
            "Epoch 4101/5000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.6378 - accuracy: 0.6362 - val_loss: 0.8997 - val_accuracy: 0.5257\n",
            "\n",
            "Epoch 04101: val_accuracy did not improve from 0.55829\n",
            "Epoch 4102/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.6607 - accuracy: 0.6221 - val_loss: 0.9026 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04102: val_accuracy did not improve from 0.55829\n",
            "Epoch 4103/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6551 - accuracy: 0.6205 - val_loss: 0.9095 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04103: val_accuracy did not improve from 0.55829\n",
            "Epoch 4104/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.6407 - accuracy: 0.6185 - val_loss: 0.8929 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 04104: val_accuracy did not improve from 0.55829\n",
            "Epoch 4105/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6332 - accuracy: 0.6207 - val_loss: 0.8858 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 04105: val_accuracy did not improve from 0.55829\n",
            "Epoch 4106/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6369 - accuracy: 0.6196 - val_loss: 0.8794 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 04106: val_accuracy did not improve from 0.55829\n",
            "Epoch 4107/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6263 - accuracy: 0.6221 - val_loss: 0.8802 - val_accuracy: 0.5315\n",
            "\n",
            "Epoch 04107: val_accuracy did not improve from 0.55829\n",
            "Epoch 4108/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6222 - accuracy: 0.6254 - val_loss: 0.8895 - val_accuracy: 0.5272\n",
            "\n",
            "Epoch 04108: val_accuracy did not improve from 0.55829\n",
            "Epoch 4109/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6145 - accuracy: 0.6337 - val_loss: 0.9116 - val_accuracy: 0.5329\n",
            "\n",
            "Epoch 04109: val_accuracy did not improve from 0.55829\n",
            "Epoch 4110/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6201 - accuracy: 0.6355 - val_loss: 0.9355 - val_accuracy: 0.5264\n",
            "\n",
            "Epoch 04110: val_accuracy did not improve from 0.55829\n",
            "Epoch 4111/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6112 - accuracy: 0.6335 - val_loss: 0.9467 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 04111: val_accuracy did not improve from 0.55829\n",
            "Epoch 4112/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6121 - accuracy: 0.6368 - val_loss: 0.9584 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04112: val_accuracy did not improve from 0.55829\n",
            "Epoch 4113/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.5993 - accuracy: 0.6415 - val_loss: 0.9550 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04113: val_accuracy did not improve from 0.55829\n",
            "Epoch 4114/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6043 - accuracy: 0.6400 - val_loss: 0.9391 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04114: val_accuracy did not improve from 0.55829\n",
            "Epoch 4115/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6025 - accuracy: 0.6404 - val_loss: 0.9243 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04115: val_accuracy did not improve from 0.55829\n",
            "Epoch 4116/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.5976 - accuracy: 0.6411 - val_loss: 0.9230 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04116: val_accuracy did not improve from 0.55829\n",
            "Epoch 4117/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.5909 - accuracy: 0.6404 - val_loss: 0.9527 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04117: val_accuracy did not improve from 0.55829\n",
            "Epoch 4118/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.5893 - accuracy: 0.6498 - val_loss: 0.9838 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04118: val_accuracy did not improve from 0.55829\n",
            "Epoch 4119/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.5883 - accuracy: 0.6487 - val_loss: 1.0185 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 04119: val_accuracy did not improve from 0.55829\n",
            "Epoch 4120/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.5777 - accuracy: 0.6596 - val_loss: 1.0457 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04120: val_accuracy did not improve from 0.55829\n",
            "Epoch 4121/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.5786 - accuracy: 0.6591 - val_loss: 1.0548 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04121: val_accuracy did not improve from 0.55829\n",
            "Epoch 4122/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.5734 - accuracy: 0.6614 - val_loss: 1.0526 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04122: val_accuracy did not improve from 0.55829\n",
            "Epoch 4123/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.5636 - accuracy: 0.6708 - val_loss: 1.0630 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04123: val_accuracy did not improve from 0.55829\n",
            "Epoch 4124/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.5670 - accuracy: 0.6650 - val_loss: 1.0809 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04124: val_accuracy did not improve from 0.55829\n",
            "Epoch 4125/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.5605 - accuracy: 0.6719 - val_loss: 1.0903 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04125: val_accuracy did not improve from 0.55829\n",
            "Epoch 4126/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.5498 - accuracy: 0.6688 - val_loss: 1.1123 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04126: val_accuracy did not improve from 0.55829\n",
            "Epoch 4127/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.5507 - accuracy: 0.6708 - val_loss: 1.1455 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04127: val_accuracy did not improve from 0.55829\n",
            "Epoch 4128/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.5443 - accuracy: 0.6817 - val_loss: 1.1822 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04128: val_accuracy did not improve from 0.55829\n",
            "Epoch 4129/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.5452 - accuracy: 0.6822 - val_loss: 1.1880 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04129: val_accuracy did not improve from 0.55829\n",
            "Epoch 4130/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.5411 - accuracy: 0.6808 - val_loss: 1.1943 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04130: val_accuracy did not improve from 0.55829\n",
            "Epoch 4131/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.5389 - accuracy: 0.6871 - val_loss: 1.2065 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04131: val_accuracy did not improve from 0.55829\n",
            "Epoch 4132/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.5348 - accuracy: 0.6871 - val_loss: 1.2329 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04132: val_accuracy did not improve from 0.55829\n",
            "Epoch 4133/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.5353 - accuracy: 0.6870 - val_loss: 1.2385 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04133: val_accuracy did not improve from 0.55829\n",
            "Epoch 4134/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.5250 - accuracy: 0.6917 - val_loss: 1.2473 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04134: val_accuracy did not improve from 0.55829\n",
            "Epoch 4135/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.5256 - accuracy: 0.6929 - val_loss: 1.2591 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04135: val_accuracy did not improve from 0.55829\n",
            "Epoch 4136/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.5257 - accuracy: 0.6953 - val_loss: 1.2670 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04136: val_accuracy did not improve from 0.55829\n",
            "Epoch 4137/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.5170 - accuracy: 0.6962 - val_loss: 1.2801 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04137: val_accuracy did not improve from 0.55829\n",
            "Epoch 4138/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.5156 - accuracy: 0.7033 - val_loss: 1.2978 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04138: val_accuracy did not improve from 0.55829\n",
            "Epoch 4139/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.5115 - accuracy: 0.7020 - val_loss: 1.3313 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04139: val_accuracy did not improve from 0.55829\n",
            "Epoch 4140/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.5106 - accuracy: 0.7038 - val_loss: 1.3592 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04140: val_accuracy did not improve from 0.55829\n",
            "Epoch 4141/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.5050 - accuracy: 0.7116 - val_loss: 1.3656 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04141: val_accuracy did not improve from 0.55829\n",
            "Epoch 4142/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.5022 - accuracy: 0.7085 - val_loss: 1.3570 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04142: val_accuracy did not improve from 0.55829\n",
            "Epoch 4143/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.5036 - accuracy: 0.7125 - val_loss: 1.3679 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04143: val_accuracy did not improve from 0.55829\n",
            "Epoch 4144/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.5000 - accuracy: 0.7158 - val_loss: 1.3274 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04144: val_accuracy did not improve from 0.55829\n",
            "Epoch 4145/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4976 - accuracy: 0.7176 - val_loss: 1.3612 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04145: val_accuracy did not improve from 0.55829\n",
            "Epoch 4146/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4965 - accuracy: 0.7139 - val_loss: 1.4474 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04146: val_accuracy did not improve from 0.55829\n",
            "Epoch 4147/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4963 - accuracy: 0.7143 - val_loss: 1.4776 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04147: val_accuracy did not improve from 0.55829\n",
            "Epoch 4148/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4972 - accuracy: 0.7096 - val_loss: 1.4845 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04148: val_accuracy did not improve from 0.55829\n",
            "Epoch 4149/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4891 - accuracy: 0.7170 - val_loss: 1.4601 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04149: val_accuracy did not improve from 0.55829\n",
            "Epoch 4150/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4926 - accuracy: 0.7141 - val_loss: 1.4205 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04150: val_accuracy did not improve from 0.55829\n",
            "Epoch 4151/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4920 - accuracy: 0.7163 - val_loss: 1.4517 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04151: val_accuracy did not improve from 0.55829\n",
            "Epoch 4152/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4873 - accuracy: 0.7232 - val_loss: 1.5277 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04152: val_accuracy did not improve from 0.55829\n",
            "Epoch 4153/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4879 - accuracy: 0.7250 - val_loss: 1.5377 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04153: val_accuracy did not improve from 0.55829\n",
            "Epoch 4154/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4821 - accuracy: 0.7266 - val_loss: 1.5268 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04154: val_accuracy did not improve from 0.55829\n",
            "Epoch 4155/5000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.4857 - accuracy: 0.7234 - val_loss: 1.5454 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04155: val_accuracy did not improve from 0.55829\n",
            "Epoch 4156/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4809 - accuracy: 0.7279 - val_loss: 1.5708 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04156: val_accuracy did not improve from 0.55829\n",
            "Epoch 4157/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4788 - accuracy: 0.7263 - val_loss: 1.5512 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04157: val_accuracy did not improve from 0.55829\n",
            "Epoch 4158/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4769 - accuracy: 0.7293 - val_loss: 1.5721 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04158: val_accuracy did not improve from 0.55829\n",
            "Epoch 4159/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.4807 - accuracy: 0.7272 - val_loss: 1.6454 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04159: val_accuracy did not improve from 0.55829\n",
            "Epoch 4160/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4773 - accuracy: 0.7292 - val_loss: 1.6729 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04160: val_accuracy did not improve from 0.55829\n",
            "Epoch 4161/5000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.4775 - accuracy: 0.7283 - val_loss: 1.6228 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04161: val_accuracy did not improve from 0.55829\n",
            "Epoch 4162/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4751 - accuracy: 0.7306 - val_loss: 1.6105 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04162: val_accuracy did not improve from 0.55829\n",
            "Epoch 4163/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4752 - accuracy: 0.7292 - val_loss: 1.6905 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04163: val_accuracy did not improve from 0.55829\n",
            "Epoch 4164/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4730 - accuracy: 0.7346 - val_loss: 1.6861 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04164: val_accuracy did not improve from 0.55829\n",
            "Epoch 4165/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4711 - accuracy: 0.7319 - val_loss: 1.6714 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04165: val_accuracy did not improve from 0.55829\n",
            "Epoch 4166/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4686 - accuracy: 0.7321 - val_loss: 1.7149 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04166: val_accuracy did not improve from 0.55829\n",
            "Epoch 4167/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4700 - accuracy: 0.7322 - val_loss: 1.7264 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04167: val_accuracy did not improve from 0.55829\n",
            "Epoch 4168/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4688 - accuracy: 0.7380 - val_loss: 1.7107 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04168: val_accuracy did not improve from 0.55829\n",
            "Epoch 4169/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4656 - accuracy: 0.7348 - val_loss: 1.6922 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04169: val_accuracy did not improve from 0.55829\n",
            "Epoch 4170/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4650 - accuracy: 0.7359 - val_loss: 1.6980 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04170: val_accuracy did not improve from 0.55829\n",
            "Epoch 4171/5000\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.4663 - accuracy: 0.7371 - val_loss: 1.7389 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04171: val_accuracy did not improve from 0.55829\n",
            "Epoch 4172/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4616 - accuracy: 0.7447 - val_loss: 1.7384 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04172: val_accuracy did not improve from 0.55829\n",
            "Epoch 4173/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4636 - accuracy: 0.7402 - val_loss: 1.7261 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04173: val_accuracy did not improve from 0.55829\n",
            "Epoch 4174/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4609 - accuracy: 0.7453 - val_loss: 1.7447 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04174: val_accuracy did not improve from 0.55829\n",
            "Epoch 4175/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4608 - accuracy: 0.7355 - val_loss: 1.7455 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04175: val_accuracy did not improve from 0.55829\n",
            "Epoch 4176/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4609 - accuracy: 0.7415 - val_loss: 1.7187 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04176: val_accuracy did not improve from 0.55829\n",
            "Epoch 4177/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4615 - accuracy: 0.7366 - val_loss: 1.7037 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04177: val_accuracy did not improve from 0.55829\n",
            "Epoch 4178/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4599 - accuracy: 0.7391 - val_loss: 1.7805 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04178: val_accuracy did not improve from 0.55829\n",
            "Epoch 4179/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4586 - accuracy: 0.7415 - val_loss: 1.7869 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04179: val_accuracy did not improve from 0.55829\n",
            "Epoch 4180/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4582 - accuracy: 0.7418 - val_loss: 1.7887 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04180: val_accuracy did not improve from 0.55829\n",
            "Epoch 4181/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4544 - accuracy: 0.7464 - val_loss: 1.7683 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04181: val_accuracy did not improve from 0.55829\n",
            "Epoch 4182/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4558 - accuracy: 0.7457 - val_loss: 1.8060 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04182: val_accuracy did not improve from 0.55829\n",
            "Epoch 4183/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4573 - accuracy: 0.7426 - val_loss: 1.8504 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04183: val_accuracy did not improve from 0.55829\n",
            "Epoch 4184/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4564 - accuracy: 0.7457 - val_loss: 1.7814 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04184: val_accuracy did not improve from 0.55829\n",
            "Epoch 4185/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4535 - accuracy: 0.7518 - val_loss: 1.7046 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04185: val_accuracy did not improve from 0.55829\n",
            "Epoch 4186/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4556 - accuracy: 0.7442 - val_loss: 1.7595 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04186: val_accuracy did not improve from 0.55829\n",
            "Epoch 4187/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.4537 - accuracy: 0.7495 - val_loss: 1.8556 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04187: val_accuracy did not improve from 0.55829\n",
            "Epoch 4188/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4564 - accuracy: 0.7447 - val_loss: 1.8602 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04188: val_accuracy did not improve from 0.55829\n",
            "Epoch 4189/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4541 - accuracy: 0.7429 - val_loss: 1.8260 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04189: val_accuracy did not improve from 0.55829\n",
            "Epoch 4190/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4505 - accuracy: 0.7482 - val_loss: 1.8093 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04190: val_accuracy did not improve from 0.55829\n",
            "Epoch 4191/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4481 - accuracy: 0.7478 - val_loss: 1.8214 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04191: val_accuracy did not improve from 0.55829\n",
            "Epoch 4192/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4428 - accuracy: 0.7533 - val_loss: 1.8879 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04192: val_accuracy did not improve from 0.55829\n",
            "Epoch 4193/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.4529 - accuracy: 0.7455 - val_loss: 1.8002 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04193: val_accuracy did not improve from 0.55829\n",
            "Epoch 4194/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4481 - accuracy: 0.7487 - val_loss: 1.7698 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04194: val_accuracy did not improve from 0.55829\n",
            "Epoch 4195/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4463 - accuracy: 0.7558 - val_loss: 1.8979 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04195: val_accuracy did not improve from 0.55829\n",
            "Epoch 4196/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4516 - accuracy: 0.7455 - val_loss: 1.8786 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04196: val_accuracy did not improve from 0.55829\n",
            "Epoch 4197/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4486 - accuracy: 0.7464 - val_loss: 1.7639 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04197: val_accuracy did not improve from 0.55829\n",
            "Epoch 4198/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4485 - accuracy: 0.7473 - val_loss: 1.8126 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04198: val_accuracy did not improve from 0.55829\n",
            "Epoch 4199/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4465 - accuracy: 0.7475 - val_loss: 1.9410 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04199: val_accuracy did not improve from 0.55829\n",
            "Epoch 4200/5000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.4498 - accuracy: 0.7467 - val_loss: 1.8471 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04200: val_accuracy did not improve from 0.55829\n",
            "Epoch 4201/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4468 - accuracy: 0.7507 - val_loss: 1.7981 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04201: val_accuracy did not improve from 0.55829\n",
            "Epoch 4202/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4454 - accuracy: 0.7480 - val_loss: 1.8473 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04202: val_accuracy did not improve from 0.55829\n",
            "Epoch 4203/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4430 - accuracy: 0.7511 - val_loss: 1.9073 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04203: val_accuracy did not improve from 0.55829\n",
            "Epoch 4204/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4437 - accuracy: 0.7545 - val_loss: 1.8516 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04204: val_accuracy did not improve from 0.55829\n",
            "Epoch 4205/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4435 - accuracy: 0.7507 - val_loss: 1.8796 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04205: val_accuracy did not improve from 0.55829\n",
            "Epoch 4206/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4405 - accuracy: 0.7605 - val_loss: 1.9129 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04206: val_accuracy did not improve from 0.55829\n",
            "Epoch 4207/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4398 - accuracy: 0.7538 - val_loss: 1.8909 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04207: val_accuracy did not improve from 0.55829\n",
            "Epoch 4208/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4442 - accuracy: 0.7511 - val_loss: 1.8353 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04208: val_accuracy did not improve from 0.55829\n",
            "Epoch 4209/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.4332 - accuracy: 0.7612 - val_loss: 1.8438 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04209: val_accuracy did not improve from 0.55829\n",
            "Epoch 4210/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.4394 - accuracy: 0.7560 - val_loss: 1.8955 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04210: val_accuracy did not improve from 0.55829\n",
            "Epoch 4211/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4382 - accuracy: 0.7576 - val_loss: 1.9291 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04211: val_accuracy did not improve from 0.55829\n",
            "Epoch 4212/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.4425 - accuracy: 0.7527 - val_loss: 1.8942 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04212: val_accuracy did not improve from 0.55829\n",
            "Epoch 4213/5000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.4381 - accuracy: 0.7571 - val_loss: 1.8796 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04213: val_accuracy did not improve from 0.55829\n",
            "Epoch 4214/5000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.4368 - accuracy: 0.7571 - val_loss: 1.9510 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04214: val_accuracy did not improve from 0.55829\n",
            "Epoch 4215/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4403 - accuracy: 0.7540 - val_loss: 2.0240 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04215: val_accuracy did not improve from 0.55829\n",
            "Epoch 4216/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4378 - accuracy: 0.7553 - val_loss: 1.8668 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04216: val_accuracy did not improve from 0.55829\n",
            "Epoch 4217/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4355 - accuracy: 0.7598 - val_loss: 1.8430 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04217: val_accuracy did not improve from 0.55829\n",
            "Epoch 4218/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4334 - accuracy: 0.7605 - val_loss: 1.9489 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04218: val_accuracy did not improve from 0.55829\n",
            "Epoch 4219/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4388 - accuracy: 0.7587 - val_loss: 1.9948 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04219: val_accuracy did not improve from 0.55829\n",
            "Epoch 4220/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4375 - accuracy: 0.7549 - val_loss: 2.0027 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04220: val_accuracy did not improve from 0.55829\n",
            "Epoch 4221/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4342 - accuracy: 0.7591 - val_loss: 1.8218 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04221: val_accuracy did not improve from 0.55829\n",
            "Epoch 4222/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4378 - accuracy: 0.7647 - val_loss: 1.8714 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04222: val_accuracy did not improve from 0.55829\n",
            "Epoch 4223/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4348 - accuracy: 0.7605 - val_loss: 2.0942 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04223: val_accuracy did not improve from 0.55829\n",
            "Epoch 4224/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.4334 - accuracy: 0.7625 - val_loss: 2.1010 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04224: val_accuracy did not improve from 0.55829\n",
            "Epoch 4225/5000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.4311 - accuracy: 0.7585 - val_loss: 2.0074 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04225: val_accuracy did not improve from 0.55829\n",
            "Epoch 4226/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4285 - accuracy: 0.7612 - val_loss: 1.9735 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04226: val_accuracy did not improve from 0.55829\n",
            "Epoch 4227/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4271 - accuracy: 0.7600 - val_loss: 1.9595 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04227: val_accuracy did not improve from 0.55829\n",
            "Epoch 4228/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4323 - accuracy: 0.7605 - val_loss: 2.0546 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04228: val_accuracy did not improve from 0.55829\n",
            "Epoch 4229/5000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.4317 - accuracy: 0.7636 - val_loss: 1.9655 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04229: val_accuracy did not improve from 0.55829\n",
            "Epoch 4230/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.4254 - accuracy: 0.7668 - val_loss: 1.9017 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04230: val_accuracy did not improve from 0.55829\n",
            "Epoch 4231/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4263 - accuracy: 0.7639 - val_loss: 2.0194 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04231: val_accuracy did not improve from 0.55829\n",
            "Epoch 4232/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4274 - accuracy: 0.7656 - val_loss: 2.1280 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04232: val_accuracy did not improve from 0.55829\n",
            "Epoch 4233/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4320 - accuracy: 0.7616 - val_loss: 1.9861 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04233: val_accuracy did not improve from 0.55829\n",
            "Epoch 4234/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4381 - accuracy: 0.7513 - val_loss: 1.9861 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04234: val_accuracy did not improve from 0.55829\n",
            "Epoch 4235/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4415 - accuracy: 0.7614 - val_loss: 1.8799 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04235: val_accuracy did not improve from 0.55829\n",
            "Epoch 4236/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4486 - accuracy: 0.7569 - val_loss: 1.9902 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04236: val_accuracy did not improve from 0.55829\n",
            "Epoch 4237/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.4458 - accuracy: 0.7527 - val_loss: 1.9752 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04237: val_accuracy did not improve from 0.55829\n",
            "Epoch 4238/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4332 - accuracy: 0.7609 - val_loss: 1.9349 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04238: val_accuracy did not improve from 0.55829\n",
            "Epoch 4239/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4391 - accuracy: 0.7572 - val_loss: 1.9209 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04239: val_accuracy did not improve from 0.55829\n",
            "Epoch 4240/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4640 - accuracy: 0.7384 - val_loss: 1.8066 - val_accuracy: 0.4888\n",
            "\n",
            "Epoch 04240: val_accuracy did not improve from 0.55829\n",
            "Epoch 4241/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4578 - accuracy: 0.7491 - val_loss: 1.8654 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04241: val_accuracy did not improve from 0.55829\n",
            "Epoch 4242/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4232 - accuracy: 0.7692 - val_loss: 2.0635 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04242: val_accuracy did not improve from 0.55829\n",
            "Epoch 4243/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4562 - accuracy: 0.7511 - val_loss: 1.9226 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 04243: val_accuracy did not improve from 0.55829\n",
            "Epoch 4244/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4834 - accuracy: 0.7326 - val_loss: 1.6764 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04244: val_accuracy did not improve from 0.55829\n",
            "Epoch 4245/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4701 - accuracy: 0.7482 - val_loss: 1.6860 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04245: val_accuracy did not improve from 0.55829\n",
            "Epoch 4246/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4334 - accuracy: 0.7716 - val_loss: 1.9406 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04246: val_accuracy did not improve from 0.55829\n",
            "Epoch 4247/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4594 - accuracy: 0.7484 - val_loss: 1.8488 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04247: val_accuracy did not improve from 0.55829\n",
            "Epoch 4248/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4351 - accuracy: 0.7592 - val_loss: 1.7670 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04248: val_accuracy did not improve from 0.55829\n",
            "Epoch 4249/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4430 - accuracy: 0.7525 - val_loss: 1.7957 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 04249: val_accuracy did not improve from 0.55829\n",
            "Epoch 4250/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4394 - accuracy: 0.7580 - val_loss: 1.9757 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04250: val_accuracy did not improve from 0.55829\n",
            "Epoch 4251/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4394 - accuracy: 0.7591 - val_loss: 1.8029 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04251: val_accuracy did not improve from 0.55829\n",
            "Epoch 4252/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.4275 - accuracy: 0.7690 - val_loss: 1.7215 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04252: val_accuracy did not improve from 0.55829\n",
            "Epoch 4253/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.4427 - accuracy: 0.7569 - val_loss: 1.8097 - val_accuracy: 0.4866\n",
            "\n",
            "Epoch 04253: val_accuracy did not improve from 0.55829\n",
            "Epoch 4254/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4307 - accuracy: 0.7598 - val_loss: 1.9526 - val_accuracy: 0.4837\n",
            "\n",
            "Epoch 04254: val_accuracy did not improve from 0.55829\n",
            "Epoch 4255/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4381 - accuracy: 0.7562 - val_loss: 1.9248 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04255: val_accuracy did not improve from 0.55829\n",
            "Epoch 4256/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4212 - accuracy: 0.7665 - val_loss: 1.9286 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04256: val_accuracy did not improve from 0.55829\n",
            "Epoch 4257/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4225 - accuracy: 0.7643 - val_loss: 1.9861 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04257: val_accuracy did not improve from 0.55829\n",
            "Epoch 4258/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.4246 - accuracy: 0.7649 - val_loss: 1.9751 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04258: val_accuracy did not improve from 0.55829\n",
            "Epoch 4259/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4244 - accuracy: 0.7775 - val_loss: 1.9553 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04259: val_accuracy did not improve from 0.55829\n",
            "Epoch 4260/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4239 - accuracy: 0.7699 - val_loss: 1.9470 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04260: val_accuracy did not improve from 0.55829\n",
            "Epoch 4261/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4159 - accuracy: 0.7730 - val_loss: 1.9465 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04261: val_accuracy did not improve from 0.55829\n",
            "Epoch 4262/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4175 - accuracy: 0.7683 - val_loss: 1.8790 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04262: val_accuracy did not improve from 0.55829\n",
            "Epoch 4263/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4178 - accuracy: 0.7703 - val_loss: 1.8501 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04263: val_accuracy did not improve from 0.55829\n",
            "Epoch 4264/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4231 - accuracy: 0.7710 - val_loss: 1.8443 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04264: val_accuracy did not improve from 0.55829\n",
            "Epoch 4265/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4164 - accuracy: 0.7699 - val_loss: 1.9020 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04265: val_accuracy did not improve from 0.55829\n",
            "Epoch 4266/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4182 - accuracy: 0.7688 - val_loss: 1.9990 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04266: val_accuracy did not improve from 0.55829\n",
            "Epoch 4267/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4125 - accuracy: 0.7663 - val_loss: 2.0482 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04267: val_accuracy did not improve from 0.55829\n",
            "Epoch 4268/5000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.4128 - accuracy: 0.7705 - val_loss: 1.9575 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04268: val_accuracy did not improve from 0.55829\n",
            "Epoch 4269/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4118 - accuracy: 0.7777 - val_loss: 1.9797 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04269: val_accuracy did not improve from 0.55829\n",
            "Epoch 4270/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4147 - accuracy: 0.7707 - val_loss: 2.0245 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04270: val_accuracy did not improve from 0.55829\n",
            "Epoch 4271/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4144 - accuracy: 0.7716 - val_loss: 1.9248 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04271: val_accuracy did not improve from 0.55829\n",
            "Epoch 4272/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4185 - accuracy: 0.7717 - val_loss: 1.9771 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04272: val_accuracy did not improve from 0.55829\n",
            "Epoch 4273/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4160 - accuracy: 0.7741 - val_loss: 2.0500 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04273: val_accuracy did not improve from 0.55829\n",
            "Epoch 4274/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4127 - accuracy: 0.7708 - val_loss: 2.1310 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04274: val_accuracy did not improve from 0.55829\n",
            "Epoch 4275/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4127 - accuracy: 0.7701 - val_loss: 2.2107 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04275: val_accuracy did not improve from 0.55829\n",
            "Epoch 4276/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4191 - accuracy: 0.7663 - val_loss: 1.9755 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04276: val_accuracy did not improve from 0.55829\n",
            "Epoch 4277/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4172 - accuracy: 0.7707 - val_loss: 1.9875 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04277: val_accuracy did not improve from 0.55829\n",
            "Epoch 4278/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4152 - accuracy: 0.7725 - val_loss: 2.0951 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04278: val_accuracy did not improve from 0.55829\n",
            "Epoch 4279/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4137 - accuracy: 0.7703 - val_loss: 2.1580 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04279: val_accuracy did not improve from 0.55829\n",
            "Epoch 4280/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4168 - accuracy: 0.7688 - val_loss: 2.1755 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04280: val_accuracy did not improve from 0.55829\n",
            "Epoch 4281/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4195 - accuracy: 0.7678 - val_loss: 2.0659 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04281: val_accuracy did not improve from 0.55829\n",
            "Epoch 4282/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4060 - accuracy: 0.7826 - val_loss: 1.9921 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 04282: val_accuracy did not improve from 0.55829\n",
            "Epoch 4283/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.4154 - accuracy: 0.7734 - val_loss: 2.1536 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04283: val_accuracy did not improve from 0.55829\n",
            "Epoch 4284/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4283 - accuracy: 0.7618 - val_loss: 2.0476 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04284: val_accuracy did not improve from 0.55829\n",
            "Epoch 4285/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4159 - accuracy: 0.7754 - val_loss: 2.0954 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04285: val_accuracy did not improve from 0.55829\n",
            "Epoch 4286/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4222 - accuracy: 0.7685 - val_loss: 2.2768 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04286: val_accuracy did not improve from 0.55829\n",
            "Epoch 4287/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4331 - accuracy: 0.7596 - val_loss: 1.9954 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04287: val_accuracy did not improve from 0.55829\n",
            "Epoch 4288/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4700 - accuracy: 0.7411 - val_loss: 2.0675 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04288: val_accuracy did not improve from 0.55829\n",
            "Epoch 4289/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.5035 - accuracy: 0.7208 - val_loss: 1.7015 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04289: val_accuracy did not improve from 0.55829\n",
            "Epoch 4290/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.4562 - accuracy: 0.7598 - val_loss: 1.7594 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04290: val_accuracy did not improve from 0.55829\n",
            "Epoch 4291/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4667 - accuracy: 0.7509 - val_loss: 2.0215 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04291: val_accuracy did not improve from 0.55829\n",
            "Epoch 4292/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4799 - accuracy: 0.7426 - val_loss: 1.8007 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 04292: val_accuracy did not improve from 0.55829\n",
            "Epoch 4293/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4435 - accuracy: 0.7583 - val_loss: 1.7956 - val_accuracy: 0.4888\n",
            "\n",
            "Epoch 04293: val_accuracy did not improve from 0.55829\n",
            "Epoch 4294/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4494 - accuracy: 0.7534 - val_loss: 2.1216 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 04294: val_accuracy did not improve from 0.55829\n",
            "Epoch 4295/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4591 - accuracy: 0.7471 - val_loss: 1.8096 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04295: val_accuracy did not improve from 0.55829\n",
            "Epoch 4296/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4334 - accuracy: 0.7618 - val_loss: 1.8137 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04296: val_accuracy did not improve from 0.55829\n",
            "Epoch 4297/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4416 - accuracy: 0.7543 - val_loss: 2.0000 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04297: val_accuracy did not improve from 0.55829\n",
            "Epoch 4298/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.4485 - accuracy: 0.7493 - val_loss: 1.8600 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04298: val_accuracy did not improve from 0.55829\n",
            "Epoch 4299/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4259 - accuracy: 0.7696 - val_loss: 1.7924 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04299: val_accuracy did not improve from 0.55829\n",
            "Epoch 4300/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4584 - accuracy: 0.7514 - val_loss: 2.0161 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04300: val_accuracy did not improve from 0.55829\n",
            "Epoch 4301/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4232 - accuracy: 0.7685 - val_loss: 2.1515 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04301: val_accuracy did not improve from 0.55829\n",
            "Epoch 4302/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4420 - accuracy: 0.7582 - val_loss: 1.9053 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04302: val_accuracy did not improve from 0.55829\n",
            "Epoch 4303/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.5033 - accuracy: 0.7281 - val_loss: 1.7981 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 04303: val_accuracy did not improve from 0.55829\n",
            "Epoch 4304/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.4699 - accuracy: 0.7370 - val_loss: 1.7137 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04304: val_accuracy did not improve from 0.55829\n",
            "Epoch 4305/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.5095 - accuracy: 0.7149 - val_loss: 1.6617 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 04305: val_accuracy did not improve from 0.55829\n",
            "Epoch 4306/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4805 - accuracy: 0.7355 - val_loss: 1.7794 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04306: val_accuracy did not improve from 0.55829\n",
            "Epoch 4307/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4766 - accuracy: 0.7395 - val_loss: 1.8836 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04307: val_accuracy did not improve from 0.55829\n",
            "Epoch 4308/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.4940 - accuracy: 0.7384 - val_loss: 1.9569 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04308: val_accuracy did not improve from 0.55829\n",
            "Epoch 4309/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4682 - accuracy: 0.7547 - val_loss: 1.8246 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04309: val_accuracy did not improve from 0.55829\n",
            "Epoch 4310/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4568 - accuracy: 0.7420 - val_loss: 1.8938 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04310: val_accuracy did not improve from 0.55829\n",
            "Epoch 4311/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4472 - accuracy: 0.7580 - val_loss: 1.9321 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04311: val_accuracy did not improve from 0.55829\n",
            "Epoch 4312/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4723 - accuracy: 0.7480 - val_loss: 1.5316 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04312: val_accuracy did not improve from 0.55829\n",
            "Epoch 4313/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4626 - accuracy: 0.7473 - val_loss: 1.6167 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04313: val_accuracy did not improve from 0.55829\n",
            "Epoch 4314/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4658 - accuracy: 0.7516 - val_loss: 1.9296 - val_accuracy: 0.4881\n",
            "\n",
            "Epoch 04314: val_accuracy did not improve from 0.55829\n",
            "Epoch 4315/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4696 - accuracy: 0.7406 - val_loss: 1.8515 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04315: val_accuracy did not improve from 0.55829\n",
            "Epoch 4316/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4421 - accuracy: 0.7545 - val_loss: 1.8363 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04316: val_accuracy did not improve from 0.55829\n",
            "Epoch 4317/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4704 - accuracy: 0.7435 - val_loss: 1.6739 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04317: val_accuracy did not improve from 0.55829\n",
            "Epoch 4318/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4398 - accuracy: 0.7591 - val_loss: 1.6522 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04318: val_accuracy did not improve from 0.55829\n",
            "Epoch 4319/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4720 - accuracy: 0.7478 - val_loss: 1.6979 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04319: val_accuracy did not improve from 0.55829\n",
            "Epoch 4320/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.4481 - accuracy: 0.7583 - val_loss: 1.7799 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04320: val_accuracy did not improve from 0.55829\n",
            "Epoch 4321/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4339 - accuracy: 0.7692 - val_loss: 1.8491 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04321: val_accuracy did not improve from 0.55829\n",
            "Epoch 4322/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4453 - accuracy: 0.7556 - val_loss: 1.8056 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 04322: val_accuracy did not improve from 0.55829\n",
            "Epoch 4323/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4303 - accuracy: 0.7630 - val_loss: 1.7299 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04323: val_accuracy did not improve from 0.55829\n",
            "Epoch 4324/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4371 - accuracy: 0.7627 - val_loss: 1.6731 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 04324: val_accuracy did not improve from 0.55829\n",
            "Epoch 4325/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4335 - accuracy: 0.7661 - val_loss: 1.7945 - val_accuracy: 0.4888\n",
            "\n",
            "Epoch 04325: val_accuracy did not improve from 0.55829\n",
            "Epoch 4326/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4302 - accuracy: 0.7643 - val_loss: 1.9925 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04326: val_accuracy did not improve from 0.55829\n",
            "Epoch 4327/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4178 - accuracy: 0.7710 - val_loss: 1.9784 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04327: val_accuracy did not improve from 0.55829\n",
            "Epoch 4328/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4351 - accuracy: 0.7605 - val_loss: 1.7277 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04328: val_accuracy did not improve from 0.55829\n",
            "Epoch 4329/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4216 - accuracy: 0.7719 - val_loss: 1.6964 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04329: val_accuracy did not improve from 0.55829\n",
            "Epoch 4330/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.4197 - accuracy: 0.7746 - val_loss: 1.7822 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04330: val_accuracy did not improve from 0.55829\n",
            "Epoch 4331/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4168 - accuracy: 0.7716 - val_loss: 1.9222 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04331: val_accuracy did not improve from 0.55829\n",
            "Epoch 4332/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4202 - accuracy: 0.7699 - val_loss: 2.0344 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 04332: val_accuracy did not improve from 0.55829\n",
            "Epoch 4333/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.4121 - accuracy: 0.7763 - val_loss: 2.0021 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 04333: val_accuracy did not improve from 0.55829\n",
            "Epoch 4334/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4134 - accuracy: 0.7743 - val_loss: 1.9006 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 04334: val_accuracy did not improve from 0.55829\n",
            "Epoch 4335/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4103 - accuracy: 0.7737 - val_loss: 1.9180 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04335: val_accuracy did not improve from 0.55829\n",
            "Epoch 4336/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4107 - accuracy: 0.7726 - val_loss: 1.9267 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04336: val_accuracy did not improve from 0.55829\n",
            "Epoch 4337/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4099 - accuracy: 0.7763 - val_loss: 1.9594 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04337: val_accuracy did not improve from 0.55829\n",
            "Epoch 4338/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4072 - accuracy: 0.7732 - val_loss: 2.0073 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04338: val_accuracy did not improve from 0.55829\n",
            "Epoch 4339/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4069 - accuracy: 0.7788 - val_loss: 2.0793 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04339: val_accuracy did not improve from 0.55829\n",
            "Epoch 4340/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.3985 - accuracy: 0.7846 - val_loss: 2.1463 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04340: val_accuracy did not improve from 0.55829\n",
            "Epoch 4341/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4041 - accuracy: 0.7777 - val_loss: 2.1180 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04341: val_accuracy did not improve from 0.55829\n",
            "Epoch 4342/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4027 - accuracy: 0.7766 - val_loss: 2.1174 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04342: val_accuracy did not improve from 0.55829\n",
            "Epoch 4343/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4035 - accuracy: 0.7770 - val_loss: 2.1062 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04343: val_accuracy did not improve from 0.55829\n",
            "Epoch 4344/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4024 - accuracy: 0.7815 - val_loss: 2.1116 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04344: val_accuracy did not improve from 0.55829\n",
            "Epoch 4345/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.3990 - accuracy: 0.7786 - val_loss: 2.1122 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04345: val_accuracy did not improve from 0.55829\n",
            "Epoch 4346/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3995 - accuracy: 0.7799 - val_loss: 2.0908 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04346: val_accuracy did not improve from 0.55829\n",
            "Epoch 4347/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3990 - accuracy: 0.7784 - val_loss: 2.0684 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04347: val_accuracy did not improve from 0.55829\n",
            "Epoch 4348/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.3980 - accuracy: 0.7821 - val_loss: 2.0600 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04348: val_accuracy did not improve from 0.55829\n",
            "Epoch 4349/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3974 - accuracy: 0.7861 - val_loss: 2.0782 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04349: val_accuracy did not improve from 0.55829\n",
            "Epoch 4350/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.4007 - accuracy: 0.7822 - val_loss: 2.1386 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04350: val_accuracy did not improve from 0.55829\n",
            "Epoch 4351/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.3987 - accuracy: 0.7833 - val_loss: 2.1534 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04351: val_accuracy did not improve from 0.55829\n",
            "Epoch 4352/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.3990 - accuracy: 0.7810 - val_loss: 2.1189 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04352: val_accuracy did not improve from 0.55829\n",
            "Epoch 4353/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.3938 - accuracy: 0.7866 - val_loss: 2.1244 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04353: val_accuracy did not improve from 0.55829\n",
            "Epoch 4354/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4003 - accuracy: 0.7795 - val_loss: 2.1490 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04354: val_accuracy did not improve from 0.55829\n",
            "Epoch 4355/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.3938 - accuracy: 0.7866 - val_loss: 2.1929 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04355: val_accuracy did not improve from 0.55829\n",
            "Epoch 4356/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.3935 - accuracy: 0.7888 - val_loss: 2.2213 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04356: val_accuracy did not improve from 0.55829\n",
            "Epoch 4357/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.3975 - accuracy: 0.7824 - val_loss: 2.2076 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04357: val_accuracy did not improve from 0.55829\n",
            "Epoch 4358/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.3920 - accuracy: 0.7851 - val_loss: 2.2557 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04358: val_accuracy did not improve from 0.55829\n",
            "Epoch 4359/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3909 - accuracy: 0.7924 - val_loss: 2.3040 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04359: val_accuracy did not improve from 0.55829\n",
            "Epoch 4360/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3935 - accuracy: 0.7848 - val_loss: 2.2314 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04360: val_accuracy did not improve from 0.55829\n",
            "Epoch 4361/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.3892 - accuracy: 0.7877 - val_loss: 2.1980 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04361: val_accuracy did not improve from 0.55829\n",
            "Epoch 4362/5000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.3882 - accuracy: 0.7879 - val_loss: 2.2832 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04362: val_accuracy did not improve from 0.55829\n",
            "Epoch 4363/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.3917 - accuracy: 0.7842 - val_loss: 2.3659 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04363: val_accuracy did not improve from 0.55829\n",
            "Epoch 4364/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.3922 - accuracy: 0.7819 - val_loss: 2.3186 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04364: val_accuracy did not improve from 0.55829\n",
            "Epoch 4365/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.3908 - accuracy: 0.7922 - val_loss: 2.2447 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04365: val_accuracy did not improve from 0.55829\n",
            "Epoch 4366/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.3936 - accuracy: 0.7835 - val_loss: 2.2109 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04366: val_accuracy did not improve from 0.55829\n",
            "Epoch 4367/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.3903 - accuracy: 0.7870 - val_loss: 2.2290 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04367: val_accuracy did not improve from 0.55829\n",
            "Epoch 4368/5000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.3942 - accuracy: 0.7846 - val_loss: 2.2859 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04368: val_accuracy did not improve from 0.55829\n",
            "Epoch 4369/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.3919 - accuracy: 0.7882 - val_loss: 2.3439 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04369: val_accuracy did not improve from 0.55829\n",
            "Epoch 4370/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.3913 - accuracy: 0.7884 - val_loss: 2.3861 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04370: val_accuracy did not improve from 0.55829\n",
            "Epoch 4371/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.3900 - accuracy: 0.7924 - val_loss: 2.3261 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04371: val_accuracy did not improve from 0.55829\n",
            "Epoch 4372/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3882 - accuracy: 0.7900 - val_loss: 2.2794 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04372: val_accuracy did not improve from 0.55829\n",
            "Epoch 4373/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.3931 - accuracy: 0.7824 - val_loss: 2.3710 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04373: val_accuracy did not improve from 0.55829\n",
            "Epoch 4374/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.3956 - accuracy: 0.7864 - val_loss: 2.3806 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04374: val_accuracy did not improve from 0.55829\n",
            "Epoch 4375/5000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.3890 - accuracy: 0.7888 - val_loss: 2.4005 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04375: val_accuracy did not improve from 0.55829\n",
            "Epoch 4376/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.3910 - accuracy: 0.7879 - val_loss: 2.4081 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04376: val_accuracy did not improve from 0.55829\n",
            "Epoch 4377/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.3926 - accuracy: 0.7851 - val_loss: 2.4043 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04377: val_accuracy did not improve from 0.55829\n",
            "Epoch 4378/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.3958 - accuracy: 0.7851 - val_loss: 2.3622 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04378: val_accuracy did not improve from 0.55829\n",
            "Epoch 4379/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.3873 - accuracy: 0.7893 - val_loss: 2.3324 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04379: val_accuracy did not improve from 0.55829\n",
            "Epoch 4380/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.3913 - accuracy: 0.7888 - val_loss: 2.3985 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04380: val_accuracy did not improve from 0.55829\n",
            "Epoch 4381/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.3912 - accuracy: 0.7870 - val_loss: 2.5044 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04381: val_accuracy did not improve from 0.55829\n",
            "Epoch 4382/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3955 - accuracy: 0.7862 - val_loss: 2.4366 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04382: val_accuracy did not improve from 0.55829\n",
            "Epoch 4383/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3895 - accuracy: 0.7913 - val_loss: 2.4123 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04383: val_accuracy did not improve from 0.55829\n",
            "Epoch 4384/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.3897 - accuracy: 0.7915 - val_loss: 2.5131 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04384: val_accuracy did not improve from 0.55829\n",
            "Epoch 4385/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.3901 - accuracy: 0.7871 - val_loss: 2.4283 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04385: val_accuracy did not improve from 0.55829\n",
            "Epoch 4386/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.3912 - accuracy: 0.7884 - val_loss: 2.3546 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04386: val_accuracy did not improve from 0.55829\n",
            "Epoch 4387/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.3944 - accuracy: 0.7832 - val_loss: 2.3823 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04387: val_accuracy did not improve from 0.55829\n",
            "Epoch 4388/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.3860 - accuracy: 0.7975 - val_loss: 2.4983 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04388: val_accuracy did not improve from 0.55829\n",
            "Epoch 4389/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.3920 - accuracy: 0.7868 - val_loss: 2.5155 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04389: val_accuracy did not improve from 0.55829\n",
            "Epoch 4390/5000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.3983 - accuracy: 0.7824 - val_loss: 2.3471 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04390: val_accuracy did not improve from 0.55829\n",
            "Epoch 4391/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4004 - accuracy: 0.7837 - val_loss: 2.4553 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04391: val_accuracy did not improve from 0.55829\n",
            "Epoch 4392/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.3880 - accuracy: 0.7902 - val_loss: 2.4966 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04392: val_accuracy did not improve from 0.55829\n",
            "Epoch 4393/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3892 - accuracy: 0.7868 - val_loss: 2.5122 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04393: val_accuracy did not improve from 0.55829\n",
            "Epoch 4394/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.3872 - accuracy: 0.7864 - val_loss: 2.5660 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04394: val_accuracy did not improve from 0.55829\n",
            "Epoch 4395/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4083 - accuracy: 0.7755 - val_loss: 2.4930 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 04395: val_accuracy did not improve from 0.55829\n",
            "Epoch 4396/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.4046 - accuracy: 0.7866 - val_loss: 2.4369 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04396: val_accuracy did not improve from 0.55829\n",
            "Epoch 4397/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4005 - accuracy: 0.7781 - val_loss: 2.4374 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04397: val_accuracy did not improve from 0.55829\n",
            "Epoch 4398/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.3875 - accuracy: 0.7928 - val_loss: 2.6609 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04398: val_accuracy did not improve from 0.55829\n",
            "Epoch 4399/5000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.3969 - accuracy: 0.7848 - val_loss: 2.6674 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04399: val_accuracy did not improve from 0.55829\n",
            "Epoch 4400/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.3924 - accuracy: 0.7862 - val_loss: 2.4247 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04400: val_accuracy did not improve from 0.55829\n",
            "Epoch 4401/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.3864 - accuracy: 0.7880 - val_loss: 2.4089 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04401: val_accuracy did not improve from 0.55829\n",
            "Epoch 4402/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3876 - accuracy: 0.7877 - val_loss: 2.6164 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04402: val_accuracy did not improve from 0.55829\n",
            "Epoch 4403/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.3844 - accuracy: 0.7895 - val_loss: 2.7993 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04403: val_accuracy did not improve from 0.55829\n",
            "Epoch 4404/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.3848 - accuracy: 0.7928 - val_loss: 2.7046 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04404: val_accuracy did not improve from 0.55829\n",
            "Epoch 4405/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.3854 - accuracy: 0.7913 - val_loss: 2.4656 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04405: val_accuracy did not improve from 0.55829\n",
            "Epoch 4406/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.3891 - accuracy: 0.7877 - val_loss: 2.4692 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04406: val_accuracy did not improve from 0.55829\n",
            "Epoch 4407/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3807 - accuracy: 0.7982 - val_loss: 2.6097 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04407: val_accuracy did not improve from 0.55829\n",
            "Epoch 4408/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.3944 - accuracy: 0.7842 - val_loss: 2.5110 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04408: val_accuracy did not improve from 0.55829\n",
            "Epoch 4409/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.3916 - accuracy: 0.7889 - val_loss: 2.5161 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04409: val_accuracy did not improve from 0.55829\n",
            "Epoch 4410/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.3976 - accuracy: 0.7855 - val_loss: 2.5527 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04410: val_accuracy did not improve from 0.55829\n",
            "Epoch 4411/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4062 - accuracy: 0.7801 - val_loss: 2.4275 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04411: val_accuracy did not improve from 0.55829\n",
            "Epoch 4412/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4079 - accuracy: 0.7803 - val_loss: 2.2787 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04412: val_accuracy did not improve from 0.55829\n",
            "Epoch 4413/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4115 - accuracy: 0.7819 - val_loss: 2.5782 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04413: val_accuracy did not improve from 0.55829\n",
            "Epoch 4414/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4145 - accuracy: 0.7728 - val_loss: 2.3793 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04414: val_accuracy did not improve from 0.55829\n",
            "Epoch 4415/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.3992 - accuracy: 0.7833 - val_loss: 2.3934 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04415: val_accuracy did not improve from 0.55829\n",
            "Epoch 4416/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4225 - accuracy: 0.7772 - val_loss: 2.4452 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04416: val_accuracy did not improve from 0.55829\n",
            "Epoch 4417/5000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.4647 - accuracy: 0.7650 - val_loss: 2.0132 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04417: val_accuracy did not improve from 0.55829\n",
            "Epoch 4418/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4366 - accuracy: 0.7668 - val_loss: 2.1947 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04418: val_accuracy did not improve from 0.55829\n",
            "Epoch 4419/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4339 - accuracy: 0.7726 - val_loss: 2.3756 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 04419: val_accuracy did not improve from 0.55829\n",
            "Epoch 4420/5000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.4561 - accuracy: 0.7600 - val_loss: 2.0847 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04420: val_accuracy did not improve from 0.55829\n",
            "Epoch 4421/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4100 - accuracy: 0.7788 - val_loss: 1.9976 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04421: val_accuracy did not improve from 0.55829\n",
            "Epoch 4422/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4430 - accuracy: 0.7623 - val_loss: 2.0715 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04422: val_accuracy did not improve from 0.55829\n",
            "Epoch 4423/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.4052 - accuracy: 0.7790 - val_loss: 2.2866 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04423: val_accuracy did not improve from 0.55829\n",
            "Epoch 4424/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4385 - accuracy: 0.7607 - val_loss: 2.3074 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04424: val_accuracy did not improve from 0.55829\n",
            "Epoch 4425/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.4996 - accuracy: 0.7375 - val_loss: 1.6431 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04425: val_accuracy did not improve from 0.55829\n",
            "Epoch 4426/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4742 - accuracy: 0.7453 - val_loss: 1.6531 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04426: val_accuracy did not improve from 0.55829\n",
            "Epoch 4427/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4922 - accuracy: 0.7451 - val_loss: 1.9007 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 04427: val_accuracy did not improve from 0.55829\n",
            "Epoch 4428/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4591 - accuracy: 0.7504 - val_loss: 2.0125 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04428: val_accuracy did not improve from 0.55829\n",
            "Epoch 4429/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4684 - accuracy: 0.7533 - val_loss: 1.8984 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04429: val_accuracy did not improve from 0.55829\n",
            "Epoch 4430/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4577 - accuracy: 0.7446 - val_loss: 1.7729 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04430: val_accuracy did not improve from 0.55829\n",
            "Epoch 4431/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4548 - accuracy: 0.7469 - val_loss: 1.8840 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04431: val_accuracy did not improve from 0.55829\n",
            "Epoch 4432/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4598 - accuracy: 0.7455 - val_loss: 2.1827 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04432: val_accuracy did not improve from 0.55829\n",
            "Epoch 4433/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.5124 - accuracy: 0.7346 - val_loss: 1.6247 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04433: val_accuracy did not improve from 0.55829\n",
            "Epoch 4434/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.5096 - accuracy: 0.7293 - val_loss: 1.4635 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04434: val_accuracy did not improve from 0.55829\n",
            "Epoch 4435/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.5394 - accuracy: 0.7208 - val_loss: 1.4933 - val_accuracy: 0.4808\n",
            "\n",
            "Epoch 04435: val_accuracy did not improve from 0.55829\n",
            "Epoch 4436/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4859 - accuracy: 0.7409 - val_loss: 1.6400 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04436: val_accuracy did not improve from 0.55829\n",
            "Epoch 4437/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4918 - accuracy: 0.7382 - val_loss: 1.7581 - val_accuracy: 0.4801\n",
            "\n",
            "Epoch 04437: val_accuracy did not improve from 0.55829\n",
            "Epoch 4438/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.5024 - accuracy: 0.7464 - val_loss: 1.6988 - val_accuracy: 0.4859\n",
            "\n",
            "Epoch 04438: val_accuracy did not improve from 0.55829\n",
            "Epoch 4439/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4530 - accuracy: 0.7650 - val_loss: 1.7408 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04439: val_accuracy did not improve from 0.55829\n",
            "Epoch 4440/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4731 - accuracy: 0.7442 - val_loss: 1.8288 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04440: val_accuracy did not improve from 0.55829\n",
            "Epoch 4441/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4488 - accuracy: 0.7531 - val_loss: 1.9620 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04441: val_accuracy did not improve from 0.55829\n",
            "Epoch 4442/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4519 - accuracy: 0.7522 - val_loss: 1.8550 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04442: val_accuracy did not improve from 0.55829\n",
            "Epoch 4443/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.4361 - accuracy: 0.7594 - val_loss: 1.6618 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04443: val_accuracy did not improve from 0.55829\n",
            "Epoch 4444/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4389 - accuracy: 0.7585 - val_loss: 1.6779 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04444: val_accuracy did not improve from 0.55829\n",
            "Epoch 4445/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4437 - accuracy: 0.7569 - val_loss: 1.8864 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04445: val_accuracy did not improve from 0.55829\n",
            "Epoch 4446/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4371 - accuracy: 0.7600 - val_loss: 1.9423 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04446: val_accuracy did not improve from 0.55829\n",
            "Epoch 4447/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4190 - accuracy: 0.7719 - val_loss: 1.9436 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04447: val_accuracy did not improve from 0.55829\n",
            "Epoch 4448/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4172 - accuracy: 0.7741 - val_loss: 2.0568 - val_accuracy: 0.4888\n",
            "\n",
            "Epoch 04448: val_accuracy did not improve from 0.55829\n",
            "Epoch 4449/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.4198 - accuracy: 0.7743 - val_loss: 2.0650 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04449: val_accuracy did not improve from 0.55829\n",
            "Epoch 4450/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4115 - accuracy: 0.7795 - val_loss: 1.9322 - val_accuracy: 0.4881\n",
            "\n",
            "Epoch 04450: val_accuracy did not improve from 0.55829\n",
            "Epoch 4451/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.4266 - accuracy: 0.7670 - val_loss: 1.9444 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04451: val_accuracy did not improve from 0.55829\n",
            "Epoch 4452/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4035 - accuracy: 0.7763 - val_loss: 1.9142 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 04452: val_accuracy did not improve from 0.55829\n",
            "Epoch 4453/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4100 - accuracy: 0.7725 - val_loss: 1.8891 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04453: val_accuracy did not improve from 0.55829\n",
            "Epoch 4454/5000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.4105 - accuracy: 0.7719 - val_loss: 1.9192 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04454: val_accuracy did not improve from 0.55829\n",
            "Epoch 4455/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4037 - accuracy: 0.7775 - val_loss: 1.9948 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04455: val_accuracy did not improve from 0.55829\n",
            "Epoch 4456/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.4000 - accuracy: 0.7779 - val_loss: 2.0594 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04456: val_accuracy did not improve from 0.55829\n",
            "Epoch 4457/5000\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.4072 - accuracy: 0.7781 - val_loss: 2.0548 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04457: val_accuracy did not improve from 0.55829\n",
            "Epoch 4458/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.4004 - accuracy: 0.7841 - val_loss: 2.1004 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04458: val_accuracy did not improve from 0.55829\n",
            "Epoch 4459/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.3986 - accuracy: 0.7790 - val_loss: 2.1342 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04459: val_accuracy did not improve from 0.55829\n",
            "Epoch 4460/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.3991 - accuracy: 0.7821 - val_loss: 2.1465 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04460: val_accuracy did not improve from 0.55829\n",
            "Epoch 4461/5000\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.3939 - accuracy: 0.7810 - val_loss: 2.1311 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04461: val_accuracy did not improve from 0.55829\n",
            "Epoch 4462/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.3888 - accuracy: 0.7926 - val_loss: 2.1762 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04462: val_accuracy did not improve from 0.55829\n",
            "Epoch 4463/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.3913 - accuracy: 0.7889 - val_loss: 2.2820 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04463: val_accuracy did not improve from 0.55829\n",
            "Epoch 4464/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.3873 - accuracy: 0.7902 - val_loss: 2.3358 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04464: val_accuracy did not improve from 0.55829\n",
            "Epoch 4465/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.3862 - accuracy: 0.7906 - val_loss: 2.2658 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04465: val_accuracy did not improve from 0.55829\n",
            "Epoch 4466/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.3861 - accuracy: 0.7924 - val_loss: 2.1904 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04466: val_accuracy did not improve from 0.55829\n",
            "Epoch 4467/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.3916 - accuracy: 0.7915 - val_loss: 2.1874 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04467: val_accuracy did not improve from 0.55829\n",
            "Epoch 4468/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.3868 - accuracy: 0.7953 - val_loss: 2.2872 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04468: val_accuracy did not improve from 0.55829\n",
            "Epoch 4469/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.3856 - accuracy: 0.7917 - val_loss: 2.3829 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04469: val_accuracy did not improve from 0.55829\n",
            "Epoch 4470/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3854 - accuracy: 0.7909 - val_loss: 2.3597 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04470: val_accuracy did not improve from 0.55829\n",
            "Epoch 4471/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.3838 - accuracy: 0.7873 - val_loss: 2.3231 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04471: val_accuracy did not improve from 0.55829\n",
            "Epoch 4472/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.3814 - accuracy: 0.7975 - val_loss: 2.2890 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04472: val_accuracy did not improve from 0.55829\n",
            "Epoch 4473/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.3817 - accuracy: 0.7933 - val_loss: 2.3260 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04473: val_accuracy did not improve from 0.55829\n",
            "Epoch 4474/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.3795 - accuracy: 0.7938 - val_loss: 2.4205 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04474: val_accuracy did not improve from 0.55829\n",
            "Epoch 4475/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.3792 - accuracy: 0.7978 - val_loss: 2.4525 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 04475: val_accuracy did not improve from 0.55829\n",
            "Epoch 4476/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.3853 - accuracy: 0.7899 - val_loss: 2.4807 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04476: val_accuracy did not improve from 0.55829\n",
            "Epoch 4477/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.3866 - accuracy: 0.7929 - val_loss: 2.4778 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04477: val_accuracy did not improve from 0.55829\n",
            "Epoch 4478/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.3793 - accuracy: 0.7966 - val_loss: 2.5080 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04478: val_accuracy did not improve from 0.55829\n",
            "Epoch 4479/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.3828 - accuracy: 0.7889 - val_loss: 2.4858 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04479: val_accuracy did not improve from 0.55829\n",
            "Epoch 4480/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.3815 - accuracy: 0.7975 - val_loss: 2.4240 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04480: val_accuracy did not improve from 0.55829\n",
            "Epoch 4481/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.3796 - accuracy: 0.7980 - val_loss: 2.4936 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04481: val_accuracy did not improve from 0.55829\n",
            "Epoch 4482/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.3792 - accuracy: 0.7944 - val_loss: 2.5677 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04482: val_accuracy did not improve from 0.55829\n",
            "Epoch 4483/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.3798 - accuracy: 0.7893 - val_loss: 2.5632 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04483: val_accuracy did not improve from 0.55829\n",
            "Epoch 4484/5000\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.3750 - accuracy: 0.7946 - val_loss: 2.5787 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04484: val_accuracy did not improve from 0.55829\n",
            "Epoch 4485/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.3823 - accuracy: 0.7888 - val_loss: 2.4706 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04485: val_accuracy did not improve from 0.55829\n",
            "Epoch 4486/5000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.3760 - accuracy: 0.7982 - val_loss: 2.4514 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04486: val_accuracy did not improve from 0.55829\n",
            "Epoch 4487/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.3805 - accuracy: 0.7938 - val_loss: 2.5918 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04487: val_accuracy did not improve from 0.55829\n",
            "Epoch 4488/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3783 - accuracy: 0.7946 - val_loss: 2.6064 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04488: val_accuracy did not improve from 0.55829\n",
            "Epoch 4489/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.3828 - accuracy: 0.7951 - val_loss: 2.5137 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04489: val_accuracy did not improve from 0.55829\n",
            "Epoch 4490/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.3853 - accuracy: 0.7918 - val_loss: 2.4548 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04490: val_accuracy did not improve from 0.55829\n",
            "Epoch 4491/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.3850 - accuracy: 0.7987 - val_loss: 2.4126 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04491: val_accuracy did not improve from 0.55829\n",
            "Epoch 4492/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.3761 - accuracy: 0.8013 - val_loss: 2.4382 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04492: val_accuracy did not improve from 0.55829\n",
            "Epoch 4493/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.3777 - accuracy: 0.7975 - val_loss: 2.4983 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04493: val_accuracy did not improve from 0.55829\n",
            "Epoch 4494/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.3800 - accuracy: 0.7913 - val_loss: 2.5354 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04494: val_accuracy did not improve from 0.55829\n",
            "Epoch 4495/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3742 - accuracy: 0.7980 - val_loss: 2.5506 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04495: val_accuracy did not improve from 0.55829\n",
            "Epoch 4496/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.3731 - accuracy: 0.7991 - val_loss: 2.5922 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04496: val_accuracy did not improve from 0.55829\n",
            "Epoch 4497/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.3772 - accuracy: 0.7969 - val_loss: 2.5998 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04497: val_accuracy did not improve from 0.55829\n",
            "Epoch 4498/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.3782 - accuracy: 0.8007 - val_loss: 2.6423 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04498: val_accuracy did not improve from 0.55829\n",
            "Epoch 4499/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.3778 - accuracy: 0.7935 - val_loss: 2.5810 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04499: val_accuracy did not improve from 0.55829\n",
            "Epoch 4500/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3743 - accuracy: 0.7991 - val_loss: 2.5754 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04500: val_accuracy did not improve from 0.55829\n",
            "Epoch 4501/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.3725 - accuracy: 0.7973 - val_loss: 2.6765 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04501: val_accuracy did not improve from 0.55829\n",
            "Epoch 4502/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.3771 - accuracy: 0.7980 - val_loss: 2.6405 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04502: val_accuracy did not improve from 0.55829\n",
            "Epoch 4503/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.3837 - accuracy: 0.7899 - val_loss: 2.5795 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04503: val_accuracy did not improve from 0.55829\n",
            "Epoch 4504/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.3930 - accuracy: 0.7822 - val_loss: 2.4428 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04504: val_accuracy did not improve from 0.55829\n",
            "Epoch 4505/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.3782 - accuracy: 0.7986 - val_loss: 2.5776 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04505: val_accuracy did not improve from 0.55829\n",
            "Epoch 4506/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3786 - accuracy: 0.7937 - val_loss: 2.7767 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04506: val_accuracy did not improve from 0.55829\n",
            "Epoch 4507/5000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.3925 - accuracy: 0.7917 - val_loss: 2.6723 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 04507: val_accuracy did not improve from 0.55829\n",
            "Epoch 4508/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.3984 - accuracy: 0.7784 - val_loss: 2.6858 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04508: val_accuracy did not improve from 0.55829\n",
            "Epoch 4509/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4110 - accuracy: 0.7676 - val_loss: 2.4659 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04509: val_accuracy did not improve from 0.55829\n",
            "Epoch 4510/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.3899 - accuracy: 0.7897 - val_loss: 2.3794 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04510: val_accuracy did not improve from 0.55829\n",
            "Epoch 4511/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4277 - accuracy: 0.7687 - val_loss: 2.5938 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04511: val_accuracy did not improve from 0.55829\n",
            "Epoch 4512/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.5379 - accuracy: 0.7281 - val_loss: 2.2240 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 04512: val_accuracy did not improve from 0.55829\n",
            "Epoch 4513/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6315 - accuracy: 0.7145 - val_loss: 1.7397 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04513: val_accuracy did not improve from 0.55829\n",
            "Epoch 4514/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.5346 - accuracy: 0.7264 - val_loss: 1.8407 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04514: val_accuracy did not improve from 0.55829\n",
            "Epoch 4515/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.5531 - accuracy: 0.7147 - val_loss: 2.0168 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04515: val_accuracy did not improve from 0.55829\n",
            "Epoch 4516/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.5652 - accuracy: 0.7071 - val_loss: 1.6641 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04516: val_accuracy did not improve from 0.55829\n",
            "Epoch 4517/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.5213 - accuracy: 0.7263 - val_loss: 1.5202 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04517: val_accuracy did not improve from 0.55829\n",
            "Epoch 4518/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.5268 - accuracy: 0.7225 - val_loss: 1.5291 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04518: val_accuracy did not improve from 0.55829\n",
            "Epoch 4519/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.5072 - accuracy: 0.7272 - val_loss: 1.6031 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04519: val_accuracy did not improve from 0.55829\n",
            "Epoch 4520/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.5292 - accuracy: 0.7167 - val_loss: 1.5770 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04520: val_accuracy did not improve from 0.55829\n",
            "Epoch 4521/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.5031 - accuracy: 0.7328 - val_loss: 1.6950 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 04521: val_accuracy did not improve from 0.55829\n",
            "Epoch 4522/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4936 - accuracy: 0.7415 - val_loss: 1.8412 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04522: val_accuracy did not improve from 0.55829\n",
            "Epoch 4523/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.4632 - accuracy: 0.7502 - val_loss: 1.9000 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04523: val_accuracy did not improve from 0.55829\n",
            "Epoch 4524/5000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.4653 - accuracy: 0.7529 - val_loss: 1.8860 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04524: val_accuracy did not improve from 0.55829\n",
            "Epoch 4525/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4556 - accuracy: 0.7495 - val_loss: 1.8524 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04525: val_accuracy did not improve from 0.55829\n",
            "Epoch 4526/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.4491 - accuracy: 0.7513 - val_loss: 1.9478 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 04526: val_accuracy did not improve from 0.55829\n",
            "Epoch 4527/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.4631 - accuracy: 0.7500 - val_loss: 1.8562 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04527: val_accuracy did not improve from 0.55829\n",
            "Epoch 4528/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4375 - accuracy: 0.7616 - val_loss: 1.8341 - val_accuracy: 0.4902\n",
            "\n",
            "Epoch 04528: val_accuracy did not improve from 0.55829\n",
            "Epoch 4529/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.4432 - accuracy: 0.7632 - val_loss: 1.9411 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04529: val_accuracy did not improve from 0.55829\n",
            "Epoch 4530/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.4464 - accuracy: 0.7556 - val_loss: 2.0066 - val_accuracy: 0.4801\n",
            "\n",
            "Epoch 04530: val_accuracy did not improve from 0.55829\n",
            "Epoch 4531/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4236 - accuracy: 0.7703 - val_loss: 1.8677 - val_accuracy: 0.4823\n",
            "\n",
            "Epoch 04531: val_accuracy did not improve from 0.55829\n",
            "Epoch 4532/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4283 - accuracy: 0.7699 - val_loss: 1.7470 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 04532: val_accuracy did not improve from 0.55829\n",
            "Epoch 4533/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4217 - accuracy: 0.7737 - val_loss: 1.8320 - val_accuracy: 0.4895\n",
            "\n",
            "Epoch 04533: val_accuracy did not improve from 0.55829\n",
            "Epoch 4534/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.4060 - accuracy: 0.7822 - val_loss: 1.9924 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04534: val_accuracy did not improve from 0.55829\n",
            "Epoch 4535/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.4109 - accuracy: 0.7813 - val_loss: 2.0163 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04535: val_accuracy did not improve from 0.55829\n",
            "Epoch 4536/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4186 - accuracy: 0.7705 - val_loss: 1.9974 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04536: val_accuracy did not improve from 0.55829\n",
            "Epoch 4537/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.4046 - accuracy: 0.7790 - val_loss: 1.8876 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04537: val_accuracy did not improve from 0.55829\n",
            "Epoch 4538/5000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.3993 - accuracy: 0.7835 - val_loss: 1.8770 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04538: val_accuracy did not improve from 0.55829\n",
            "Epoch 4539/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.3961 - accuracy: 0.7893 - val_loss: 2.0370 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04539: val_accuracy did not improve from 0.55829\n",
            "Epoch 4540/5000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.3918 - accuracy: 0.7851 - val_loss: 2.1551 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04540: val_accuracy did not improve from 0.55829\n",
            "Epoch 4541/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.3997 - accuracy: 0.7828 - val_loss: 2.0694 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04541: val_accuracy did not improve from 0.55829\n",
            "Epoch 4542/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.3914 - accuracy: 0.7891 - val_loss: 2.0008 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04542: val_accuracy did not improve from 0.55829\n",
            "Epoch 4543/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.3889 - accuracy: 0.7993 - val_loss: 2.0620 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04543: val_accuracy did not improve from 0.55829\n",
            "Epoch 4544/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3889 - accuracy: 0.7940 - val_loss: 2.1480 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04544: val_accuracy did not improve from 0.55829\n",
            "Epoch 4545/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.3895 - accuracy: 0.7884 - val_loss: 2.1786 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04545: val_accuracy did not improve from 0.55829\n",
            "Epoch 4546/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3833 - accuracy: 0.7924 - val_loss: 2.1510 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04546: val_accuracy did not improve from 0.55829\n",
            "Epoch 4547/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.3844 - accuracy: 0.7928 - val_loss: 2.1345 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04547: val_accuracy did not improve from 0.55829\n",
            "Epoch 4548/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.3833 - accuracy: 0.7946 - val_loss: 2.1357 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04548: val_accuracy did not improve from 0.55829\n",
            "Epoch 4549/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.3809 - accuracy: 0.7967 - val_loss: 2.1112 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04549: val_accuracy did not improve from 0.55829\n",
            "Epoch 4550/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.3769 - accuracy: 0.7976 - val_loss: 2.1143 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04550: val_accuracy did not improve from 0.55829\n",
            "Epoch 4551/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.3813 - accuracy: 0.7964 - val_loss: 2.2040 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 04551: val_accuracy did not improve from 0.55829\n",
            "Epoch 4552/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.3799 - accuracy: 0.7915 - val_loss: 2.3002 - val_accuracy: 0.5011\n",
            "\n",
            "Epoch 04552: val_accuracy did not improve from 0.55829\n",
            "Epoch 4553/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3826 - accuracy: 0.7911 - val_loss: 2.2114 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04553: val_accuracy did not improve from 0.55829\n",
            "Epoch 4554/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.3749 - accuracy: 0.7976 - val_loss: 2.1607 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04554: val_accuracy did not improve from 0.55829\n",
            "Epoch 4555/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.3765 - accuracy: 0.7986 - val_loss: 2.2322 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04555: val_accuracy did not improve from 0.55829\n",
            "Epoch 4556/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.3721 - accuracy: 0.7991 - val_loss: 2.3597 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04556: val_accuracy did not improve from 0.55829\n",
            "Epoch 4557/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.3757 - accuracy: 0.7978 - val_loss: 2.3710 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04557: val_accuracy did not improve from 0.55829\n",
            "Epoch 4558/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.3717 - accuracy: 0.8027 - val_loss: 2.2934 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04558: val_accuracy did not improve from 0.55829\n",
            "Epoch 4559/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.3768 - accuracy: 0.8004 - val_loss: 2.2964 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04559: val_accuracy did not improve from 0.55829\n",
            "Epoch 4560/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.3756 - accuracy: 0.7971 - val_loss: 2.3225 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04560: val_accuracy did not improve from 0.55829\n",
            "Epoch 4561/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.3766 - accuracy: 0.7928 - val_loss: 2.3519 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04561: val_accuracy did not improve from 0.55829\n",
            "Epoch 4562/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3697 - accuracy: 0.8022 - val_loss: 2.4130 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04562: val_accuracy did not improve from 0.55829\n",
            "Epoch 4563/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.3723 - accuracy: 0.7973 - val_loss: 2.4878 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04563: val_accuracy did not improve from 0.55829\n",
            "Epoch 4564/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.3706 - accuracy: 0.7971 - val_loss: 2.5284 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04564: val_accuracy did not improve from 0.55829\n",
            "Epoch 4565/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.3722 - accuracy: 0.7993 - val_loss: 2.4772 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04565: val_accuracy did not improve from 0.55829\n",
            "Epoch 4566/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.3711 - accuracy: 0.7951 - val_loss: 2.4150 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04566: val_accuracy did not improve from 0.55829\n",
            "Epoch 4567/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.3753 - accuracy: 0.7967 - val_loss: 2.3989 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04567: val_accuracy did not improve from 0.55829\n",
            "Epoch 4568/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3689 - accuracy: 0.8043 - val_loss: 2.4795 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04568: val_accuracy did not improve from 0.55829\n",
            "Epoch 4569/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3729 - accuracy: 0.8014 - val_loss: 2.5295 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04569: val_accuracy did not improve from 0.55829\n",
            "Epoch 4570/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.3722 - accuracy: 0.7958 - val_loss: 2.5297 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04570: val_accuracy did not improve from 0.55829\n",
            "Epoch 4571/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.3732 - accuracy: 0.7960 - val_loss: 2.5180 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04571: val_accuracy did not improve from 0.55829\n",
            "Epoch 4572/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.3638 - accuracy: 0.8042 - val_loss: 2.4589 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04572: val_accuracy did not improve from 0.55829\n",
            "Epoch 4573/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.3706 - accuracy: 0.8011 - val_loss: 2.4547 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04573: val_accuracy did not improve from 0.55829\n",
            "Epoch 4574/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.3681 - accuracy: 0.8027 - val_loss: 2.5449 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04574: val_accuracy did not improve from 0.55829\n",
            "Epoch 4575/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.3669 - accuracy: 0.8014 - val_loss: 2.5891 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04575: val_accuracy did not improve from 0.55829\n",
            "Epoch 4576/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3681 - accuracy: 0.7971 - val_loss: 2.5599 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04576: val_accuracy did not improve from 0.55829\n",
            "Epoch 4577/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.3652 - accuracy: 0.8029 - val_loss: 2.5392 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04577: val_accuracy did not improve from 0.55829\n",
            "Epoch 4578/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.3687 - accuracy: 0.8005 - val_loss: 2.5436 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04578: val_accuracy did not improve from 0.55829\n",
            "Epoch 4579/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.3616 - accuracy: 0.8062 - val_loss: 2.5917 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04579: val_accuracy did not improve from 0.55829\n",
            "Epoch 4580/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3665 - accuracy: 0.7989 - val_loss: 2.6260 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04580: val_accuracy did not improve from 0.55829\n",
            "Epoch 4581/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.3636 - accuracy: 0.8038 - val_loss: 2.6194 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04581: val_accuracy did not improve from 0.55829\n",
            "Epoch 4582/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.3644 - accuracy: 0.8016 - val_loss: 2.6138 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04582: val_accuracy did not improve from 0.55829\n",
            "Epoch 4583/5000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.3654 - accuracy: 0.7998 - val_loss: 2.5946 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04583: val_accuracy did not improve from 0.55829\n",
            "Epoch 4584/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.3626 - accuracy: 0.8031 - val_loss: 2.6264 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04584: val_accuracy did not improve from 0.55829\n",
            "Epoch 4585/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.3615 - accuracy: 0.8076 - val_loss: 2.6313 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04585: val_accuracy did not improve from 0.55829\n",
            "Epoch 4586/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.3634 - accuracy: 0.8054 - val_loss: 2.6489 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04586: val_accuracy did not improve from 0.55829\n",
            "Epoch 4587/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.3639 - accuracy: 0.8007 - val_loss: 2.6792 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04587: val_accuracy did not improve from 0.55829\n",
            "Epoch 4588/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.3667 - accuracy: 0.8036 - val_loss: 2.6530 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04588: val_accuracy did not improve from 0.55829\n",
            "Epoch 4589/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.3656 - accuracy: 0.8051 - val_loss: 2.6129 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04589: val_accuracy did not improve from 0.55829\n",
            "Epoch 4590/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.3640 - accuracy: 0.8062 - val_loss: 2.6452 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04590: val_accuracy did not improve from 0.55829\n",
            "Epoch 4591/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.3617 - accuracy: 0.8040 - val_loss: 2.6823 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04591: val_accuracy did not improve from 0.55829\n",
            "Epoch 4592/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.3629 - accuracy: 0.8022 - val_loss: 2.7219 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04592: val_accuracy did not improve from 0.55829\n",
            "Epoch 4593/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.3592 - accuracy: 0.8076 - val_loss: 2.7286 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04593: val_accuracy did not improve from 0.55829\n",
            "Epoch 4594/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.3610 - accuracy: 0.8065 - val_loss: 2.6794 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04594: val_accuracy did not improve from 0.55829\n",
            "Epoch 4595/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3605 - accuracy: 0.8042 - val_loss: 2.7530 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04595: val_accuracy did not improve from 0.55829\n",
            "Epoch 4596/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.3637 - accuracy: 0.8063 - val_loss: 2.7545 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04596: val_accuracy did not improve from 0.55829\n",
            "Epoch 4597/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.3674 - accuracy: 0.8056 - val_loss: 2.7625 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04597: val_accuracy did not improve from 0.55829\n",
            "Epoch 4598/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.3671 - accuracy: 0.8025 - val_loss: 2.7560 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04598: val_accuracy did not improve from 0.55829\n",
            "Epoch 4599/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.3669 - accuracy: 0.8043 - val_loss: 2.7597 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04599: val_accuracy did not improve from 0.55829\n",
            "Epoch 4600/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.3617 - accuracy: 0.8094 - val_loss: 2.7646 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04600: val_accuracy did not improve from 0.55829\n",
            "Epoch 4601/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.3585 - accuracy: 0.8094 - val_loss: 2.8724 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04601: val_accuracy did not improve from 0.55829\n",
            "Epoch 4602/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.3664 - accuracy: 0.8056 - val_loss: 2.9517 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04602: val_accuracy did not improve from 0.55829\n",
            "Epoch 4603/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.3626 - accuracy: 0.8016 - val_loss: 2.8211 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04603: val_accuracy did not improve from 0.55829\n",
            "Epoch 4604/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.3543 - accuracy: 0.8138 - val_loss: 2.7533 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04604: val_accuracy did not improve from 0.55829\n",
            "Epoch 4605/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.3599 - accuracy: 0.8049 - val_loss: 2.8298 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04605: val_accuracy did not improve from 0.55829\n",
            "Epoch 4606/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.3591 - accuracy: 0.7993 - val_loss: 2.8599 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04606: val_accuracy did not improve from 0.55829\n",
            "Epoch 4607/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.3609 - accuracy: 0.8025 - val_loss: 2.8019 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04607: val_accuracy did not improve from 0.55829\n",
            "Epoch 4608/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.3641 - accuracy: 0.8038 - val_loss: 2.7754 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04608: val_accuracy did not improve from 0.55829\n",
            "Epoch 4609/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.3764 - accuracy: 0.8002 - val_loss: 2.6591 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04609: val_accuracy did not improve from 0.55829\n",
            "Epoch 4610/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4296 - accuracy: 0.7717 - val_loss: 2.7305 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04610: val_accuracy did not improve from 0.55829\n",
            "Epoch 4611/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.8881 - accuracy: 0.6594 - val_loss: 1.3786 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04611: val_accuracy did not improve from 0.55829\n",
            "Epoch 4612/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.7388 - accuracy: 0.6361 - val_loss: 1.1292 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04612: val_accuracy did not improve from 0.55829\n",
            "Epoch 4613/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.8000 - accuracy: 0.5661 - val_loss: 0.9750 - val_accuracy: 0.4794\n",
            "\n",
            "Epoch 04613: val_accuracy did not improve from 0.55829\n",
            "Epoch 4614/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.7612 - accuracy: 0.5464 - val_loss: 0.8112 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04614: val_accuracy did not improve from 0.55829\n",
            "Epoch 4615/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.7281 - accuracy: 0.5370 - val_loss: 0.7544 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04615: val_accuracy did not improve from 0.55829\n",
            "Epoch 4616/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.7179 - accuracy: 0.5239 - val_loss: 0.7322 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04616: val_accuracy did not improve from 0.55829\n",
            "Epoch 4617/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.7168 - accuracy: 0.5176 - val_loss: 0.7255 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04617: val_accuracy did not improve from 0.55829\n",
            "Epoch 4618/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.7174 - accuracy: 0.5143 - val_loss: 0.7222 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04618: val_accuracy did not improve from 0.55829\n",
            "Epoch 4619/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.7118 - accuracy: 0.5170 - val_loss: 0.7199 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04619: val_accuracy did not improve from 0.55829\n",
            "Epoch 4620/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.7092 - accuracy: 0.5127 - val_loss: 0.7178 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04620: val_accuracy did not improve from 0.55829\n",
            "Epoch 4621/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.7081 - accuracy: 0.5120 - val_loss: 0.7151 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04621: val_accuracy did not improve from 0.55829\n",
            "Epoch 4622/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.7091 - accuracy: 0.5083 - val_loss: 0.7142 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04622: val_accuracy did not improve from 0.55829\n",
            "Epoch 4623/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.7061 - accuracy: 0.5087 - val_loss: 0.7135 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 04623: val_accuracy did not improve from 0.55829\n",
            "Epoch 4624/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.7038 - accuracy: 0.5116 - val_loss: 0.7161 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 04624: val_accuracy did not improve from 0.55829\n",
            "Epoch 4625/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.7015 - accuracy: 0.5054 - val_loss: 0.7194 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 04625: val_accuracy did not improve from 0.55829\n",
            "Epoch 4626/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.7016 - accuracy: 0.5094 - val_loss: 0.7221 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04626: val_accuracy did not improve from 0.55829\n",
            "Epoch 4627/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6995 - accuracy: 0.5127 - val_loss: 0.7244 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 04627: val_accuracy did not improve from 0.55829\n",
            "Epoch 4628/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6993 - accuracy: 0.5139 - val_loss: 0.7251 - val_accuracy: 0.5206\n",
            "\n",
            "Epoch 04628: val_accuracy did not improve from 0.55829\n",
            "Epoch 4629/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.6967 - accuracy: 0.5185 - val_loss: 0.7247 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 04629: val_accuracy did not improve from 0.55829\n",
            "Epoch 4630/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6971 - accuracy: 0.5147 - val_loss: 0.7231 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04630: val_accuracy did not improve from 0.55829\n",
            "Epoch 4631/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6950 - accuracy: 0.5210 - val_loss: 0.7212 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 04631: val_accuracy did not improve from 0.55829\n",
            "Epoch 4632/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6953 - accuracy: 0.5161 - val_loss: 0.7188 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04632: val_accuracy did not improve from 0.55829\n",
            "Epoch 4633/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6940 - accuracy: 0.5226 - val_loss: 0.7166 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04633: val_accuracy did not improve from 0.55829\n",
            "Epoch 4634/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6937 - accuracy: 0.5223 - val_loss: 0.7146 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04634: val_accuracy did not improve from 0.55829\n",
            "Epoch 4635/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6934 - accuracy: 0.5214 - val_loss: 0.7128 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04635: val_accuracy did not improve from 0.55829\n",
            "Epoch 4636/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6932 - accuracy: 0.5208 - val_loss: 0.7113 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 04636: val_accuracy did not improve from 0.55829\n",
            "Epoch 4637/5000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.6934 - accuracy: 0.5179 - val_loss: 0.7100 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04637: val_accuracy did not improve from 0.55829\n",
            "Epoch 4638/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6916 - accuracy: 0.5272 - val_loss: 0.7090 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04638: val_accuracy did not improve from 0.55829\n",
            "Epoch 4639/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6915 - accuracy: 0.5208 - val_loss: 0.7080 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04639: val_accuracy did not improve from 0.55829\n",
            "Epoch 4640/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6919 - accuracy: 0.5138 - val_loss: 0.7071 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04640: val_accuracy did not improve from 0.55829\n",
            "Epoch 4641/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6926 - accuracy: 0.5132 - val_loss: 0.7062 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04641: val_accuracy did not improve from 0.55829\n",
            "Epoch 4642/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6915 - accuracy: 0.5257 - val_loss: 0.7051 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04642: val_accuracy did not improve from 0.55829\n",
            "Epoch 4643/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6909 - accuracy: 0.5254 - val_loss: 0.7041 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04643: val_accuracy did not improve from 0.55829\n",
            "Epoch 4644/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.6912 - accuracy: 0.5210 - val_loss: 0.7034 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04644: val_accuracy did not improve from 0.55829\n",
            "Epoch 4645/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.6914 - accuracy: 0.5196 - val_loss: 0.7029 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04645: val_accuracy did not improve from 0.55829\n",
            "Epoch 4646/5000\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.6913 - accuracy: 0.5214 - val_loss: 0.7026 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04646: val_accuracy did not improve from 0.55829\n",
            "Epoch 4647/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6915 - accuracy: 0.5207 - val_loss: 0.7023 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04647: val_accuracy did not improve from 0.55829\n",
            "Epoch 4648/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6911 - accuracy: 0.5183 - val_loss: 0.7020 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04648: val_accuracy did not improve from 0.55829\n",
            "Epoch 4649/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6923 - accuracy: 0.5125 - val_loss: 0.7018 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04649: val_accuracy did not improve from 0.55829\n",
            "Epoch 4650/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6909 - accuracy: 0.5196 - val_loss: 0.7017 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04650: val_accuracy did not improve from 0.55829\n",
            "Epoch 4651/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6908 - accuracy: 0.5201 - val_loss: 0.7016 - val_accuracy: 0.4960\n",
            "\n",
            "Epoch 04651: val_accuracy did not improve from 0.55829\n",
            "Epoch 4652/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6897 - accuracy: 0.5261 - val_loss: 0.7015 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04652: val_accuracy did not improve from 0.55829\n",
            "Epoch 4653/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6916 - accuracy: 0.5174 - val_loss: 0.7015 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04653: val_accuracy did not improve from 0.55829\n",
            "Epoch 4654/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6899 - accuracy: 0.5234 - val_loss: 0.7015 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04654: val_accuracy did not improve from 0.55829\n",
            "Epoch 4655/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.6907 - accuracy: 0.5264 - val_loss: 0.7015 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04655: val_accuracy did not improve from 0.55829\n",
            "Epoch 4656/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6894 - accuracy: 0.5236 - val_loss: 0.7015 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04656: val_accuracy did not improve from 0.55829\n",
            "Epoch 4657/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.6912 - accuracy: 0.5205 - val_loss: 0.7016 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04657: val_accuracy did not improve from 0.55829\n",
            "Epoch 4658/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.6899 - accuracy: 0.5259 - val_loss: 0.7016 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04658: val_accuracy did not improve from 0.55829\n",
            "Epoch 4659/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6900 - accuracy: 0.5239 - val_loss: 0.7017 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04659: val_accuracy did not improve from 0.55829\n",
            "Epoch 4660/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6917 - accuracy: 0.5168 - val_loss: 0.7017 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04660: val_accuracy did not improve from 0.55829\n",
            "Epoch 4661/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6899 - accuracy: 0.5313 - val_loss: 0.7017 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04661: val_accuracy did not improve from 0.55829\n",
            "Epoch 4662/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6894 - accuracy: 0.5236 - val_loss: 0.7018 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04662: val_accuracy did not improve from 0.55829\n",
            "Epoch 4663/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.6899 - accuracy: 0.5205 - val_loss: 0.7018 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04663: val_accuracy did not improve from 0.55829\n",
            "Epoch 4664/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6909 - accuracy: 0.5214 - val_loss: 0.7018 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04664: val_accuracy did not improve from 0.55829\n",
            "Epoch 4665/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6897 - accuracy: 0.5243 - val_loss: 0.7019 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04665: val_accuracy did not improve from 0.55829\n",
            "Epoch 4666/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6899 - accuracy: 0.5217 - val_loss: 0.7019 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04666: val_accuracy did not improve from 0.55829\n",
            "Epoch 4667/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.6903 - accuracy: 0.5254 - val_loss: 0.7019 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04667: val_accuracy did not improve from 0.55829\n",
            "Epoch 4668/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6904 - accuracy: 0.5156 - val_loss: 0.7019 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04668: val_accuracy did not improve from 0.55829\n",
            "Epoch 4669/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6886 - accuracy: 0.5333 - val_loss: 0.7019 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04669: val_accuracy did not improve from 0.55829\n",
            "Epoch 4670/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6899 - accuracy: 0.5243 - val_loss: 0.7019 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04670: val_accuracy did not improve from 0.55829\n",
            "Epoch 4671/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6899 - accuracy: 0.5290 - val_loss: 0.7019 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04671: val_accuracy did not improve from 0.55829\n",
            "Epoch 4672/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.6901 - accuracy: 0.5136 - val_loss: 0.7019 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04672: val_accuracy did not improve from 0.55829\n",
            "Epoch 4673/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6905 - accuracy: 0.5183 - val_loss: 0.7019 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04673: val_accuracy did not improve from 0.55829\n",
            "Epoch 4674/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6878 - accuracy: 0.5263 - val_loss: 0.7019 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 04674: val_accuracy did not improve from 0.55829\n",
            "Epoch 4675/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6879 - accuracy: 0.5361 - val_loss: 0.7019 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 04675: val_accuracy did not improve from 0.55829\n",
            "Epoch 4676/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.6883 - accuracy: 0.5339 - val_loss: 0.7019 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04676: val_accuracy did not improve from 0.55829\n",
            "Epoch 4677/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6887 - accuracy: 0.5272 - val_loss: 0.7019 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04677: val_accuracy did not improve from 0.55829\n",
            "Epoch 4678/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6889 - accuracy: 0.5259 - val_loss: 0.7019 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 04678: val_accuracy did not improve from 0.55829\n",
            "Epoch 4679/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6890 - accuracy: 0.5279 - val_loss: 0.7019 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 04679: val_accuracy did not improve from 0.55829\n",
            "Epoch 4680/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6908 - accuracy: 0.5194 - val_loss: 0.7019 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04680: val_accuracy did not improve from 0.55829\n",
            "Epoch 4681/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.6898 - accuracy: 0.5250 - val_loss: 0.7019 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04681: val_accuracy did not improve from 0.55829\n",
            "Epoch 4682/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.6887 - accuracy: 0.5301 - val_loss: 0.7020 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 04682: val_accuracy did not improve from 0.55829\n",
            "Epoch 4683/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6889 - accuracy: 0.5281 - val_loss: 0.7021 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 04683: val_accuracy did not improve from 0.55829\n",
            "Epoch 4684/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6879 - accuracy: 0.5243 - val_loss: 0.7022 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 04684: val_accuracy did not improve from 0.55829\n",
            "Epoch 4685/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6894 - accuracy: 0.5261 - val_loss: 0.7023 - val_accuracy: 0.4909\n",
            "\n",
            "Epoch 04685: val_accuracy did not improve from 0.55829\n",
            "Epoch 4686/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6884 - accuracy: 0.5266 - val_loss: 0.7024 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04686: val_accuracy did not improve from 0.55829\n",
            "Epoch 4687/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.6900 - accuracy: 0.5197 - val_loss: 0.7025 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04687: val_accuracy did not improve from 0.55829\n",
            "Epoch 4688/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6894 - accuracy: 0.5279 - val_loss: 0.7026 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 04688: val_accuracy did not improve from 0.55829\n",
            "Epoch 4689/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6891 - accuracy: 0.5255 - val_loss: 0.7027 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 04689: val_accuracy did not improve from 0.55829\n",
            "Epoch 4690/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6889 - accuracy: 0.5196 - val_loss: 0.7028 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04690: val_accuracy did not improve from 0.55829\n",
            "Epoch 4691/5000\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.6885 - accuracy: 0.5299 - val_loss: 0.7028 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04691: val_accuracy did not improve from 0.55829\n",
            "Epoch 4692/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6897 - accuracy: 0.5254 - val_loss: 0.7028 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04692: val_accuracy did not improve from 0.55829\n",
            "Epoch 4693/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6892 - accuracy: 0.5288 - val_loss: 0.7029 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04693: val_accuracy did not improve from 0.55829\n",
            "Epoch 4694/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6886 - accuracy: 0.5342 - val_loss: 0.7030 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04694: val_accuracy did not improve from 0.55829\n",
            "Epoch 4695/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.6888 - accuracy: 0.5250 - val_loss: 0.7031 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04695: val_accuracy did not improve from 0.55829\n",
            "Epoch 4696/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.6892 - accuracy: 0.5304 - val_loss: 0.7031 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 04696: val_accuracy did not improve from 0.55829\n",
            "Epoch 4697/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6888 - accuracy: 0.5313 - val_loss: 0.7033 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 04697: val_accuracy did not improve from 0.55829\n",
            "Epoch 4698/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.6885 - accuracy: 0.5293 - val_loss: 0.7034 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04698: val_accuracy did not improve from 0.55829\n",
            "Epoch 4699/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6889 - accuracy: 0.5342 - val_loss: 0.7036 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04699: val_accuracy did not improve from 0.55829\n",
            "Epoch 4700/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6883 - accuracy: 0.5228 - val_loss: 0.7037 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04700: val_accuracy did not improve from 0.55829\n",
            "Epoch 4701/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6886 - accuracy: 0.5277 - val_loss: 0.7038 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 04701: val_accuracy did not improve from 0.55829\n",
            "Epoch 4702/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6874 - accuracy: 0.5357 - val_loss: 0.7039 - val_accuracy: 0.4924\n",
            "\n",
            "Epoch 04702: val_accuracy did not improve from 0.55829\n",
            "Epoch 4703/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6878 - accuracy: 0.5351 - val_loss: 0.7040 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04703: val_accuracy did not improve from 0.55829\n",
            "Epoch 4704/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.6885 - accuracy: 0.5281 - val_loss: 0.7042 - val_accuracy: 0.4917\n",
            "\n",
            "Epoch 04704: val_accuracy did not improve from 0.55829\n",
            "Epoch 4705/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6886 - accuracy: 0.5310 - val_loss: 0.7043 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04705: val_accuracy did not improve from 0.55829\n",
            "Epoch 4706/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6895 - accuracy: 0.5261 - val_loss: 0.7046 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04706: val_accuracy did not improve from 0.55829\n",
            "Epoch 4707/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6876 - accuracy: 0.5279 - val_loss: 0.7047 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04707: val_accuracy did not improve from 0.55829\n",
            "Epoch 4708/5000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.6884 - accuracy: 0.5297 - val_loss: 0.7049 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04708: val_accuracy did not improve from 0.55829\n",
            "Epoch 4709/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6887 - accuracy: 0.5277 - val_loss: 0.7052 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04709: val_accuracy did not improve from 0.55829\n",
            "Epoch 4710/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.6880 - accuracy: 0.5346 - val_loss: 0.7054 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04710: val_accuracy did not improve from 0.55829\n",
            "Epoch 4711/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6891 - accuracy: 0.5252 - val_loss: 0.7055 - val_accuracy: 0.4931\n",
            "\n",
            "Epoch 04711: val_accuracy did not improve from 0.55829\n",
            "Epoch 4712/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.6879 - accuracy: 0.5353 - val_loss: 0.7056 - val_accuracy: 0.4953\n",
            "\n",
            "Epoch 04712: val_accuracy did not improve from 0.55829\n",
            "Epoch 4713/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6876 - accuracy: 0.5293 - val_loss: 0.7057 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04713: val_accuracy did not improve from 0.55829\n",
            "Epoch 4714/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6878 - accuracy: 0.5310 - val_loss: 0.7057 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 04714: val_accuracy did not improve from 0.55829\n",
            "Epoch 4715/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6883 - accuracy: 0.5284 - val_loss: 0.7057 - val_accuracy: 0.4946\n",
            "\n",
            "Epoch 04715: val_accuracy did not improve from 0.55829\n",
            "Epoch 4716/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6873 - accuracy: 0.5268 - val_loss: 0.7058 - val_accuracy: 0.4967\n",
            "\n",
            "Epoch 04716: val_accuracy did not improve from 0.55829\n",
            "Epoch 4717/5000\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.6882 - accuracy: 0.5246 - val_loss: 0.7059 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04717: val_accuracy did not improve from 0.55829\n",
            "Epoch 4718/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6880 - accuracy: 0.5268 - val_loss: 0.7059 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 04718: val_accuracy did not improve from 0.55829\n",
            "Epoch 4719/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6876 - accuracy: 0.5326 - val_loss: 0.7061 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04719: val_accuracy did not improve from 0.55829\n",
            "Epoch 4720/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6880 - accuracy: 0.5252 - val_loss: 0.7062 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04720: val_accuracy did not improve from 0.55829\n",
            "Epoch 4721/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6873 - accuracy: 0.5361 - val_loss: 0.7063 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04721: val_accuracy did not improve from 0.55829\n",
            "Epoch 4722/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.6879 - accuracy: 0.5255 - val_loss: 0.7063 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04722: val_accuracy did not improve from 0.55829\n",
            "Epoch 4723/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6868 - accuracy: 0.5317 - val_loss: 0.7064 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 04723: val_accuracy did not improve from 0.55829\n",
            "Epoch 4724/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6884 - accuracy: 0.5304 - val_loss: 0.7064 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 04724: val_accuracy did not improve from 0.55829\n",
            "Epoch 4725/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.6869 - accuracy: 0.5357 - val_loss: 0.7065 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04725: val_accuracy did not improve from 0.55829\n",
            "Epoch 4726/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6879 - accuracy: 0.5332 - val_loss: 0.7067 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04726: val_accuracy did not improve from 0.55829\n",
            "Epoch 4727/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.6886 - accuracy: 0.5248 - val_loss: 0.7068 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04727: val_accuracy did not improve from 0.55829\n",
            "Epoch 4728/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6867 - accuracy: 0.5402 - val_loss: 0.7070 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04728: val_accuracy did not improve from 0.55829\n",
            "Epoch 4729/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.6864 - accuracy: 0.5371 - val_loss: 0.7072 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04729: val_accuracy did not improve from 0.55829\n",
            "Epoch 4730/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.6867 - accuracy: 0.5402 - val_loss: 0.7074 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04730: val_accuracy did not improve from 0.55829\n",
            "Epoch 4731/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6873 - accuracy: 0.5339 - val_loss: 0.7076 - val_accuracy: 0.5004\n",
            "\n",
            "Epoch 04731: val_accuracy did not improve from 0.55829\n",
            "Epoch 4732/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6879 - accuracy: 0.5277 - val_loss: 0.7078 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 04732: val_accuracy did not improve from 0.55829\n",
            "Epoch 4733/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6878 - accuracy: 0.5335 - val_loss: 0.7080 - val_accuracy: 0.4982\n",
            "\n",
            "Epoch 04733: val_accuracy did not improve from 0.55829\n",
            "Epoch 4734/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6870 - accuracy: 0.5319 - val_loss: 0.7081 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04734: val_accuracy did not improve from 0.55829\n",
            "Epoch 4735/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6872 - accuracy: 0.5382 - val_loss: 0.7081 - val_accuracy: 0.4989\n",
            "\n",
            "Epoch 04735: val_accuracy did not improve from 0.55829\n",
            "Epoch 4736/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6874 - accuracy: 0.5322 - val_loss: 0.7081 - val_accuracy: 0.4996\n",
            "\n",
            "Epoch 04736: val_accuracy did not improve from 0.55829\n",
            "Epoch 4737/5000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.6886 - accuracy: 0.5254 - val_loss: 0.7080 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04737: val_accuracy did not improve from 0.55829\n",
            "Epoch 4738/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6868 - accuracy: 0.5317 - val_loss: 0.7080 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04738: val_accuracy did not improve from 0.55829\n",
            "Epoch 4739/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6872 - accuracy: 0.5306 - val_loss: 0.7082 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04739: val_accuracy did not improve from 0.55829\n",
            "Epoch 4740/5000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.6875 - accuracy: 0.5268 - val_loss: 0.7084 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04740: val_accuracy did not improve from 0.55829\n",
            "Epoch 4741/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6876 - accuracy: 0.5308 - val_loss: 0.7085 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04741: val_accuracy did not improve from 0.55829\n",
            "Epoch 4742/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.6864 - accuracy: 0.5362 - val_loss: 0.7087 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04742: val_accuracy did not improve from 0.55829\n",
            "Epoch 4743/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6867 - accuracy: 0.5297 - val_loss: 0.7088 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04743: val_accuracy did not improve from 0.55829\n",
            "Epoch 4744/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6868 - accuracy: 0.5310 - val_loss: 0.7088 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04744: val_accuracy did not improve from 0.55829\n",
            "Epoch 4745/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6873 - accuracy: 0.5348 - val_loss: 0.7088 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04745: val_accuracy did not improve from 0.55829\n",
            "Epoch 4746/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6872 - accuracy: 0.5245 - val_loss: 0.7090 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04746: val_accuracy did not improve from 0.55829\n",
            "Epoch 4747/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6863 - accuracy: 0.5404 - val_loss: 0.7092 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04747: val_accuracy did not improve from 0.55829\n",
            "Epoch 4748/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6869 - accuracy: 0.5339 - val_loss: 0.7094 - val_accuracy: 0.5018\n",
            "\n",
            "Epoch 04748: val_accuracy did not improve from 0.55829\n",
            "Epoch 4749/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6861 - accuracy: 0.5319 - val_loss: 0.7097 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04749: val_accuracy did not improve from 0.55829\n",
            "Epoch 4750/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6857 - accuracy: 0.5377 - val_loss: 0.7099 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04750: val_accuracy did not improve from 0.55829\n",
            "Epoch 4751/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6866 - accuracy: 0.5310 - val_loss: 0.7103 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04751: val_accuracy did not improve from 0.55829\n",
            "Epoch 4752/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6849 - accuracy: 0.5322 - val_loss: 0.7106 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04752: val_accuracy did not improve from 0.55829\n",
            "Epoch 4753/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6868 - accuracy: 0.5379 - val_loss: 0.7108 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04753: val_accuracy did not improve from 0.55829\n",
            "Epoch 4754/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6859 - accuracy: 0.5339 - val_loss: 0.7110 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04754: val_accuracy did not improve from 0.55829\n",
            "Epoch 4755/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.6856 - accuracy: 0.5339 - val_loss: 0.7112 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04755: val_accuracy did not improve from 0.55829\n",
            "Epoch 4756/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6863 - accuracy: 0.5458 - val_loss: 0.7111 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04756: val_accuracy did not improve from 0.55829\n",
            "Epoch 4757/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6875 - accuracy: 0.5243 - val_loss: 0.7110 - val_accuracy: 0.5025\n",
            "\n",
            "Epoch 04757: val_accuracy did not improve from 0.55829\n",
            "Epoch 4758/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6860 - accuracy: 0.5348 - val_loss: 0.7109 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04758: val_accuracy did not improve from 0.55829\n",
            "Epoch 4759/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6862 - accuracy: 0.5348 - val_loss: 0.7107 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04759: val_accuracy did not improve from 0.55829\n",
            "Epoch 4760/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6866 - accuracy: 0.5319 - val_loss: 0.7107 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04760: val_accuracy did not improve from 0.55829\n",
            "Epoch 4761/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6857 - accuracy: 0.5408 - val_loss: 0.7108 - val_accuracy: 0.5033\n",
            "\n",
            "Epoch 04761: val_accuracy did not improve from 0.55829\n",
            "Epoch 4762/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6866 - accuracy: 0.5351 - val_loss: 0.7109 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04762: val_accuracy did not improve from 0.55829\n",
            "Epoch 4763/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6856 - accuracy: 0.5404 - val_loss: 0.7111 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 04763: val_accuracy did not improve from 0.55829\n",
            "Epoch 4764/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6847 - accuracy: 0.5467 - val_loss: 0.7114 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04764: val_accuracy did not improve from 0.55829\n",
            "Epoch 4765/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6864 - accuracy: 0.5333 - val_loss: 0.7118 - val_accuracy: 0.5047\n",
            "\n",
            "Epoch 04765: val_accuracy did not improve from 0.55829\n",
            "Epoch 4766/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6857 - accuracy: 0.5422 - val_loss: 0.7120 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04766: val_accuracy did not improve from 0.55829\n",
            "Epoch 4767/5000\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.6872 - accuracy: 0.5261 - val_loss: 0.7123 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04767: val_accuracy did not improve from 0.55829\n",
            "Epoch 4768/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.6860 - accuracy: 0.5315 - val_loss: 0.7127 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04768: val_accuracy did not improve from 0.55829\n",
            "Epoch 4769/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6867 - accuracy: 0.5255 - val_loss: 0.7128 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04769: val_accuracy did not improve from 0.55829\n",
            "Epoch 4770/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6851 - accuracy: 0.5380 - val_loss: 0.7130 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04770: val_accuracy did not improve from 0.55829\n",
            "Epoch 4771/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6864 - accuracy: 0.5324 - val_loss: 0.7129 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04771: val_accuracy did not improve from 0.55829\n",
            "Epoch 4772/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.6860 - accuracy: 0.5295 - val_loss: 0.7129 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04772: val_accuracy did not improve from 0.55829\n",
            "Epoch 4773/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.6858 - accuracy: 0.5391 - val_loss: 0.7128 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04773: val_accuracy did not improve from 0.55829\n",
            "Epoch 4774/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6858 - accuracy: 0.5355 - val_loss: 0.7127 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04774: val_accuracy did not improve from 0.55829\n",
            "Epoch 4775/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.6862 - accuracy: 0.5344 - val_loss: 0.7128 - val_accuracy: 0.5054\n",
            "\n",
            "Epoch 04775: val_accuracy did not improve from 0.55829\n",
            "Epoch 4776/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6855 - accuracy: 0.5350 - val_loss: 0.7129 - val_accuracy: 0.5062\n",
            "\n",
            "Epoch 04776: val_accuracy did not improve from 0.55829\n",
            "Epoch 4777/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6863 - accuracy: 0.5350 - val_loss: 0.7131 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04777: val_accuracy did not improve from 0.55829\n",
            "Epoch 4778/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6856 - accuracy: 0.5344 - val_loss: 0.7134 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04778: val_accuracy did not improve from 0.55829\n",
            "Epoch 4779/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6864 - accuracy: 0.5315 - val_loss: 0.7135 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04779: val_accuracy did not improve from 0.55829\n",
            "Epoch 4780/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.6847 - accuracy: 0.5388 - val_loss: 0.7137 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04780: val_accuracy did not improve from 0.55829\n",
            "Epoch 4781/5000\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.6856 - accuracy: 0.5304 - val_loss: 0.7137 - val_accuracy: 0.5069\n",
            "\n",
            "Epoch 04781: val_accuracy did not improve from 0.55829\n",
            "Epoch 4782/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6854 - accuracy: 0.5373 - val_loss: 0.7140 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04782: val_accuracy did not improve from 0.55829\n",
            "Epoch 4783/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6860 - accuracy: 0.5333 - val_loss: 0.7142 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04783: val_accuracy did not improve from 0.55829\n",
            "Epoch 4784/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6853 - accuracy: 0.5384 - val_loss: 0.7141 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04784: val_accuracy did not improve from 0.55829\n",
            "Epoch 4785/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.6867 - accuracy: 0.5328 - val_loss: 0.7138 - val_accuracy: 0.5083\n",
            "\n",
            "Epoch 04785: val_accuracy did not improve from 0.55829\n",
            "Epoch 4786/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6857 - accuracy: 0.5333 - val_loss: 0.7137 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04786: val_accuracy did not improve from 0.55829\n",
            "Epoch 4787/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6853 - accuracy: 0.5429 - val_loss: 0.7136 - val_accuracy: 0.5091\n",
            "\n",
            "Epoch 04787: val_accuracy did not improve from 0.55829\n",
            "Epoch 4788/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6861 - accuracy: 0.5355 - val_loss: 0.7138 - val_accuracy: 0.5076\n",
            "\n",
            "Epoch 04788: val_accuracy did not improve from 0.55829\n",
            "Epoch 4789/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6852 - accuracy: 0.5337 - val_loss: 0.7141 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04789: val_accuracy did not improve from 0.55829\n",
            "Epoch 4790/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.6865 - accuracy: 0.5382 - val_loss: 0.7148 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04790: val_accuracy did not improve from 0.55829\n",
            "Epoch 4791/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6853 - accuracy: 0.5402 - val_loss: 0.7157 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04791: val_accuracy did not improve from 0.55829\n",
            "Epoch 4792/5000\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.6857 - accuracy: 0.5339 - val_loss: 0.7165 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04792: val_accuracy did not improve from 0.55829\n",
            "Epoch 4793/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6850 - accuracy: 0.5393 - val_loss: 0.7175 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04793: val_accuracy did not improve from 0.55829\n",
            "Epoch 4794/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6858 - accuracy: 0.5357 - val_loss: 0.7185 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04794: val_accuracy did not improve from 0.55829\n",
            "Epoch 4795/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.6861 - accuracy: 0.5333 - val_loss: 0.7191 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04795: val_accuracy did not improve from 0.55829\n",
            "Epoch 4796/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.6847 - accuracy: 0.5377 - val_loss: 0.7196 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04796: val_accuracy did not improve from 0.55829\n",
            "Epoch 4797/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6852 - accuracy: 0.5438 - val_loss: 0.7195 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04797: val_accuracy did not improve from 0.55829\n",
            "Epoch 4798/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.6859 - accuracy: 0.5370 - val_loss: 0.7192 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04798: val_accuracy did not improve from 0.55829\n",
            "Epoch 4799/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6852 - accuracy: 0.5431 - val_loss: 0.7188 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04799: val_accuracy did not improve from 0.55829\n",
            "Epoch 4800/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.6848 - accuracy: 0.5388 - val_loss: 0.7186 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04800: val_accuracy did not improve from 0.55829\n",
            "Epoch 4801/5000\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.6851 - accuracy: 0.5389 - val_loss: 0.7184 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04801: val_accuracy did not improve from 0.55829\n",
            "Epoch 4802/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6859 - accuracy: 0.5346 - val_loss: 0.7184 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04802: val_accuracy did not improve from 0.55829\n",
            "Epoch 4803/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.6849 - accuracy: 0.5397 - val_loss: 0.7186 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04803: val_accuracy did not improve from 0.55829\n",
            "Epoch 4804/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6844 - accuracy: 0.5415 - val_loss: 0.7193 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04804: val_accuracy did not improve from 0.55829\n",
            "Epoch 4805/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.6854 - accuracy: 0.5295 - val_loss: 0.7200 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04805: val_accuracy did not improve from 0.55829\n",
            "Epoch 4806/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6851 - accuracy: 0.5384 - val_loss: 0.7207 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04806: val_accuracy did not improve from 0.55829\n",
            "Epoch 4807/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6843 - accuracy: 0.5351 - val_loss: 0.7211 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04807: val_accuracy did not improve from 0.55829\n",
            "Epoch 4808/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.6860 - accuracy: 0.5355 - val_loss: 0.7214 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04808: val_accuracy did not improve from 0.55829\n",
            "Epoch 4809/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6856 - accuracy: 0.5333 - val_loss: 0.7216 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04809: val_accuracy did not improve from 0.55829\n",
            "Epoch 4810/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6856 - accuracy: 0.5379 - val_loss: 0.7220 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04810: val_accuracy did not improve from 0.55829\n",
            "Epoch 4811/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6850 - accuracy: 0.5359 - val_loss: 0.7222 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04811: val_accuracy did not improve from 0.55829\n",
            "Epoch 4812/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6843 - accuracy: 0.5433 - val_loss: 0.7229 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04812: val_accuracy did not improve from 0.55829\n",
            "Epoch 4813/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6856 - accuracy: 0.5328 - val_loss: 0.7234 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04813: val_accuracy did not improve from 0.55829\n",
            "Epoch 4814/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6847 - accuracy: 0.5379 - val_loss: 0.7240 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04814: val_accuracy did not improve from 0.55829\n",
            "Epoch 4815/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6854 - accuracy: 0.5339 - val_loss: 0.7241 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04815: val_accuracy did not improve from 0.55829\n",
            "Epoch 4816/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6848 - accuracy: 0.5344 - val_loss: 0.7241 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04816: val_accuracy did not improve from 0.55829\n",
            "Epoch 4817/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.6841 - accuracy: 0.5348 - val_loss: 0.7244 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 04817: val_accuracy did not improve from 0.55829\n",
            "Epoch 4818/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6844 - accuracy: 0.5420 - val_loss: 0.7248 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04818: val_accuracy did not improve from 0.55829\n",
            "Epoch 4819/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.6842 - accuracy: 0.5395 - val_loss: 0.7250 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04819: val_accuracy did not improve from 0.55829\n",
            "Epoch 4820/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6829 - accuracy: 0.5482 - val_loss: 0.7255 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04820: val_accuracy did not improve from 0.55829\n",
            "Epoch 4821/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6854 - accuracy: 0.5355 - val_loss: 0.7256 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04821: val_accuracy did not improve from 0.55829\n",
            "Epoch 4822/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6841 - accuracy: 0.5406 - val_loss: 0.7255 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04822: val_accuracy did not improve from 0.55829\n",
            "Epoch 4823/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6845 - accuracy: 0.5373 - val_loss: 0.7255 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04823: val_accuracy did not improve from 0.55829\n",
            "Epoch 4824/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6843 - accuracy: 0.5397 - val_loss: 0.7260 - val_accuracy: 0.5119\n",
            "\n",
            "Epoch 04824: val_accuracy did not improve from 0.55829\n",
            "Epoch 4825/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6850 - accuracy: 0.5375 - val_loss: 0.7272 - val_accuracy: 0.5112\n",
            "\n",
            "Epoch 04825: val_accuracy did not improve from 0.55829\n",
            "Epoch 4826/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6833 - accuracy: 0.5420 - val_loss: 0.7283 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04826: val_accuracy did not improve from 0.55829\n",
            "Epoch 4827/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.6846 - accuracy: 0.5337 - val_loss: 0.7291 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04827: val_accuracy did not improve from 0.55829\n",
            "Epoch 4828/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6830 - accuracy: 0.5428 - val_loss: 0.7300 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04828: val_accuracy did not improve from 0.55829\n",
            "Epoch 4829/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6844 - accuracy: 0.5362 - val_loss: 0.7308 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04829: val_accuracy did not improve from 0.55829\n",
            "Epoch 4830/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6840 - accuracy: 0.5422 - val_loss: 0.7310 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04830: val_accuracy did not improve from 0.55829\n",
            "Epoch 4831/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6852 - accuracy: 0.5386 - val_loss: 0.7311 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04831: val_accuracy did not improve from 0.55829\n",
            "Epoch 4832/5000\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.6858 - accuracy: 0.5359 - val_loss: 0.7307 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04832: val_accuracy did not improve from 0.55829\n",
            "Epoch 4833/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6826 - accuracy: 0.5480 - val_loss: 0.7308 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04833: val_accuracy did not improve from 0.55829\n",
            "Epoch 4834/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6839 - accuracy: 0.5366 - val_loss: 0.7309 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04834: val_accuracy did not improve from 0.55829\n",
            "Epoch 4835/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6841 - accuracy: 0.5413 - val_loss: 0.7313 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04835: val_accuracy did not improve from 0.55829\n",
            "Epoch 4836/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6838 - accuracy: 0.5411 - val_loss: 0.7321 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04836: val_accuracy did not improve from 0.55829\n",
            "Epoch 4837/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6846 - accuracy: 0.5393 - val_loss: 0.7326 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04837: val_accuracy did not improve from 0.55829\n",
            "Epoch 4838/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6829 - accuracy: 0.5408 - val_loss: 0.7329 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04838: val_accuracy did not improve from 0.55829\n",
            "Epoch 4839/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.6838 - accuracy: 0.5411 - val_loss: 0.7332 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04839: val_accuracy did not improve from 0.55829\n",
            "Epoch 4840/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.6839 - accuracy: 0.5496 - val_loss: 0.7337 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04840: val_accuracy did not improve from 0.55829\n",
            "Epoch 4841/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6843 - accuracy: 0.5406 - val_loss: 0.7345 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04841: val_accuracy did not improve from 0.55829\n",
            "Epoch 4842/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.6828 - accuracy: 0.5444 - val_loss: 0.7354 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04842: val_accuracy did not improve from 0.55829\n",
            "Epoch 4843/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6840 - accuracy: 0.5361 - val_loss: 0.7365 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04843: val_accuracy did not improve from 0.55829\n",
            "Epoch 4844/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6837 - accuracy: 0.5366 - val_loss: 0.7366 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04844: val_accuracy did not improve from 0.55829\n",
            "Epoch 4845/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6837 - accuracy: 0.5409 - val_loss: 0.7354 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04845: val_accuracy did not improve from 0.55829\n",
            "Epoch 4846/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6835 - accuracy: 0.5379 - val_loss: 0.7343 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04846: val_accuracy did not improve from 0.55829\n",
            "Epoch 4847/5000\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.6828 - accuracy: 0.5428 - val_loss: 0.7333 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04847: val_accuracy did not improve from 0.55829\n",
            "Epoch 4848/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6842 - accuracy: 0.5395 - val_loss: 0.7327 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04848: val_accuracy did not improve from 0.55829\n",
            "Epoch 4849/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6843 - accuracy: 0.5357 - val_loss: 0.7327 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04849: val_accuracy did not improve from 0.55829\n",
            "Epoch 4850/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6834 - accuracy: 0.5458 - val_loss: 0.7336 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04850: val_accuracy did not improve from 0.55829\n",
            "Epoch 4851/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6832 - accuracy: 0.5453 - val_loss: 0.7347 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04851: val_accuracy did not improve from 0.55829\n",
            "Epoch 4852/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6835 - accuracy: 0.5411 - val_loss: 0.7358 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04852: val_accuracy did not improve from 0.55829\n",
            "Epoch 4853/5000\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6825 - accuracy: 0.5428 - val_loss: 0.7371 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04853: val_accuracy did not improve from 0.55829\n",
            "Epoch 4854/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6835 - accuracy: 0.5417 - val_loss: 0.7385 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04854: val_accuracy did not improve from 0.55829\n",
            "Epoch 4855/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6822 - accuracy: 0.5431 - val_loss: 0.7398 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04855: val_accuracy did not improve from 0.55829\n",
            "Epoch 4856/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.6836 - accuracy: 0.5496 - val_loss: 0.7408 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04856: val_accuracy did not improve from 0.55829\n",
            "Epoch 4857/5000\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.6834 - accuracy: 0.5431 - val_loss: 0.7408 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04857: val_accuracy did not improve from 0.55829\n",
            "Epoch 4858/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6822 - accuracy: 0.5442 - val_loss: 0.7405 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04858: val_accuracy did not improve from 0.55829\n",
            "Epoch 4859/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6830 - accuracy: 0.5446 - val_loss: 0.7399 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04859: val_accuracy did not improve from 0.55829\n",
            "Epoch 4860/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6834 - accuracy: 0.5384 - val_loss: 0.7388 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04860: val_accuracy did not improve from 0.55829\n",
            "Epoch 4861/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6835 - accuracy: 0.5382 - val_loss: 0.7382 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04861: val_accuracy did not improve from 0.55829\n",
            "Epoch 4862/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6837 - accuracy: 0.5351 - val_loss: 0.7373 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04862: val_accuracy did not improve from 0.55829\n",
            "Epoch 4863/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6826 - accuracy: 0.5426 - val_loss: 0.7368 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04863: val_accuracy did not improve from 0.55829\n",
            "Epoch 4864/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6836 - accuracy: 0.5339 - val_loss: 0.7375 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04864: val_accuracy did not improve from 0.55829\n",
            "Epoch 4865/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.6828 - accuracy: 0.5431 - val_loss: 0.7387 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04865: val_accuracy did not improve from 0.55829\n",
            "Epoch 4866/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6824 - accuracy: 0.5413 - val_loss: 0.7402 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04866: val_accuracy did not improve from 0.55829\n",
            "Epoch 4867/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.6822 - accuracy: 0.5420 - val_loss: 0.7422 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04867: val_accuracy did not improve from 0.55829\n",
            "Epoch 4868/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.6811 - accuracy: 0.5514 - val_loss: 0.7442 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04868: val_accuracy did not improve from 0.55829\n",
            "Epoch 4869/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6831 - accuracy: 0.5350 - val_loss: 0.7458 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04869: val_accuracy did not improve from 0.55829\n",
            "Epoch 4870/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6836 - accuracy: 0.5415 - val_loss: 0.7469 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04870: val_accuracy did not improve from 0.55829\n",
            "Epoch 4871/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.6808 - accuracy: 0.5467 - val_loss: 0.7477 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04871: val_accuracy did not improve from 0.55829\n",
            "Epoch 4872/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6823 - accuracy: 0.5433 - val_loss: 0.7467 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04872: val_accuracy did not improve from 0.55829\n",
            "Epoch 4873/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6833 - accuracy: 0.5399 - val_loss: 0.7452 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04873: val_accuracy did not improve from 0.55829\n",
            "Epoch 4874/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.6821 - accuracy: 0.5415 - val_loss: 0.7427 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04874: val_accuracy did not improve from 0.55829\n",
            "Epoch 4875/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6819 - accuracy: 0.5440 - val_loss: 0.7413 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04875: val_accuracy did not improve from 0.55829\n",
            "Epoch 4876/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6821 - accuracy: 0.5453 - val_loss: 0.7406 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04876: val_accuracy did not improve from 0.55829\n",
            "Epoch 4877/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6824 - accuracy: 0.5451 - val_loss: 0.7416 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 04877: val_accuracy did not improve from 0.55829\n",
            "Epoch 4878/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6822 - accuracy: 0.5433 - val_loss: 0.7428 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04878: val_accuracy did not improve from 0.55829\n",
            "Epoch 4879/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.6814 - accuracy: 0.5476 - val_loss: 0.7437 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04879: val_accuracy did not improve from 0.55829\n",
            "Epoch 4880/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6808 - accuracy: 0.5513 - val_loss: 0.7453 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 04880: val_accuracy did not improve from 0.55829\n",
            "Epoch 4881/5000\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.6827 - accuracy: 0.5422 - val_loss: 0.7470 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 04881: val_accuracy did not improve from 0.55829\n",
            "Epoch 4882/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6826 - accuracy: 0.5404 - val_loss: 0.7490 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04882: val_accuracy did not improve from 0.55829\n",
            "Epoch 4883/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.6817 - accuracy: 0.5502 - val_loss: 0.7493 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 04883: val_accuracy did not improve from 0.55829\n",
            "Epoch 4884/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6819 - accuracy: 0.5467 - val_loss: 0.7494 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04884: val_accuracy did not improve from 0.55829\n",
            "Epoch 4885/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6808 - accuracy: 0.5513 - val_loss: 0.7487 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04885: val_accuracy did not improve from 0.55829\n",
            "Epoch 4886/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6817 - accuracy: 0.5440 - val_loss: 0.7487 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04886: val_accuracy did not improve from 0.55829\n",
            "Epoch 4887/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.6803 - accuracy: 0.5489 - val_loss: 0.7491 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04887: val_accuracy did not improve from 0.55829\n",
            "Epoch 4888/5000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.6795 - accuracy: 0.5547 - val_loss: 0.7509 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04888: val_accuracy did not improve from 0.55829\n",
            "Epoch 4889/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6812 - accuracy: 0.5514 - val_loss: 0.7536 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 04889: val_accuracy did not improve from 0.55829\n",
            "Epoch 4890/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6825 - accuracy: 0.5393 - val_loss: 0.7559 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04890: val_accuracy did not improve from 0.55829\n",
            "Epoch 4891/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.6811 - accuracy: 0.5424 - val_loss: 0.7580 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 04891: val_accuracy did not improve from 0.55829\n",
            "Epoch 4892/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6828 - accuracy: 0.5446 - val_loss: 0.7590 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04892: val_accuracy did not improve from 0.55829\n",
            "Epoch 4893/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6818 - accuracy: 0.5478 - val_loss: 0.7603 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 04893: val_accuracy did not improve from 0.55829\n",
            "Epoch 4894/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6819 - accuracy: 0.5413 - val_loss: 0.7621 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04894: val_accuracy did not improve from 0.55829\n",
            "Epoch 4895/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6790 - accuracy: 0.5489 - val_loss: 0.7649 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04895: val_accuracy did not improve from 0.55829\n",
            "Epoch 4896/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6807 - accuracy: 0.5502 - val_loss: 0.7674 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04896: val_accuracy did not improve from 0.55829\n",
            "Epoch 4897/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6796 - accuracy: 0.5553 - val_loss: 0.7702 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04897: val_accuracy did not improve from 0.55829\n",
            "Epoch 4898/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6811 - accuracy: 0.5482 - val_loss: 0.7730 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04898: val_accuracy did not improve from 0.55829\n",
            "Epoch 4899/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6816 - accuracy: 0.5397 - val_loss: 0.7760 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04899: val_accuracy did not improve from 0.55829\n",
            "Epoch 4900/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6817 - accuracy: 0.5422 - val_loss: 0.7793 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04900: val_accuracy did not improve from 0.55829\n",
            "Epoch 4901/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6820 - accuracy: 0.5424 - val_loss: 0.7815 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04901: val_accuracy did not improve from 0.55829\n",
            "Epoch 4902/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.6806 - accuracy: 0.5406 - val_loss: 0.7835 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04902: val_accuracy did not improve from 0.55829\n",
            "Epoch 4903/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6807 - accuracy: 0.5397 - val_loss: 0.7864 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04903: val_accuracy did not improve from 0.55829\n",
            "Epoch 4904/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.6811 - accuracy: 0.5486 - val_loss: 0.7902 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04904: val_accuracy did not improve from 0.55829\n",
            "Epoch 4905/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6789 - accuracy: 0.5480 - val_loss: 0.7931 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04905: val_accuracy did not improve from 0.55829\n",
            "Epoch 4906/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6802 - accuracy: 0.5554 - val_loss: 0.7962 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04906: val_accuracy did not improve from 0.55829\n",
            "Epoch 4907/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6801 - accuracy: 0.5511 - val_loss: 0.7965 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04907: val_accuracy did not improve from 0.55829\n",
            "Epoch 4908/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6791 - accuracy: 0.5507 - val_loss: 0.7941 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04908: val_accuracy did not improve from 0.55829\n",
            "Epoch 4909/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6792 - accuracy: 0.5529 - val_loss: 0.7935 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04909: val_accuracy did not improve from 0.55829\n",
            "Epoch 4910/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6802 - accuracy: 0.5464 - val_loss: 0.7936 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04910: val_accuracy did not improve from 0.55829\n",
            "Epoch 4911/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.6803 - accuracy: 0.5455 - val_loss: 0.7934 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04911: val_accuracy did not improve from 0.55829\n",
            "Epoch 4912/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6793 - accuracy: 0.5542 - val_loss: 0.7949 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04912: val_accuracy did not improve from 0.55829\n",
            "Epoch 4913/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6792 - accuracy: 0.5545 - val_loss: 0.7981 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04913: val_accuracy did not improve from 0.55829\n",
            "Epoch 4914/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6803 - accuracy: 0.5502 - val_loss: 0.8004 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04914: val_accuracy did not improve from 0.55829\n",
            "Epoch 4915/5000\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.6793 - accuracy: 0.5473 - val_loss: 0.8017 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04915: val_accuracy did not improve from 0.55829\n",
            "Epoch 4916/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6794 - accuracy: 0.5457 - val_loss: 0.8023 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 04916: val_accuracy did not improve from 0.55829\n",
            "Epoch 4917/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6788 - accuracy: 0.5558 - val_loss: 0.8026 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 04917: val_accuracy did not improve from 0.55829\n",
            "Epoch 4918/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6783 - accuracy: 0.5543 - val_loss: 0.8037 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 04918: val_accuracy did not improve from 0.55829\n",
            "Epoch 4919/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6782 - accuracy: 0.5504 - val_loss: 0.8039 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 04919: val_accuracy did not improve from 0.55829\n",
            "Epoch 4920/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6790 - accuracy: 0.5478 - val_loss: 0.8037 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 04920: val_accuracy did not improve from 0.55829\n",
            "Epoch 4921/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.6788 - accuracy: 0.5498 - val_loss: 0.8052 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04921: val_accuracy did not improve from 0.55829\n",
            "Epoch 4922/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.6781 - accuracy: 0.5543 - val_loss: 0.8075 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04922: val_accuracy did not improve from 0.55829\n",
            "Epoch 4923/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.6774 - accuracy: 0.5531 - val_loss: 0.8098 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 04923: val_accuracy did not improve from 0.55829\n",
            "Epoch 4924/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6793 - accuracy: 0.5531 - val_loss: 0.8099 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 04924: val_accuracy did not improve from 0.55829\n",
            "Epoch 4925/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6782 - accuracy: 0.5496 - val_loss: 0.8079 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 04925: val_accuracy did not improve from 0.55829\n",
            "Epoch 4926/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6782 - accuracy: 0.5549 - val_loss: 0.8070 - val_accuracy: 0.5185\n",
            "\n",
            "Epoch 04926: val_accuracy did not improve from 0.55829\n",
            "Epoch 4927/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6769 - accuracy: 0.5556 - val_loss: 0.8080 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04927: val_accuracy did not improve from 0.55829\n",
            "Epoch 4928/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6777 - accuracy: 0.5542 - val_loss: 0.8086 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04928: val_accuracy did not improve from 0.55829\n",
            "Epoch 4929/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.6768 - accuracy: 0.5576 - val_loss: 0.8138 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04929: val_accuracy did not improve from 0.55829\n",
            "Epoch 4930/5000\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.6770 - accuracy: 0.5551 - val_loss: 0.8161 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04930: val_accuracy did not improve from 0.55829\n",
            "Epoch 4931/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6771 - accuracy: 0.5542 - val_loss: 0.8146 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04931: val_accuracy did not improve from 0.55829\n",
            "Epoch 4932/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6772 - accuracy: 0.5522 - val_loss: 0.8123 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04932: val_accuracy did not improve from 0.55829\n",
            "Epoch 4933/5000\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.6781 - accuracy: 0.5545 - val_loss: 0.8100 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04933: val_accuracy did not improve from 0.55829\n",
            "Epoch 4934/5000\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.6767 - accuracy: 0.5589 - val_loss: 0.8103 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04934: val_accuracy did not improve from 0.55829\n",
            "Epoch 4935/5000\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6764 - accuracy: 0.5518 - val_loss: 0.8214 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 04935: val_accuracy did not improve from 0.55829\n",
            "Epoch 4936/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6773 - accuracy: 0.5522 - val_loss: 0.8330 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 04936: val_accuracy did not improve from 0.55829\n",
            "Epoch 4937/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6766 - accuracy: 0.5629 - val_loss: 0.8406 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 04937: val_accuracy did not improve from 0.55829\n",
            "Epoch 4938/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.6775 - accuracy: 0.5514 - val_loss: 0.8360 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 04938: val_accuracy did not improve from 0.55829\n",
            "Epoch 4939/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6772 - accuracy: 0.5554 - val_loss: 0.8229 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 04939: val_accuracy did not improve from 0.55829\n",
            "Epoch 4940/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6761 - accuracy: 0.5527 - val_loss: 0.8138 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04940: val_accuracy did not improve from 0.55829\n",
            "Epoch 4941/5000\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.6760 - accuracy: 0.5556 - val_loss: 0.8100 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04941: val_accuracy did not improve from 0.55829\n",
            "Epoch 4942/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.6776 - accuracy: 0.5513 - val_loss: 0.8126 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04942: val_accuracy did not improve from 0.55829\n",
            "Epoch 4943/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.6753 - accuracy: 0.5589 - val_loss: 0.8193 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04943: val_accuracy did not improve from 0.55829\n",
            "Epoch 4944/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6775 - accuracy: 0.5498 - val_loss: 0.8245 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 04944: val_accuracy did not improve from 0.55829\n",
            "Epoch 4945/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6772 - accuracy: 0.5543 - val_loss: 0.8208 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04945: val_accuracy did not improve from 0.55829\n",
            "Epoch 4946/5000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.6760 - accuracy: 0.5536 - val_loss: 0.8164 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 04946: val_accuracy did not improve from 0.55829\n",
            "Epoch 4947/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6766 - accuracy: 0.5576 - val_loss: 0.8158 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 04947: val_accuracy did not improve from 0.55829\n",
            "Epoch 4948/5000\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.6759 - accuracy: 0.5578 - val_loss: 0.8241 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 04948: val_accuracy did not improve from 0.55829\n",
            "Epoch 4949/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6756 - accuracy: 0.5525 - val_loss: 0.8337 - val_accuracy: 0.5257\n",
            "\n",
            "Epoch 04949: val_accuracy did not improve from 0.55829\n",
            "Epoch 4950/5000\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.6774 - accuracy: 0.5542 - val_loss: 0.8415 - val_accuracy: 0.5272\n",
            "\n",
            "Epoch 04950: val_accuracy did not improve from 0.55829\n",
            "Epoch 4951/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.6755 - accuracy: 0.5527 - val_loss: 0.8386 - val_accuracy: 0.5272\n",
            "\n",
            "Epoch 04951: val_accuracy did not improve from 0.55829\n",
            "Epoch 4952/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6761 - accuracy: 0.5545 - val_loss: 0.8390 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 04952: val_accuracy did not improve from 0.55829\n",
            "Epoch 4953/5000\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.6759 - accuracy: 0.5487 - val_loss: 0.8387 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 04953: val_accuracy did not improve from 0.55829\n",
            "Epoch 4954/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6758 - accuracy: 0.5574 - val_loss: 0.8419 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 04954: val_accuracy did not improve from 0.55829\n",
            "Epoch 4955/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.6749 - accuracy: 0.5621 - val_loss: 0.8467 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 04955: val_accuracy did not improve from 0.55829\n",
            "Epoch 4956/5000\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.6751 - accuracy: 0.5547 - val_loss: 0.8517 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 04956: val_accuracy did not improve from 0.55829\n",
            "Epoch 4957/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6758 - accuracy: 0.5560 - val_loss: 0.8501 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 04957: val_accuracy did not improve from 0.55829\n",
            "Epoch 4958/5000\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.6758 - accuracy: 0.5525 - val_loss: 0.8488 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 04958: val_accuracy did not improve from 0.55829\n",
            "Epoch 4959/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6755 - accuracy: 0.5578 - val_loss: 0.8443 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 04959: val_accuracy did not improve from 0.55829\n",
            "Epoch 4960/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.6757 - accuracy: 0.5556 - val_loss: 0.8440 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 04960: val_accuracy did not improve from 0.55829\n",
            "Epoch 4961/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6748 - accuracy: 0.5542 - val_loss: 0.8488 - val_accuracy: 0.5235\n",
            "\n",
            "Epoch 04961: val_accuracy did not improve from 0.55829\n",
            "Epoch 4962/5000\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.6754 - accuracy: 0.5551 - val_loss: 0.8560 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 04962: val_accuracy did not improve from 0.55829\n",
            "Epoch 4963/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6742 - accuracy: 0.5571 - val_loss: 0.8631 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 04963: val_accuracy did not improve from 0.55829\n",
            "Epoch 4964/5000\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.6747 - accuracy: 0.5620 - val_loss: 0.8612 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04964: val_accuracy did not improve from 0.55829\n",
            "Epoch 4965/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6734 - accuracy: 0.5601 - val_loss: 0.8607 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04965: val_accuracy did not improve from 0.55829\n",
            "Epoch 4966/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6745 - accuracy: 0.5605 - val_loss: 0.8612 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04966: val_accuracy did not improve from 0.55829\n",
            "Epoch 4967/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.6742 - accuracy: 0.5589 - val_loss: 0.8624 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04967: val_accuracy did not improve from 0.55829\n",
            "Epoch 4968/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6743 - accuracy: 0.5589 - val_loss: 0.8635 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04968: val_accuracy did not improve from 0.55829\n",
            "Epoch 4969/5000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.6739 - accuracy: 0.5601 - val_loss: 0.8681 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04969: val_accuracy did not improve from 0.55829\n",
            "Epoch 4970/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.6746 - accuracy: 0.5558 - val_loss: 0.8769 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04970: val_accuracy did not improve from 0.55829\n",
            "Epoch 4971/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6745 - accuracy: 0.5572 - val_loss: 0.8593 - val_accuracy: 0.5141\n",
            "\n",
            "Epoch 04971: val_accuracy did not improve from 0.55829\n",
            "Epoch 4972/5000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.6745 - accuracy: 0.5585 - val_loss: 0.8522 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04972: val_accuracy did not improve from 0.55829\n",
            "Epoch 4973/5000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.6755 - accuracy: 0.5627 - val_loss: 0.8588 - val_accuracy: 0.5192\n",
            "\n",
            "Epoch 04973: val_accuracy did not improve from 0.55829\n",
            "Epoch 4974/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6746 - accuracy: 0.5625 - val_loss: 0.8756 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04974: val_accuracy did not improve from 0.55829\n",
            "Epoch 4975/5000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.6719 - accuracy: 0.5710 - val_loss: 0.8597 - val_accuracy: 0.5206\n",
            "\n",
            "Epoch 04975: val_accuracy did not improve from 0.55829\n",
            "Epoch 4976/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.6737 - accuracy: 0.5598 - val_loss: 0.8473 - val_accuracy: 0.5214\n",
            "\n",
            "Epoch 04976: val_accuracy did not improve from 0.55829\n",
            "Epoch 4977/5000\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.6748 - accuracy: 0.5638 - val_loss: 0.8454 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 04977: val_accuracy did not improve from 0.55829\n",
            "Epoch 4978/5000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.6732 - accuracy: 0.5672 - val_loss: 0.8506 - val_accuracy: 0.5199\n",
            "\n",
            "Epoch 04978: val_accuracy did not improve from 0.55829\n",
            "Epoch 4979/5000\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.6723 - accuracy: 0.5649 - val_loss: 0.8611 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04979: val_accuracy did not improve from 0.55829\n",
            "Epoch 4980/5000\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.6732 - accuracy: 0.5567 - val_loss: 0.8669 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04980: val_accuracy did not improve from 0.55829\n",
            "Epoch 4981/5000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.6729 - accuracy: 0.5605 - val_loss: 0.8612 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04981: val_accuracy did not improve from 0.55829\n",
            "Epoch 4982/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6738 - accuracy: 0.5580 - val_loss: 0.8522 - val_accuracy: 0.5134\n",
            "\n",
            "Epoch 04982: val_accuracy did not improve from 0.55829\n",
            "Epoch 4983/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.6726 - accuracy: 0.5609 - val_loss: 0.8524 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04983: val_accuracy did not improve from 0.55829\n",
            "Epoch 4984/5000\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 0.6733 - accuracy: 0.5609 - val_loss: 0.8658 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04984: val_accuracy did not improve from 0.55829\n",
            "Epoch 4985/5000\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 0.6731 - accuracy: 0.5600 - val_loss: 0.8809 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 04985: val_accuracy did not improve from 0.55829\n",
            "Epoch 4986/5000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.6731 - accuracy: 0.5663 - val_loss: 0.8792 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04986: val_accuracy did not improve from 0.55829\n",
            "Epoch 4987/5000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.6721 - accuracy: 0.5583 - val_loss: 0.8631 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04987: val_accuracy did not improve from 0.55829\n",
            "Epoch 4988/5000\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 0.6730 - accuracy: 0.5572 - val_loss: 0.8544 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04988: val_accuracy did not improve from 0.55829\n",
            "Epoch 4989/5000\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6706 - accuracy: 0.5639 - val_loss: 0.8641 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04989: val_accuracy did not improve from 0.55829\n",
            "Epoch 4990/5000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.6713 - accuracy: 0.5663 - val_loss: 0.8839 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 04990: val_accuracy did not improve from 0.55829\n",
            "Epoch 4991/5000\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.6709 - accuracy: 0.5645 - val_loss: 0.9035 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04991: val_accuracy did not improve from 0.55829\n",
            "Epoch 4992/5000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.6717 - accuracy: 0.5639 - val_loss: 0.9073 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04992: val_accuracy did not improve from 0.55829\n",
            "Epoch 4993/5000\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.6717 - accuracy: 0.5611 - val_loss: 0.8945 - val_accuracy: 0.5163\n",
            "\n",
            "Epoch 04993: val_accuracy did not improve from 0.55829\n",
            "Epoch 4994/5000\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.6717 - accuracy: 0.5658 - val_loss: 0.8857 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04994: val_accuracy did not improve from 0.55829\n",
            "Epoch 4995/5000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.6725 - accuracy: 0.5645 - val_loss: 0.8760 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 04995: val_accuracy did not improve from 0.55829\n",
            "Epoch 4996/5000\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.6721 - accuracy: 0.5603 - val_loss: 0.8734 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04996: val_accuracy did not improve from 0.55829\n",
            "Epoch 4997/5000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.6698 - accuracy: 0.5679 - val_loss: 0.8898 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 04997: val_accuracy did not improve from 0.55829\n",
            "Epoch 4998/5000\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.6708 - accuracy: 0.5678 - val_loss: 0.9093 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04998: val_accuracy did not improve from 0.55829\n",
            "Epoch 4999/5000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.6705 - accuracy: 0.5707 - val_loss: 0.9018 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 04999: val_accuracy did not improve from 0.55829\n",
            "Epoch 5000/5000\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.6712 - accuracy: 0.5623 - val_loss: 0.8927 - val_accuracy: 0.5156\n",
            "\n",
            "Epoch 05000: val_accuracy did not improve from 0.55829\n",
            "173/173 [==============================] - 2s 13ms/step - loss: 0.6680 - accuracy: 0.5696\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.8927 - accuracy: 0.5156\n",
            "Train: 0.570, Test: 0.516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_latest/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_latest/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OazllwgwAK7D"
      },
      "source": [
        "### Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KQZBSrOQRRq"
      },
      "source": [
        "def calc_confusion_matrix(X, y_true, y_pred, min_change=1):\n",
        "    assert len(X) == len(y_true)\n",
        "    assert len(X) == len(y_pred)\n",
        "    n = len(y_true)\n",
        "    y_true_direction = [get_direction(X[i][-1], y_true[i], min_change) for i in range(n)]\n",
        "    y_pred_direction = [get_direction(X[i][-1], y_pred[i], min_change) for i in range(n)]\n",
        "    cm = confusion_matrix(y_true_direction, y_pred_direction, labels=[-1, 0, 1])\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                                  display_labels=[-1, 0, 1])\n",
        "    disp.plot()\n",
        "    return cm\n",
        "\n",
        "def calc_accuracy(X, y_true, y_pred, min_change=1):\n",
        "    assert len(X) == len(y_true)\n",
        "    assert len(X) == len(y_pred)\n",
        "    n = len(y_true)\n",
        "    y_true_direction = [get_direction(X[i][-1], y_true[i], min_change) for i in range(n)]\n",
        "    y_pred_direction = [get_direction(X[i][-1], y_pred[i], min_change) for i in range(n)]\n",
        "    return accuracy_score(y_true_direction, y_pred_direction)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeeHAryE_2Y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4646740-d572-4b4e-c9bb-30f172784f84"
      },
      "source": [
        "lstm_model = keras.models.load_model(\"/content/drive/MyDrive/PredictPriceByPattern/LSTM_double_top_best_val_acc\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b50yUip0AQwQ"
      },
      "source": [
        "# y_pred = lstm_model.predict(np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1)))\n",
        "y_pred = lstm_model.predict(np.reshape(X_test_scaled, (X_test_scaled.shape[0], X_test_scaled.shape[1], 1)))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWVeRc87ARJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51af1b27-764d-41ab-9671-eb49063e3a4e"
      },
      "source": [
        "y_test_scaled_categorical, y_pred"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        ...,\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.]], dtype=float32), array([[0.49948263, 0.5107165 ],\n",
              "        [0.41406503, 0.57762533],\n",
              "        [0.50004953, 0.5117473 ],\n",
              "        ...,\n",
              "        [0.4729133 , 0.52918935],\n",
              "        [0.44549006, 0.5561306 ],\n",
              "        [0.5200995 , 0.48245943]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTeRsDeZit4J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "outputId": "ebddbeca-4328-4f57-ce12-7974974698ae"
      },
      "source": [
        "def show_val_res_label(y_true_label, y_pred_label):\n",
        "    # y_true_label = y_true_label - 1\n",
        "    # y_pred_label = y_pred_label - 1\n",
        "    labels = list(range(np.min(y_true_label), np.max(y_true_label) + 1))\n",
        "    cm = confusion_matrix(y_true_label, y_pred_label, labels=labels)\n",
        "    print('confusion_matrix:', cm)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                                  display_labels=labels)\n",
        "    disp.plot()\n",
        "\n",
        "    acc = accuracy_score(y_true_label, y_pred_label)\n",
        "    print('acc:', acc)\n",
        "\n",
        "    print(classification_report(y_true_label, y_pred_label))\n",
        "\n",
        "def show_val_res_categorical(y_true, y_pred):\n",
        "    y_true_label = np.argmax(y_true, axis=1)\n",
        "    y_pred_label = np.argmax(y_pred, axis=1)\n",
        "    show_val_res_label(y_true_label, y_pred_label)\n",
        "\n",
        "# show_val_res_categorical(y_test_categorical, y_pred)\n",
        "# show_val_res_categorical(y_train_categorical, lstm_model.predict(np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))))\n",
        "show_val_res_categorical(y_test_scaled_categorical, y_pred)\n",
        "show_val_res_categorical(y_train_scaled_categorical, lstm_model.predict(np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion_matrix: [[334 352]\n",
            " [258 437]]\n",
            "acc: 0.558291093410572\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.49      0.52       686\n",
            "           1       0.55      0.63      0.59       695\n",
            "\n",
            "    accuracy                           0.56      1381\n",
            "   macro avg       0.56      0.56      0.56      1381\n",
            "weighted avg       0.56      0.56      0.56      1381\n",
            "\n",
            "confusion_matrix: [[1447 1329]\n",
            " [ 956 1788]]\n",
            "acc: 0.5860507246376812\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.52      0.56      2776\n",
            "           1       0.57      0.65      0.61      2744\n",
            "\n",
            "    accuracy                           0.59      5520\n",
            "   macro avg       0.59      0.59      0.58      5520\n",
            "weighted avg       0.59      0.59      0.58      5520\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEGCAYAAADxD4m3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhcVZnH8e/bWzpbd6ezkRUCBAgEggiEgGxBCYsjyLA7yiAYUVAGVBwZBwQRBXVQUGQQEILsq4hJCLJIWMMyELNAaAgkISFbZ+90Ol31zh/3dNJJb3XTVemum9/nee5D1bmn7jnVTd4+9557z2vujohIEhV0dAdERHJFAU5EEksBTkQSSwFORBJLAU5EEquoozvQWGG37l5cUdnR3ZAYvLijeyBx1C+vJrV2nbXnGOOO7u7Lq1MZ1X1z+oan3P249rTXHp0qwBVXVLLL+Zd2dDckhtoBmf2PLp3Dol/8tt3HWF6dYtpTQzOqWzjg/T7tbrAdOlWAE5HOz4E06Y7uRkYU4EQkFsfZ6PkxcleAE5HYNIITkURynFSePOKpACcisaVRgBORBHIgpQAnIkmlEZyIJJIDG3UNTkSSyHGdoopIQjmk8iO+KcCJSDzRkwz5QQFORGIyUrTref3tRgFORGKJJhkU4EQkgaL74BTgRCSh0hrBiUgS5dMITkuWi0gsjpGiIKMtE2ZWaGb/Z2ZPhvf3mNl7ZjbDzO4ws+JQbmZ2o5lVmdl0MzugrWMrwIlIbGm3jLYMXQzMbvT+HmAvYF+gK3B+KD8eGB628cAf2jqwApyIxOIYdV6Y0dYWMxsMnAjctun47hM9AKYBg8Ouk4AJYderQIWZDWjt+ApwIhJLdKNvQUYb0MfM3mi0jd/qcL8BLqOZe4fDqelXgcmhaBAwv1GVBaGsRZpkEJHYYkwyLHP3A5vbYWZfBJa4+5tmdlQzVW4GXnD3qdvWSwU4EYnJ3Uh5Vk7+DgO+ZGYnAKVAmZn92d3/zcyuBPoC32xU/xNgSKP3g0NZi3SKKiKxpbGMtta4+4/cfbC77wKcCTwbgtv5wDjgLHdvfOr6BPC1MJt6CLDK3Re11oZGcCISSzTJkNPQcQvwMfCKmQE86u5XAxOBE4AqoAY4t60DKcCJSCwNkwxZPab788Dz4XWzcSnMql4Y57gKcCISW0qPaolIEjU8yZAPFOBEJLZ0dmZRc04BTkRiiR62V4ATkQRyjI0ZPIbVGSjAiUgs7mTrRt+cU4ATkZjavom3s1CAE5FYHI3gRCTBNMkgIonkxFrMskMpwIlILFHawPwIHfnRSxHpRJT4WUQSytGTDCKSYBrBiUgiuVvejODyo5ci0mlEkwyFGW2ZaCYv6jAzey3kP33AzEpCeZfwvirs36WtYyvAiUhMUU6GTLYMbZ0X9TrgBnffHVgBnBfKzwNWhPIbQr1WKcCJSCzRJEN2Ej9vnRfVojXKxwIPhyp3ASeH1yeF94T9x4T6LdI1OBGJLcaTDH3M7I1G729191sbvW/Ii9ozvO8NrHT3+vC+ce7TTXlR3b3ezFaF+staalwBTkRiifkkQ3vyorabApyIxJalpDNN8qICvwUqzKwojOIa5z5tyIu6wMyKgHJgeWsN6BqciMTiDhvTBRltrR+n2byoXwGeA04N1c4B/hJePxHeE/Y/GzJttUgjOBGJJTpFzenY6IfA/WZ2DfB/wO2h/HbgbjOrAqqJgmKrFOBEJLZsP8mwVV7UD4GDm6lTC5wW57gKcFspKaxnwsl/oaQwRVFBmikf7MrvXt/yZ33GPjM5a+QM0m6s21jMT54/kg9WVLar3UE9V/PrY5+morSWmUv78p9/P4aN6ULOGfUOp46YTb0bK9Z35cfPHs3CtT3bPuAOxDamGXzDLKzeIeWs/Uwl1V8cvEWdnq8spc/j80iVlwCw8sj+rD6sX7vaLVhXz4A73qdo+Qbqe3dh0XnDSXcroue0ZfR6eiEA6S6FLDlzF+oGd29XW51Jw20i+SCnAc7MjiO6aFgI3Obuv8hle9lQlyrk63/5EjX1xRQVpPjzlx/nhXlDmb54p011npwznAdm7gPA0bvM5bLDXuabT34xo+OfvOe7DCpbw+9fP2iL8u+NeZW73tmPSVXDufLIf3DKiNk8MHMks5f24bSZ/0ptfTFn7DOD7x36Ct+bcmz2vnACeJGx4Lsj8NJCSKUZ8utZ1OxTTu2wLf8QrD2gN0vP2CX28bvOWU3Zq0tZ/LXdtijvNWUhNXuWs+LYgfSaspBeUxay/OShbOzThQWX7E26WxHdZq6k/71zmX/ZyPZ8xU5Gj2phZoXA74Hjgb2Bs8xs71y1lz1GTX0xAEUFaYoK0rDVX6t1G0s2ve5aVB/9SQMKLM33x7zMA6c+zGNnPMDpe8/MsE1n9KBPmPJB9A/o8Xf35JhhHwEwbeEgakN/pi/uT//u67b9qyWVWRTcAEs5pB2PcQpV8fRChlw3g6E/m07lkwsy/lyP6StYPboPAKtH96HHOysAqN21J+lu0dihdlgPilbWZXzMfJEOeRna2jpaLkdwBwNV4XwaM7uf6E7kWTlsMysKLM3Dpz3M0PJV3PvPkUxf0r9JnbNGzuCcUe9QXJji63/5EgD/OuJd1tR14YyHT6W4IMU9pzzGS/OH8MmaslbbqyitZU1dyaZHWxav60H/7mub1DtlxLtMnTc0C98wgdLO0F/MoHhpLSuP7M+GYT2aVOnxdjVdq1ZT16+UZafuTH2vLnSbvZKSpbXMv2wfcBj4v3MofX81tcNb/50BFK7ZuOmUN1VWTOGajU3qlL28lHX7VLT/+3Ui0Syq0gZuuus4WACM3rqSmY0HxgMUlffKYXcyl/YCTnnwdHqWbODG4yeze+Vyqqp7b1HnvhkjuW/GSE4cPodvfvZNLn/2GA4dMp89ey9n3G4fANCjpI6dK1axtq6EP530BADlXTZQXJjimGFzAfjh349haU23Nvv0L3vMYWTfJXzt8ZPbrLtDKjDmXb4vBTX1DLh1DiULa6gbuPnnum7fCtYe2BsvLqBs6mL6T/iQTy4eQbfZq+g2exVDfz4DANuQomRpLbXDyxhy/Qys3rENKQpr6hl67T8BWHbyEGr23ipoNfPEUNc5qyh/eQnzL82DE5cYtGR5DOGxjVsBSgcOafWelu1tTV0Xpn0yiMOHzm8S4BpMfH84VxwxFQDD+dnUz/HS/KajrFMePB1o6Rqc07OkjkJLk/IC+ndfy+J1m0cgYwYvYPxn3+Scx0/Km7+cHSXdrYj1e5TRbdaqLQJcukfxpterD+tHn8fD316H6mMHsvrwpqP0hutmLV2DS/UspnBVHanykui/PTe3UfJJDf3umcvCb++5RdtJ0RlOPzORyyuFDXcdN2h8R3Kn1at0PT1LNgDQpbCeQwfP58MVW/613rl85abXR+78MR+vKgfgpflDOXOfmRQVpDbV61rU9LSlKWPaJwM5Noz8Tt7rPZ6duwsAI/os5coj/8FFE4+nen3bI70dUeGajRTURI8uWl2abu+uZmP/0i3rrNp8Haz79BXU7RTtr9m7nPJXlmK10e+scGVds6eazVm3by/KXosegyx7bRlr94vOQIqqNzDg1jksPmc3Nvbv2r4v1wll82H7XMvlCO51YLiZDSMKbGcCZ+ewvazo272Gn499loKCNAU4kz/YnX98vAsXHTSNmUv78txHwzh73xmMGbyA+nQBqzZ04fJnxgLw8KwRDOq5modPexjDqa7tyncmHZdRu79+dQy/+sLTXDx6GrOX9uGR2SMA+P6YV+hWvJEbxk0BYOGaHlw06YScfPd8Vbh6I/0nfIClHRzWHlDJun17UfnkAjYM7c66/XpR8fyndJ++EgqNVLdCFn81Go3VjKig5NNahvw6mhBKdylk8Tm7bTEaa0n1sQMYcHsVZS8vob4yuk0EoHLSJxSuq6ff/R8B4IXG/B8maRY1f5YstzaedGjfwaNnzH5DdJvIHe7+s9bqlw4c4rucf2nO+iPZVzsg1dFdkBgW/eK3bPh4fruGVr326udj7zi17YrAo4f94c2WHrbfHnJ6Dc7dJwITc9mGiGx/neH0MxMdPskgIvlFTzKISKIpwIlIIuk+OBFJtHy5D04BTkRicYf6Nhaz7Czyo5ci0qlk40ZfMys1s2lm9o6ZzTSzq0L5MWb2lpm9bWYvmtnuoVx5UUUktxquwWXhSYYNwFh3HwXsDxxnZocAfwC+4u77A/cCPw71lRdVRHLP3TLaWj+Gu7s3LJtTHDYPW8NyLuXAwvBaeVFFJPeyNckQ1o18E9gd+L27v2Zm5wMTzWw9sBo4JFSPnRdVIzgRicU91jW4Pmb2RqNt/JbH8lQ4FR0MHGxmI4FLgBPcfTDwJ+B/trWvGsGJSExGKvNZ1BYTPzfm7ivN7DmiFcBHuftrYdcDwOTwWnlRRST3snENzsz6mllFeN0V+AIwGyg3sz1CtYYyUF5UEcm1LD6LOgC4K1yHKwAedPcnzewbwCNmlgZWAF8P9ZUXVURyzKPrcO0+jPt04DPNlD8GPNZMufKiikju6VEtEUkkjzfJ0KEU4EQkthwuBJ5VCnAiEltbM6SdhQKciMTirgAnIgmmBS9FJLF0DU5EEskx0ppFFZGkypMBnAKciMSkSQYRSbQ8GcIpwIlIbHk/gjOzm2glTrv7d3PSIxHp1BxIp/M8wAFvbLdeiEj+cCDfR3Duflfj92bWzd1rct8lEens8uU+uDZvZjGzMWY2C3g3vB9lZjfnvGci0nl5hlsHy+Ruvd8A4whrn7v7O8ARueyUiHRmmS1XnsGS5S0lfjYz+5mZzTGz2Wb23UblN4bEz9PN7IC2eprRLKq7z98q/WAqk8+JSEJlZ3TWkPh5rZkVAy+a2SRgBFFymb3cPW1m/UL944HhYRtNlCB6dGsNZBLg5pvZoYCHTlzM5iQQIrKjcfAszKKGhDHNJX7+FnC2u6dDvSWhzknAhPC5V82swswGuPuiltrI5BT1AuBCoqSrC4H9w3sR2WFZhlvreVHNrNDM3gaWAE+HdIG7AWeE+pPMbHiovinxc7AglLWozRGcuy8DvtJWPRHZgWR+itpqXlR3TwH7h/SBj4XEz12AWnc/0MxOAe4ADt+WbmYyi7qrmf3VzJaa2RIz+4uZ7botjYlIQmR5FtXdVwLPAccRjcweDbseA/YLrxsSPzcYHMpalMkp6r3Ag0Q5DAcCDwH3ZdpxEUmYhht9M9la0ULi53eBx4GjQ7UjgTnh9RPA18Js6iHAqtauv0Fmkwzd3P3uRu//bGY/yOBzIpJQWbrRt6XEzy8C95jZJUSTEOeH+hOBE4AqoAY4t60GWnsWtTK8nGRm/wncTxS7zwgNiciOKjuzqC0lfl4JnNhMuRNzgrO1EdybRAGt4Zt8s3FbwI/iNCQiyWGd4CmFTLT2LOqw7dkREckTneQxrExk9CRDmLrdGyhtKHP3CbnqlIh0Zm1PIHQWbQY4M7sSOIoowE0kelziRUABTmRHlScjuExuEzkVOAb41N3PBUYB5TntlYh0bukMtw6WySnq+vDAa72ZlRE9UjGkrQ+JSEIlYcHLRt4IN+P9kWhmdS3wSk57JSKdWt7PojZw92+Hl7eY2WSgLNy/IiI7qnwPcK0tJmdmB7j7W7npkohIdrQ2gvt1K/scGJvlvlCyaB1Drnk524eVHHpq4dsd3QWJ4eBblmblOHl/iuruR7e0T0R2YE5WHtXaHpT4WUTiy/cRnIhIS/L+FFVEpEV5EuAyWdHXzOzfzOyK8H6omR2c+66JSKeVoLyoNwNjgLPC+zXA73PWIxHp1Mwz31o9Tgt5URvtv9HM1jZ638XMHgh5UV8zs13a6msmAW60u18I1AK4+wqgJIPPiUhSpS2zrXUNeVFHEWXrOy4sRY6ZHQj02qr+ecAKd98duAG4rq0GMglwG8OSwh4a7kuneIxWRDpKNkZwHmmSFzXEm18Cl231kZOAu8Lrh4FjbKuM9FvLJMDdSJTZpp+Z/YxoqaRrM/iciCRV5tfgtiUv6kXAE80klNmUF9Xd64FVQO/WupnJs6j3mNmbREsmGXCyuyuzvciOKoPRWSNx86IeAZxGtAZlu2Wy4OVQogw2f21c5u7zstEBEclDWZ4hdfeVZvYcUbrA3YGqcPbZzcyqwnW3hryoC8ysiGhdyuWtHTeT++D+xubkM6XAMOA9YJ9t/C4ikucsC1fhw/X8jSG4NeRFvc7dd2pUZ20IbhDlRT2HaLm2U4FnQ6atFmVyirrvVp06APh2C9VFRDLVbF7UVurfDtxtZlVANXBmWw3EfpLB3d8ys9FxPyciCZKFU9SW8qJuVadHo9e1RNfnMpbJNbhLG70tAA4AFsZpREQSJN4kQ4fKZATXs9HreqJrco/kpjsikheSEODCuXFPd//+duqPiOSDfA9wZlbk7vVmdtj27JCIdG5GdmZRt4fWRnDTiK63vW1mTwAPAesadrr7oznum4h0Rgm7BldKdDPdWDbfD+eAApzIjioBAa5fmEGdwebA1iBPvp6I5ESeRIDWAlwh0IMtA1uDPPl6IpILSThFXeTuV2+3nohI/khAgMuPvGAisn15MmZRj9luvRCR/JLvIzh3r96eHRGR/JGEa3AiIs1TgBORROokKQEzoQAnIrEY+XOKmknSGRGRLeQyL6qZ3WNm75nZDDO7w8yKQ7mFXKlVZjY9LL7bKgU4EYkvO5ntW8qLeg+wF7Av0BU4P9Q/HhgetvHAH9pqQAFOROLLQoBrKS+qu08M+5xo0Y/Boc5JwISw61WgwswGtNaGApyIxJPh6alte17Uhn3FwFeByaFoU17UYEEoa5EmGUQkvtzlRR3p7jPC7puBF9x96rZ2UyM4EYnN0pltmXL3lcBzwHEAZnYl0BdonBOmIS9qg8GhrEUKcCISW5ZmUfuGkRuN8qK+a2bnA+OAs9y9cZh8AvhamE09BFjl7otaa0OnqCIST/Zu9G02L6qZ1QMfA6+E7PaPhpWNJgInAFVADXBuWw0owIlIfDnMi+ruzcalMKt6YZw2FOBEJJZ8epJBAU5EYrN0fkQ4BTgRiUcP24tIkukUVUSSSwFORJJKIzgRSS4FOBFJpIRk1RIRaUL3wYlIsnl+RDgFOBGJTSO4PNV3YB0/+O08KvrWg8PEP/fm8dv7Nqm335i1XHD1JxQVOauqi/jBv+7ernaLS9L84MZ5DN93PatXFHHtBTuzeEEJBxyxhq9fvoiiYqd+o/HHnw7gnZd6tqutpEql4DvH7UHvARv56YS5zdaZ+rdyrvnGMG6a9B57jFrfrvY+nVfCtd/amdUrihi+bw2X3TSP4hLnkf/ty+R7e1NY5JT3rufS/5lH/8Eb29VWp5JHN/rmbLmkkCxiiZnNaLt255GqN269eiDjj9qLi784nH/592UMHV67RZ3uZSku+vkCrvz3YYw/ei+uGb9zxsfvP7iO6x+ualI+7qxq1q4s4tzDRvDoH/tw3o8XArCqupArzhnGBcfsyS8vHsJlN85r3xdMsMdv68uQ4Rta3F+ztoDHb+vLXgesi3XcKQ9UcvevdmpSftvPBnDKN5Zy58uz6VGRYvJ9lQDsNnI9N016j1ueeY/PnbiS2346MN4XyQPZXg8uV3K5HtydhMXr8kn1kmKq/tkNgPXrCplfVUqfAVv+9T36yyt4aWI5Sz8pAWDV8uJN+8aesoIb/zaHm59+j+9eN5+Cgsz+1I0Zt4qnH+oFwNQnK9j/c2sB54MZ3aheHB3/4/dK6VLqFJd0gv9zOpmlC4uZ9kwZx5+9vMU6d10/gNMvXEJJl82/k1QK/nj1QL5z/B5ccMye/O3u3hm15w7vvNiTw7+4EoAvnFbNK5PLAdj/sLWUdovaGHFADcsWFbd4nHy1wwc4d38BqM7V8beH/oPr2G3ket59q9sW5YN33UCPihTXP1zF7ybP4fOnRl9zyO61HHnSSi45aTjf/sKepFPG2FNWZNRWn53qWbow+oeQThnrVhdSVpnaos7nTlxF1YyubKzTOqVbu+XKQZz/44VYCz+a96d3ZenCYkZ/fvUW5U/d15vuZSlumjSHGyfOYdI9vfl0Xkmb7a2uLqR7eYrCcJGnz4CNLPu0aSCbfF8lB41dE/v7dGpOFOEz2TpYh1+DC0koxgOU0q2N2ttPabcU/33bR9xyxUBq1hZusa+wyBm+73p+ePqudOnq/OaJ95n9Vnc+c/hahu9bw02T5gBQUuqsXB79iK+4fS47Da2jqNjpN2gjNz/9HhCdVk15oLLN/uy8Ry3n/dciLj9r1yx/0/z36tNlVPSpZ/h+63nn5R5N9qfTcOtVg/jeb5qe3r/5j57MnV3K1CcrAFi3poBPPuxCtx4pfnh6dF11zcpC6jcaL4cR2mU3fUxlv7avqT3zSC/en96NXz7S9JJEvtMkQ4bc/VbgVoAyq+wUP7bCIue/b/uIZx/txUuTKprsX7qomNUritiwvpAN6+Gfr/Vg173XgzlPP1TJn37eNJPZ1ecNA6JR4fd+M4/LTt1yUmLZp0X0HbiRZYtKKCh0upelWF0dBdY+A+q44va5/PLioSz6uEsOvnF+m/V6d16dUsbrz+xN3QajZk0h1100lB/+Lgpo69cW8NG7pVwWJoKqlxZx5b/vylV3fog7fPuaTzjwqKajrD/8PfojNOWBShbPL+Gr3/900z53WLeqkFQ9FBbBskXF9Nlpc9B764Ue3Pfb/vzq0aotTokTIwtfycxKgReALkSx6GF3v9LMhgH3A72BN4GvunudmXUBJgCfBZYDZ7j7R621oXOdJpxLfz2f+e+X8uitTWdPAV6ZXM4+B62joNDp0jXNXp+pYd77XXh7ak8OP3El5b2j/9F7VtTTb1BdRq2+OqWcL5wWnc4e/sWVvPNiD8DoXpbipxPmcse1A5j1evesfMOk+frli7jnzVlMmDaLH/3hY0Z9bs2m4AbQvSzNQzNnMGFaVGfEATVcdeeH7DFqPQcetYYn7+pDfYhNCz7oQm1N2/8szGDUYWs3jfyefqiSMeNWAVD1z67c+MMhXHXnh1T0qc/+F+5gDTf6tjcnAy0nfr4OuMHddwdWAOeF+ucBK0L5DaFeqzp8BNfZ7HPwOj5/2go+nFW66TTyTz8fsClQ/e3uPsyvKuWN53tyyzPv4Wlj8r2VfPxeVwDuun4nfn7/h5hFM7K/u3wQSz5p+5rO5PsquezGefzppdmsWVnItd+KZma/dO4yBg6r4yuXLuYrly4G4Edn7rrFxIY0767rd2KPUTWMGbe6xTrHnb2cT+eXcOG4PXGH8t71/OSO5m8x2dp5/7WQa7+1M3deP4DdR65n3FnRtdg//nQg69cVcM34aNTeb1AdV92V2THzgntWFrwMS5A3SfwMjAXODuV3AT8hymJ/UngN8DDwOzOzcJxmWSv72sXM7gOOAvoAi4Er3f321j5TZpU+2o7JSX8kN55a+HZHd0FiOHjcfN54p9bac4yeFYP9M0dcnFHdqX+97GNgWaOiW8NlKSBK/Ex0Gro78Hvgl8CrYZSGmQ0BJrn7yHDL2XHuviDs+wAY7e6Nj7+FnI3g3P2sXB1bRDpWjEmGWImfgb3a37vNdA1OROJxIO2ZbZkecnPi5zFAhZk1DL4aJ3felPg57C8nmmxokQKciMTnGW6taCHx82yiQHdqqHYO8Jfw+onwnrD/2dauv4EmGURkG2TpPriWEj/PAu43s2uA/wMart3fDtxtZlVEDxGc2VYDCnAiEluWZlFbSvz8IXBwM+W1wGlx2lCAE5F48mg1EQU4EYklutE3PyKcApyIxNcJVgrJhAKciMSmEZyIJJOuwYlIcmXnWdTtQQFOROLTKaqIJJISP4tIomkEJyKJlR/xTQFOROKzdH6coyrAiUg8jm70FZFkMlw3+opIginAiUhi5UmA04q+IhJPwzW4TLZWmNkQM3vOzGaZ2UwzuziU729mr5rZ22b2hpkdHMrNzG40syozm25mB7TVVY3gRCS2LM2i1gPfc/e3zKwn8KaZPQ1cD1zl7pPM7ITw/ijgeGB42EYTpRIc3VoDCnAiEpNn5RTV3RcBi8LrNWY2GxgUNUBZqFYOLAyvTwImhDwMr5pZhZkNCMdplgKciMTjxAlwfczsjUbvt8iL2sDMdiFavvw14D+Ap8zsV0SX0Q4N1QYB8xt9bEEoU4ATkSzK/Ay11byoAGbWA3gE+A93Xx2SzVzi7o+Y2elEyWY+vy3d1CSDiMRm7hltbR7HrJgouN3j7o+G4nOAhtcPsTkBzaa8qEHjnKnNUoATkfjcM9taYWZGNDqb7e7/02jXQuDI8Hos8H54/QTwtTCbegiwqrXrb6BTVBGJyx1SWZlFPQz4KvBPM3s7lF0OfAP4bcheXwuMD/smAicAVUANcG5bDSjAiUh82ZlFfZEoSVdzPttMfQcujNOGApyIxJcnTzIowIlIPA4oJ4OIJJOD58d6SQpwIhKPk61JhpxTgBOR+HQNTkQSSwFORJIpOw/bbw8KcCISjwNKOiMiiaURnIgkU9Ye1co5BTgRicfBdR+ciCSWnmQQkcTSNTgRSSR3zaKKSILlyQhOK/qKSEyOp1IZba1pKS9q2PcdM3s3lF/fqPxHIS/qe2Y2rq2eagQnIvFkb7mklvKi9idKETjK3TeYWT8AM9sbOBPYBxgI/N3M9nD3FiOpRnAiEp+nM9taO4T7Ind/K7xeAzTkRf0W8At33xD2LQkfOQm43903uPtcoqXLD2565M0U4EQkFgc87RlthLyojbbxzR1zq7yoewCHm9lrZvYPMzsoVGspL2qLdIoqIvF4rAUvtyUvahFQCRwCHAQ8aGa7bktXFeBEJLa2JhAy1UJe1AXAoyHJzDQzSwN92Ia8qOadaLrXzJYCH3d0P3KgD7CsozshsST1d7azu/dtzwHMbDLRzycTy9z9uBaOY8BdQLW7/0ej8guAge5+hZntATwDDAX2Bu4luu42MJQPb22SoVON4Nr7g++szOyNtobp0rnod9aylgLWNmgpL+odwB1mNgOoA84Jo7mZZvYgMItoBvbC1oIbdLIRXFLpH0v+0e8sGTSLKiKJpQC3fdza0Vu9QOQAAAPwSURBVB2Q2PQ7SwCdoopIYmkEJyKJpQAnIomlAJdDZnZcWPWgysz+s6P7I20zszvMbEm4RUHynAJcjphZIfB74HiiGxTPCqshSOd2J5Ct+7ykgynA5c7BQJW7f+judcD9RKshSCfm7i8A1R3dD8kOBbjcib3ygYhklwKciCSWAlzuxF75QESySwEud14HhpvZMDMrIVpq+YkO7pPIDkUBLkfcvR64CHiKaCnmB919Zsf2StpiZvcBrwB7mtkCMzuvo/sk206PaolIYmkEJyKJpQAnIomlACciiaUAJyKJpQAnIomlAJdHzCxlZm+b2Qwze8jMurXjWHea2anh9W2tLQRgZkeZ2aHb0MZHZtYk+1JL5VvVWRuzrZ+Y2ffj9lGSTQEuv6x39/3dfSRRtqELGu8MCXNjc/fz3X1WK1WOAmIHOJGOpgCXv6YCu4fR1VQzewKYZWaFZvZLM3vdzKab2TchykFpZr8L69P9HejXcCAze97MDgyvjzOzt8zsHTN7xsx2IQqkl4TR4+Fm1tfMHgltvG5mh4XP9jazKWY208xuA6ytL2Fmj5vZm+Ez47fad0Mof8bM+oay3cxscvjMVDPbKxs/TEmmTpUXVTITRmrHA5ND0QHASHefG4LEKnc/yMy6AC+Z2RTgM8CeRGvT9SfKLXnHVsftC/wROCIcq9Ldq83sFmCtu/8q1LsXuMHdXzSzoURPa4wArgRedPerzexEIJOnAL4e2ugKvG5mj7j7cqA78Ia7X2JmV4RjX0SUDOYCd3/fzEYDNwNjt+HHKDsABbj80rVRgtypwO1Ep47T3H1uKD8W2K/h+hpQDgwHjgDuC4lyF5rZs80c/xDghYZjuXtL66J9Htg7SkwOQJmZ9QhtnBI++zczW5HBd/qumX05vB4S+rocSAMPhPI/A4+GNg4FHmrUdpcM2pAdlAJcflnv7vs3Lgj/0Nc1LgK+4+5PbVXvhCz2owA4xN1rm+lLxszsKKJgOcbda8zseaC0heoe2l259c9ApCW6Bpc8TwHfMrNiADPbw8y6Ay8AZ4RrdAOAo5v57KvAEWY2LHy2MpSvAXo2qjcF+E7DGzNrCDgvAGeHsuOBXm30tRxYEYLbXkQjyAYFQMMo9GyiU9/VwFwzOy20YWY2qo02ZAemAJc8txFdX3srJE75X6KR+mPA+2HfBKIVM7bg7kuB8USng++w+RTxr8CXGyYZgO8CB4ZJjFlsns29iihAziQ6VZ3XRl8nA0VmNhv4BVGAbbAOODh8h7HA1aH8K8B5oX8z0TLw0gqtJiIiiaURnIgklgKciCSWApyIJJYCnIgklgKciCSWApyIJJYCnIgk1v8DK5Tq6EITqtMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s89wSFWRG0If"
      },
      "source": [
        "## XGBoost model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyj7ERelHsdv"
      },
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJul8GFRbV6b"
      },
      "source": [
        "#@title Build model\n",
        "seed = 100 #@param {type:\"integer\"}\n",
        "n_estimators = 100 #@param {type:\"integer\"}\n",
        "max_depth = 3 #@param {type:\"integer\"}\n",
        "learning_rate = 0.1 #@param {type:\"number\"}\n",
        "min_child_weight = 0 #@param {type:\"number\"}\n",
        "subsample = 1 #@param {type:\"number\"}\n",
        "colsample_bytree = 1 #@param {type:\"number\"}\n",
        "colsample_bylevel = 1 #@param {type:\"number\"}\n",
        "gamma = 0 #@param {type:\"number\"}\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# xgbmodel = XGBClassifier(seed=seed,\n",
        "#                         n_estimators=n_estimators,\n",
        "#                         max_depth=max_depth,\n",
        "#                         learning_rate=learning_rate,\n",
        "#                         min_child_weight=min_child_weight,\n",
        "#                         subsample=subsample,\n",
        "#                         colsample_bytree=colsample_bytree,\n",
        "#                         colsample_bylevel=colsample_bylevel,\n",
        "#                         gamma=gamma)\n",
        "xgbmodel = XGBClassifier()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raMH4SDILQg5"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdKqzd_pHq2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac54a01-b4f1-4a47-c636-e5b5fc4f213a"
      },
      "source": [
        "# xgbmodel.fit(X_train, y_train_label)\n",
        "xgbmodel.fit(X_train_scaled, y_train_scaled_label)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK8mFeHhLlPP"
      },
      "source": [
        "### Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0U2JZJTLm35"
      },
      "source": [
        "# y_pred = xgbmodel.predict(X_test)\n",
        "y_pred = xgbmodel.predict(X_test_scaled)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0_KL2VEbUea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "e263b2e3-c324-4b37-cd9a-a489497fbe1e"
      },
      "source": [
        "# show_val_res_label(y_test_label, y_pred)\n",
        "show_val_res_label(y_test_scaled_label, y_pred)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion_matrix: [[355 331]\n",
            " [358 337]]\n",
            "acc: 0.501086169442433\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.52      0.51       686\n",
            "           1       0.50      0.48      0.49       695\n",
            "\n",
            "    accuracy                           0.50      1381\n",
            "   macro avg       0.50      0.50      0.50      1381\n",
            "weighted avg       0.50      0.50      0.50      1381\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMIt3B0XgrVm"
      },
      "source": [
        ""
      ],
      "execution_count": 69,
      "outputs": []
    }
  ]
}